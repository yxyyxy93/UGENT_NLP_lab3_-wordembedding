{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "NLP_lab3_student.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FZBHAqeMgbEC",
        "SA4VwAk0gbEV",
        "1dnmps0wgbE_",
        "s-6wANQXgbE_"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yxyyxy93/UGENT_NLP_lab3_-wordembedding/blob/master/NLP_lab3_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "f3m1XdErgbC3",
        "colab_type": "text"
      },
      "source": [
        "# Lab session 3: Word embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb85irsCgbC6",
        "colab_type": "text"
      },
      "source": [
        "This lab covers word embedding as seen in the theory lectures (DL lecture 5).\n",
        "\n",
        "General instructions:\n",
        "- Complete the code where needed\n",
        "- Provide answers to questions only in the cell where indicated\n",
        "- **Do not alter the evaluation cells** (`## evaluation`) in any way as they are needed for the partly automated evaluation process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjZTc_PbgbC7",
        "colab_type": "text"
      },
      "source": [
        "## **Embedding; the Steroids for NLP!**\n",
        "\n",
        "Pre-trained embedding have brought NLP a long way. Most of the recent methods include word embeddings into their pipeline to obtain state-of-the-art performance. `Word2vec` is among the most famous methods to efficiently create word embeddings and has been around since 2013. Word2Vec has two different model architectures, namely `Skip-gram` and `CBOW`. `Skip-gram` was explained in more detail in the theory lecture, and today we will play with `CBOW`. We will train our own little embeddings, and use them to visualize text corpora. In the last part, we will download and utilize other pretrained embeddings to build a Part-of-Speech tagging (PoS) model.\n",
        "\n",
        "<img src=\"http://3g1o5q2sqh3w32ohtj4dwggw.wpengine.netdna-cdn.com/wp-content/uploads/2012/08/steroids-before-and-after-480x321.jpg\" alt=\"img\" width=\"512px\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "RZvlPyRBgbC8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50eaf423-161e-480d-8762-912f820507a9"
      },
      "source": [
        "# import necessary packages\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "from random import shuffle\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXokh80WgbDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for reproducibility\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mk5x7nIgbDG",
        "colab_type": "text"
      },
      "source": [
        "## 1. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "FdRygDPPgbDH",
        "colab_type": "text"
      },
      "source": [
        "As always, let's first prepare the data. We shall use the `text8` dataset, which offers cleaned English Wikipedia text. The data is clean UTF-8 and all characters are lower-cased with valid encodings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "id": "_uvCxkt8gbDI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "be394d93-3113-44d9-df74-45ae3f212bc6"
      },
      "source": [
        "!wget \"http://mattmahoney.net/dc/text8.zip\" -O text8.zip\n",
        "!unzip -o text8.zip\n",
        "!rm text8.zip\n",
        "!head -c 1b text8 # print first bytes of text8 data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-28 06:39:37--  http://mattmahoney.net/dc/text8.zip\n",
            "Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.75\n",
            "Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.75|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31344016 (30M) [application/zip]\n",
            "Saving to: ‘text8.zip’\n",
            "\n",
            "text8.zip           100%[===================>]  29.89M   710KB/s    in 44s     \n",
            "\n",
            "2020-04-28 06:40:21 (703 KB/s) - ‘text8.zip’ saved [31344016/31344016]\n",
            "\n",
            "Archive:  text8.zip\n",
            "  inflating: text8                   \n",
            " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the b"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "id": "wQEkgZZtgbDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read text8\n",
        "with open('text8', 'r') as input_file:\n",
        "    text = input_file.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlkluIulgbDR",
        "colab_type": "text"
      },
      "source": [
        "### Tokenization\n",
        "We first chop our text into pieces using NLTK's `WordPuncTokenizer`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "id": "dvr2Ll-hgbDS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d2b151cb-3fe9-4447-e533-8826539209c5"
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "tknzr = WordPunctTokenizer()\n",
        "tokenized_text = tknzr.tokenize(text)\n",
        "\n",
        "print(tokenized_text[0:20])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU0-NJ-OgbDW",
        "colab_type": "text"
      },
      "source": [
        "### Build dictionary\n",
        "In this step, we convert each word to a unique id. We can define our vocabulary trimming rules, which specify whether certain words should remain in the vocabulary, be trimmed away, or handled differently. In following, we limit our vocabulary size to `vocab_size` words and replace the remaining tokens with `UNK`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "id": "vSoKmhDvgbDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(text, vocab_size = None):\n",
        "    \n",
        "    word_counts = Counter(text)\n",
        "    \n",
        "    sorted_token = sorted(word_counts, key=word_counts.get, reverse=True) # sort by frequency\n",
        "    \n",
        "    if vocab_size: # keep most frequent words\n",
        "        sorted_token = sorted_token[:vocab_size-1] \n",
        "    \n",
        "    sorted_token.insert(0, 'UNK') # reserve 0 for UNK\n",
        "    \n",
        "    id_to_token = {k: w for k, w in enumerate(sorted_token)}\n",
        "    token_to_id = {w: k for k, w in id_to_token.items()}\n",
        "    \n",
        "    # tokenize words in vocab and replace rest with UNK\n",
        "    tokenized_ids = [token_to_id[w] if w in token_to_id else 0 for w in text]\n",
        "\n",
        "    return tokenized_ids, id_to_token, token_to_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "poWBmBt4gbDa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "a07f1fed-46a5-4141-bb88-4fc74f8f071a"
      },
      "source": [
        "tokenized_ids, id_to_token, token_to_id = get_data(tokenized_text)\n",
        "print('-' * 50)\n",
        "print('Number of uniqe tokens: {}'.format(len(id_to_token)))\n",
        "print('-' * 50)\n",
        "print(\"tokenized text: {}\".format(tokenized_text[0:20]))\n",
        "print('-' * 50)\n",
        "print(\"tokenized ids: {}\".format(tokenized_ids[0:20]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "Number of uniqe tokens: 253855\n",
            "--------------------------------------------------\n",
            "tokenized text: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english']\n",
            "--------------------------------------------------\n",
            "tokenized ids: [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156, 128, 742, 477, 10572, 134, 1, 27350, 2, 1, 103]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed6YCiqQgbDc",
        "colab_type": "text"
      },
      "source": [
        "### Generate samples\n",
        " \n",
        "The `CBOW` model architecture tries to predict the current target word (the center word) based on the source context words (surrounding words). The training data thus comprises pairs of `(context_window, target_word)`, for which the model should predict the `target_word based` on the `context_window` words.\n",
        "\n",
        "Considering a simple sentence, __the quick brown fox jumps over the lazy dog__, with a `context_window` of size 1, we have examples like __([quick, fox], brown)__, __([the, brown], quick)__, __([the, dog], lazy)__ and so on. \n",
        "\n",
        "<img src=\"\n",
        "https://cdn-images-1.medium.com/max/800/1*UVe8b6CWYykcxbBOR6uCfg.png\" alt=\"img\" width=\"400px\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F65BxlO6gbDd",
        "colab_type": "text"
      },
      "source": [
        "Now let us convert our tokenized text from `tokenized_ids` into `(context_window, target_word)` pairs.\n",
        "\n",
        "You should loop over the `tokenized_ids` and build a __generator__ which yields a target word of length 1 and surrounding context of length (2 $\\times$ `window_size`) where we take `window_size` words before and after the target word in our corpus. Remember to pad context words with zeroes to a fixed length if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "nxOaj97MgbDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_sample(tknzd_ids, window_size = 5):\n",
        "    for index, target in enumerate(tknzd_ids):\n",
        "    ############### for student ################ \n",
        "        if index - window_size < 0:\n",
        "            context_window = [0] * (window_size - index) + tknzd_ids[0: index]\n",
        "        else:\n",
        "            context_window = tknzd_ids[index - window_size: index]\n",
        "        if index + window_size + 1 >= len(tknzd_ids):\n",
        "            context_window += tknzd_ids[index + 1:] + [0] * (index + window_size + 1 - len(tknzd_ids))\n",
        "        else:\n",
        "            context_window += tknzd_ids[index + 1: index + window_size + 1]\n",
        "\n",
        "        print(context_window)\n",
        "    ############################################\n",
        "        yield context_window, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "T98m7j2VgbDf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "0839b849-6f6d-4596-94c8-83e02c033551"
      },
      "source": [
        "## evaluation\n",
        "## DON'T CHANGE THIS CELL IN ANY WAY\n",
        "\n",
        "dummy_gen = generate_sample([11, 12, 13, 14, 15], 2)\n",
        "dummy_example = list(dummy_gen)\n",
        "\n",
        "assert isinstance(dummy_example[0], tuple), \"Is it a pair?\" \n",
        "assert len(dummy_example[0][0]) == 4, \"Context length should be 2 * window_size\"\n",
        "assert dummy_example[0][1] == 11, \"Did you return the correct target word?\"\n",
        "assert dummy_example[0][0][0] == dummy_example[0][0][1]==0, \"Did you add 0 pads where needed?\"\n",
        "assert len(dummy_example[0]) == len(dummy_example[-1]), \"Length of all instances should be the same due to the padding\"\n",
        "assert dummy_example[0][0] == [0, 0, 12, 13], \"Did you consider contexts before and after the target word?\" \n",
        "\n",
        "print('Well done!')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 12, 13]\n",
            "[0, 11, 13, 14]\n",
            "[11, 12, 14, 15]\n",
            "[12, 13, 15, 0]\n",
            "[13, 14, 0, 0]\n",
            "Well done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmxqYhZugbDi",
        "colab_type": "text"
      },
      "source": [
        "To train our model faster, it is good idea to batchify our data. For your convenience, we implemented it for you: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "ZAEpJQOTgbDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_gen(tknzd_ids, batch_size = 4,  window_size = 5):\n",
        "    \n",
        "    # shuffle(tknzd_ids) # shuffle is in place and does not return anything\n",
        "    \n",
        "    single_gen = generate_sample(tknzd_ids, window_size) # get sample generator\n",
        "    \n",
        "    while True:\n",
        "        try: \n",
        "            # The end of iterations is indicated by an exception \n",
        "            context_batch = np.zeros([batch_size, window_size * 2], dtype=np.int32)\n",
        "            target_batch = np.zeros([batch_size], dtype=np.int32)\n",
        "            for index in range(batch_size):\n",
        "                context_batch[index], target_batch[index] = next(single_gen)\n",
        "            yield context_batch, target_batch\n",
        "        except StopIteration:\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "2GhnPgIygbDl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "634a0464-81d7-4347-c9e9-5ec1c2e68443"
      },
      "source": [
        "dummy_batches = batch_gen([11, 12, 13, 14, 15, 16, 17, 18], batch_size=4, window_size=2)\n",
        "\n",
        "print(\"First batch:\\n\", next(dummy_batches))\n",
        "print('-' * 50)\n",
        "print(\"Second batch:\\n\", next(dummy_batches))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 12, 13]\n",
            "[0, 11, 13, 14]\n",
            "[11, 12, 14, 15]\n",
            "[12, 13, 15, 16]\n",
            "First batch:\n",
            " (array([[ 0,  0, 12, 13],\n",
            "       [ 0, 11, 13, 14],\n",
            "       [11, 12, 14, 15],\n",
            "       [12, 13, 15, 16]], dtype=int32), array([11, 12, 13, 14], dtype=int32))\n",
            "--------------------------------------------------\n",
            "[13, 14, 16, 17]\n",
            "[14, 15, 17, 18]\n",
            "[15, 16, 18, 0]\n",
            "[16, 17, 0, 0]\n",
            "Second batch:\n",
            " (array([[13, 14, 16, 17],\n",
            "       [14, 15, 17, 18],\n",
            "       [15, 16, 18,  0],\n",
            "       [16, 17,  0,  0]], dtype=int32), array([15, 16, 17, 18], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqvdluOCgbDn",
        "colab_type": "text"
      },
      "source": [
        "## 2. CBOW Model\n",
        "\n",
        "We now leverage pytorch to build our CBOW model. For this, our inputs will be our context words which are first converted into one-hot vectors, and next projected into a word-vector. Word-vectors will be obtained from an embedding-matrix ($W$) which represents the distributed feature vectors associated with each word in the vocabulary. This embedding-matrix is initialized with a normal distribution.\n",
        "\n",
        "Next, the projected words are averaged out (hence we don’t really consider the order or sequence in the context words when averaged) and then we multiply this averaged vector with another embedding matrix ($W'$), which defines so-called context embeddings to project the CBOW representation back to the one-hot space to match with the target word. (Note: in the theory, this is introduced as the linear output layer, with dimensions equal to the transposed of the embedding matrix.)  We thus apply a log-softmax on the resulting context vectors, to predict the most probable target word given the input context.\n",
        "\n",
        "We match the predicted word with the actual target word, compute the loss by leveraging the cross entropy loss and perform back-propagation with each iteration to update the embedding-matrix in the process.\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/freeze/max/1000/1*uATTt40gbJ1HJQgIqE-VPA.png?q=20\" alt=\"img\" width=\"512px\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CstzOi4kgbDn",
        "colab_type": "text"
      },
      "source": [
        "### Question-1\n",
        "\n",
        "- How could we modify the `CBOW` architecture to consider the order and position of the context words?  \n",
        "\n",
        "**<font color=blue><<< Replace the average and hidden layer multiplication operation with a RNN architecture to predict the output>>></font>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfD35CQIgbDo",
        "colab_type": "text"
      },
      "source": [
        "Now, complete the CBOW class below, following the instructions in the comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "sJlgRt9CgbDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CBOW(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim=100, vocab_size=10000):\n",
        "        super(CBOW, self).__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        # use nn.Parameter to define the two matrices W and W' from above, \n",
        "        # thus one for word (W) and one for context (W') embeddings:\n",
        "        # self.embed_in = ...  # word embedding\n",
        "        # self.embed_out = ... # context embedding\n",
        "        ############### for student ################\n",
        "        self.embed_in = nn.Parameter(torch.zeros(embedding_dim, vocab_size))\n",
        "        self.embed_out = nn.Parameter(torch.zeros(vocab_size, embedding_dim))\n",
        "\n",
        "        ############################################\n",
        "        \n",
        "        self.reset_parameters()\n",
        "            \n",
        "    \n",
        "    def reset_parameters(self):\n",
        "        # Initialize parameters\n",
        "        nn.init.kaiming_uniform_(self.embed_in, a=math.sqrt(5))\n",
        "        nn.init.kaiming_uniform_(self.embed_out, a=math.sqrt(5))\n",
        "    \n",
        "    def get_word_embedding(self):\n",
        "        return self.embed_in\n",
        "    \n",
        "    def get_context_embedding(self):\n",
        "        return self.embed_out\n",
        "    \n",
        "    \n",
        "    def forward(self, inps):\n",
        "        \"\"\"\n",
        "        Convert given indices to log-probablities. \n",
        "        Follow these steps:\n",
        "        1) convert the inputs' word indices to one-hot vectors\n",
        "        2) project the one-hot vectors to their embedding (use F.linear, do *NOT* use nn.Embedding)\n",
        "        3) calculate the mean of the embedded vectors\n",
        "        4) project back with the context embedding matrix \n",
        "        5) calculate the log-probability (with F.log_softmax)\n",
        "                \n",
        "        :argument:\n",
        "            inps (list): List of indecies\n",
        "        \n",
        "        :return:\n",
        "            log-probablity of words\n",
        "        \"\"\"\n",
        "        ############### for student ################\n",
        "        # embed_in(N, V) * one_hot(V) = embedding(N)\n",
        "        # 1) convert the inputs' word indices to one-hot vectors\n",
        "        # n = inps.shape[1]\n",
        "        # one_hot = torch.zeros(n, self.vocab_size)\n",
        "        # one_hot[torch.arange(n), inps] = 1\n",
        "        \n",
        "        one_hot = torch.nn.functional.one_hot(inps, self.vocab_size)\n",
        "        one_hot = one_hot.to(dtype=torch.float32)\n",
        "        print(one_hot.shape)\n",
        "        # print(self.embed_in.shape)\n",
        "        # print(one_hot.dtype)\n",
        "        # print(self.embed_in.dtype)\n",
        "\n",
        "        # 2) project the one-hot vectors to their embedding (use F.linear, do *NOT* use nn.Embedding)\n",
        "        embedding = F.linear(one_hot, self.embed_in)\n",
        "        print(embedding.shape)\n",
        "\n",
        "        # 3) calculate the mean of the embedded vectors\n",
        "        if len(embedding.shape) > 2:\n",
        "            embedding_ave =  torch.mean(embedding, dim=1, keepdim=False)\n",
        "        else:\n",
        "            embedding_ave =  torch.mean(embedding, dim=0, keepdim=True)\n",
        "        print(embedding_ave.shape)\n",
        "\n",
        "        # 4) project back with the context embedding matrix \n",
        "        Out_put = F.linear(embedding_ave, self.embed_out)\n",
        "        print(Out_put.shape)\n",
        "        \n",
        "        # 5) calculate the log-probability (with F.log_softmax)\n",
        "        log_probs = F.log_softmax(Out_put)\n",
        "        print(log_probs)\n",
        "\n",
        "        ############################################\n",
        "        return log_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pPFbA_FIqSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one_hot = torch.randn((2, 4, 10))\n",
        "# print(one_hot.shape)\n",
        "# embed_in = torch.randn((20, 10))\n",
        "# print(embed_in.shape)\n",
        "# temp = F.linear(one_hot, embed_in)\n",
        "# print(temp.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-R0cdcw8nZD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "9df47676-3c65-4a3a-a2c7-8d408ccf1b83"
      },
      "source": [
        "dummy_model = CBOW(20, 10)\n",
        "dummy_inps2 = torch.tensor([[6, 7, 9, 0], [1, 2, 3, 4]], dtype=torch.long)\n",
        "dummy_pred1 = dummy_model(dummy_inps2)"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 4, 10])\n",
            "torch.Size([2, 4, 20])\n",
            "torch.Size([2, 20])\n",
            "torch.Size([2, 10])\n",
            "tensor([[-2.2789, -2.3172, -2.3153, -2.3168, -2.3121, -2.2767, -2.2632, -2.2728,\n",
            "         -2.3450, -2.3312],\n",
            "        [-2.3766, -2.3987, -2.2271, -2.3053, -2.2720, -2.3562, -2.3239, -2.3105,\n",
            "         -2.2142, -2.2587]], grad_fn=<LogSoftmaxBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:79: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "s8CAB7lkgbDq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "7d1b0c9c-3796-4565-efb5-04f055ad3392"
      },
      "source": [
        "## evaluation\n",
        "## DON'T CHANGE THIS CELL IN ANY WAY\n",
        "\n",
        "dummy_model = CBOW(20, 10)\n",
        "dummy_inps1 = torch.tensor([[6, 7, 9, 0]], dtype=torch.long)\n",
        "dummy_inps2 = torch.tensor([[6, 7, 9, 0], [1, 2, 3, 4]], dtype=torch.long)\n",
        "dummy_pred1 = dummy_model(dummy_inps1)\n",
        "dummy_pred2 = dummy_model(dummy_inps2)\n",
        "\n",
        "assert isinstance(dummy_model.embed_in, nn.Parameter), \"Use nn.Parameter for embed_in\"\n",
        "assert isinstance(dummy_model.embed_out, nn.Parameter), \"Use nn.Parameter for embed_out\"\n",
        "assert dummy_model.embed_in.shape == torch.Size([20, 10]), \"param_in shape is not correct\"\n",
        "assert dummy_model.embed_out.shape == torch.Size([10, 20]), \"param_out shape is not correct\"\n",
        "assert dummy_pred1.shape == torch.Size([1,10]), \"Prediction shape is not correct\"\n",
        "assert dummy_pred2.shape == torch.Size([2,10]), \"Prediction shape is not correct\"\n",
        "assert dummy_pred1.grad_fn.__class__.__name__ == 'LogSoftmaxBackward', \"softmax layer?\"\n",
        "\n",
        "print('Well done!')"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 10])\n",
            "torch.Size([1, 4, 20])\n",
            "torch.Size([1, 20])\n",
            "torch.Size([1, 10])\n",
            "tensor([[-2.2139, -2.3625, -2.2115, -2.3181, -2.3558, -2.3497, -2.3327, -2.2591,\n",
            "         -2.2980, -2.3394]], grad_fn=<LogSoftmaxBackward>)\n",
            "torch.Size([2, 4, 10])\n",
            "torch.Size([2, 4, 20])\n",
            "torch.Size([2, 20])\n",
            "torch.Size([2, 10])\n",
            "tensor([[-2.2139, -2.3625, -2.2115, -2.3181, -2.3558, -2.3497, -2.3327, -2.2591,\n",
            "         -2.2980, -2.3394],\n",
            "        [-2.2885, -2.2856, -2.2546, -2.2930, -2.3903, -2.3146, -2.2616, -2.3209,\n",
            "         -2.3831, -2.2448]], grad_fn=<LogSoftmaxBackward>)\n",
            "Well done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:79: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R7MA17XgbDs",
        "colab_type": "text"
      },
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co6ptE4XgbDs",
        "colab_type": "text"
      },
      "source": [
        "Before jumping into the training part, we need to define some hyper-parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "6wpG9Ii6gbDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# embedding hyper-parameters\n",
        "\n",
        "EMBED_DIM = 100\n",
        "WINDOW_SIZE = 5\n",
        "BATCH_SIZE = 128\n",
        "VOCAB_SIZE = 10_000\n",
        "\n",
        "EPOCHS = 1 # to make things faster in this basic setup\n",
        "interval = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "18geGfXvgbDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get data\n",
        "\n",
        "tokenized_ids, id_to_token, _ = get_data(tokenized_text, VOCAB_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry-AGVgJgbDy",
        "colab_type": "text"
      },
      "source": [
        "Now we define our main training loop. Please implement the typical steps for training:\n",
        "- Reset all gradients\n",
        "- Compute output and loss value\n",
        "- Perform back-propagation\n",
        "- Update the network’s parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "IOLiP-whgbDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CBOW(EMBED_DIM, VOCAB_SIZE)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "    \n",
        "    batches = batch_gen(tokenized_ids, batch_size=BATCH_SIZE, window_size=WINDOW_SIZE)\n",
        "    total_loss = 0.0\n",
        "    \n",
        "    for iteration, (context, target) in enumerate(batches):\n",
        "        \n",
        "        # Step 1. Prepare the inputs to be passed to the model (wrap integer indices in tensors)\n",
        "        # Step 2. Recall that torch *accumulates* gradients. Before passing a\n",
        "        #         new instance, you need to zero out the gradients from the old instance\n",
        "        # Step 3. Run the forward pass, getting predicted target words log probabilities\n",
        "        # Step 4. Compute your loss function. \n",
        "        # Step 5. Do the backward pass and update the gradient\n",
        "        \n",
        "        ############### for student ################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ############################################\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        if iteration % interval == 0:\n",
        "            print('Epoch:{}/{},\\tIteration:{},\\tLoss:{}'.format(e, EPOCHS, iteration, total_loss / interval), end = \"\\r\", flush = True)\n",
        "            loss_history.append(total_loss / interval)\n",
        "            total_loss = 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "mvZuqd6igbD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## evaluation\n",
        "## DON'T CHANGE THIS CELL IN ANY WAY\n",
        "\n",
        "assert loss_history[-1] < 6.5\n",
        "\n",
        "print('Well done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRCFzsSugbD4",
        "colab_type": "text"
      },
      "source": [
        "### Nearest words\n",
        "\n",
        "So far, we trained the __CBOW__ successfully, now it is time to explore it more. In this part, we want to find the $k$ nearest word to a given word, i.e., nearby in the vector space.\n",
        "\n",
        "<img src=\"https://i0.wp.com/i.imgur.com/IeZt839.png\" alt=\"img\" width=\"480px\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNDP8WFWgbD5",
        "colab_type": "text"
      },
      "source": [
        "Define a helper function to retrieve the corresponding vector for a given word:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "HjeXxHDWgbD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# be sure jupyter session is not terminated!\n",
        "# use token_to_id to retrieve the index\n",
        "\n",
        "def get_vector(embedding, word):\n",
        "    \"\"\"\n",
        "    :argument:\n",
        "        embedding (matrix): embedding matrix \n",
        "        word (str): The given input\n",
        "    :return:\n",
        "        word-vector for a given word\n",
        "    \"\"\"\n",
        "    ############### for student ################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ############################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "zI8y0yu4gbD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## evaluation\n",
        "## DON'T CHANGE THIS CELL IN ANY WAY\n",
        "\n",
        "embedding = model.embed_in.data\n",
        "\n",
        "assert get_vector(embedding, 'the').shape == torch.Size([100, 1]), \"vector size should be (embed_dim, 1)\"\n",
        "assert np.allclose(embedding[:,(0,)].data.cpu().numpy(), get_vector(embedding, 'UNK').data.cpu().numpy()), \"Do you retrieve correct vector?\"\n",
        "print('Well done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fFMZdAZgbD9",
        "colab_type": "text"
      },
      "source": [
        "Define a function to return the list of $k$ most similar words, e.g., based on `cosine-similarity`, to a given word:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "V2xmYc7wgbD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def most_similar_words(embedding, word, k=1):\n",
        "    \"\"\"\n",
        "    return k similar (based on cosine similarity) items\n",
        "    :argument:\n",
        "        embedding (matrix): embedding matrix \n",
        "        word (str): The given input\n",
        "        k (int): The number of similar items    \n",
        "    :return:\n",
        "        list of k similar items\n",
        "    \"\"\"\n",
        "    most_similar = []\n",
        "    x = get_vector(embedding, word) # 300, 1\n",
        "    # ...\n",
        "    # most_similar = ...\n",
        "    ############### for student ################\n",
        "\n",
        "\n",
        "    F.cosine_similarity\n",
        "    # transpose if needed\n",
        "\n",
        "\n",
        "    ############################################\n",
        "    return most_similar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "qKNM-cl_gbEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## evaluation\n",
        "## DON'T CHANGE THIS CELL IN ANY WAY\n",
        "\n",
        "embedding = model.embed_in.data\n",
        "\n",
        "dummy_list = most_similar_words(embedding, \"mutual\", 3)\n",
        "s1 = F.cosine_similarity(get_vector(embedding, dummy_list[0]).T, get_vector(embedding, \"mutual\").T)\n",
        "s2 = F.cosine_similarity(get_vector(embedding, dummy_list[1]).T, get_vector(embedding, \"mutual\").T)\n",
        "s3 = F.cosine_similarity(get_vector(embedding, dummy_list[2]).T, get_vector(embedding, \"mutual\").T)\n",
        "\n",
        "assert len(dummy_list) == 3, \"return k nearest words\"\n",
        "assert s1.data.cpu().numpy()[0] >= s2.data.cpu().numpy()[0], \"first item should have higher probablity to the given word\"\n",
        "assert s2.data.cpu().numpy()[0] >= s3.data.cpu().numpy()[0], \"second item should have higher probability\"\n",
        "assert s1.data.cpu().numpy()[0] != 1 , \"Similarity score of one means you return the word itself\"\n",
        "\n",
        "print('Well done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZBHAqeMgbEC",
        "colab_type": "text"
      },
      "source": [
        "### Linear projection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "90ROkFG9gbED",
        "colab_type": "text"
      },
      "source": [
        "The simplest linear dimensionality reduction method is __P__rincipial __C__omponent __A__nalysis.\n",
        "\n",
        "In geometric terms, PCA tries to find axes along which most of the variance occurs. The \"natural\" axes, if you wish.\n",
        "\n",
        "\n",
        "<img src=\"https://hackernoon.com/hn-images/1*ZFqnPuxa1PtUece-OHBoTA.png\" alt=\"img\" width=\"512px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "VflNn0bJgbED",
        "colab_type": "text"
      },
      "source": [
        "Under the hood, it attempts to decompose an object-feature matrix $X$ into two smaller matrices: $W$ and $\\hat W$ minimizing the *mean squared error*:\n",
        "\n",
        "$$\\min_{W, \\hat{W}} \\ \\ \\|(X W) \\hat{W} - X\\|^2_2 $$\n",
        "\n",
        "with\n",
        "- $X \\in \\mathbb{R}^{n \\times m}$ - object matrix (**centered**);\n",
        "- $W \\in \\mathbb{R}^{m \\times d}$ - matrix of direct transformation;\n",
        "- $\\hat{W} \\in \\mathbb{R}^{d \\times m}$ - matrix of reverse transformation;\n",
        "- $n$ samples, $m$ original dimensions and $d$ target dimensions;\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "1y5hLWENgbED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Map word vectors onto a 2D plane with PCA. Use the good old sklearn API (fit, transform).\n",
        "# Finally, normalize the mapped vectors, to make sure they have zero mean and unit variance \n",
        "\n",
        "# word_vectors = ...\n",
        "# ...\n",
        "# word_vectors_pca = ...  # normalized vectors\n",
        "\n",
        "############### for student ################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "v4A6a9ztgbEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## evaluation\n",
        "## DON'T CHANGE THIS CELL IN ANY WAY\n",
        "\n",
        "assert word_vectors_pca.shape == (len(word_vectors), 2), \"there must be a 2D vector for each word\"\n",
        "assert max(abs(word_vectors_pca.mean(0))) < 1e-5, \"points must be zero-centered\"\n",
        "assert max(abs(1.0 - word_vectors_pca.std(0))) < 1e-2, \"points must have unit variance\"\n",
        "\n",
        "print('Well done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "EvcUpwr9gbEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install bokeh\n",
        "\n",
        "import bokeh.models as bm, bokeh.plotting as pl\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "output_notebook()\n",
        "\n",
        "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
        "                 width=600, height=400, show=True, **kwargs):\n",
        "    \"\"\" draws an interactive plot for data points with auxiliary info on hover \"\"\"\n",
        "    if isinstance(color, str): color = [color] * len(x)\n",
        "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
        "\n",
        "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
        "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
        "\n",
        "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
        "    if show: pl.show(fig)\n",
        "    return fig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "JtMH4cJigbEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "draw_vectors(word_vectors_pca[:, 0], word_vectors_pca[:, 1], token=list(id_to_token.values()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zXZPLH7gbEL",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing neighbors with t-SNE\n",
        "PCA is nice but it's strictly linear and thus only able to capture coarse high-level structure of the data.\n",
        "\n",
        "If we instead want to focus on keeping neighboring points near, we could use TSNE, which is itself an embedding method. Here you can read __[more on TSNE](https://distill.pub/2016/misread-tsne/)__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "RyhOG_-wgbEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Map word vectors onto a 2d plane with TSNE. (Hint: use verbose=100 to see what it's doing.)\n",
        "# Normalize them just like with PCE into word_tsne\n",
        "\n",
        "# ...\n",
        "# word_tsne = ...\n",
        "\n",
        "############### for student ################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "scrolled": true,
        "id": "r2u84sUxgbEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='green', token=list(id_to_token.values()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "458faV4vgbEP",
        "colab_type": "text"
      },
      "source": [
        "## 3. POS tagging task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZPhOVWJgbEP",
        "colab_type": "text"
      },
      "source": [
        "The embeddings by themselves are nice to have, but the main objective of course is to solve a particular (NLP) task. Further, so far we have trained our own embedding from a given corpus, but often it is beneficial to use existing word embeddings.\n",
        "\n",
        "Now, let's use embeddings to train a simple Part of Speech (PoS) tagging model, using pretrained word embeddings. We shall use [50d glove word vectors](https://nlp.stanford.edu/projects/glove/) for the rest of this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nS1jY5LgbEQ",
        "colab_type": "text"
      },
      "source": [
        "Before jumping into our neural POS tagger, it is better to set up a baseline to give us an intuition how the neural model performs compared to other models. The baseline model is the [Conditional-Random-Field (CRF)](https://en.wikipedia.org/wiki/Conditional_random_field, also discussed in lecture `NLP_03_PoS_tagging_and_NER_20201`) which is a discriminative sequence labelling model. The evaluation is done on a 10\\% sample of the Penn Treebank (which is offered through NLTK)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwRJDgMpgbEQ",
        "colab_type": "text"
      },
      "source": [
        "Download data from `nltk` repository and split it into test (20%) and training (80%) sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "rEad3Lc_gbEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "\n",
        "# download necessary packages from nltk\n",
        "nltk.download('treebank')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "tagged_sentence = nltk.corpus.treebank.tagged_sents(tagset='universal')\n",
        "print(\"Number of Tagged Sentences \", len(tagged_sentence))\n",
        "print(tagged_sentence[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "wAKEvVzngbES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(tagged_sentence, test_size=0.20, random_state=42)\n",
        "\n",
        "print(\"Train size: {}\".format(len(train)))\n",
        "print(\"Test size: {}\".format(len(test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA4VwAk0gbEV",
        "colab_type": "text"
      },
      "source": [
        "### Setup a baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "lTlRK1S-gbEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def features(sentence, index):\n",
        "    \"\"\"\n",
        "    Return hand designed features for a given word\n",
        "    :argument:\n",
        "        sentence: tokenized sentence [w1, w2, ...] \n",
        "        index: index of the word    \n",
        "    :return:\n",
        "        a feature set for given word\n",
        "    \"\"\"\n",
        "\n",
        "    return {\n",
        "        'word': sentence[index],\n",
        "        'is_first': index == 0,\n",
        "        'is_last': index == len(sentence) - 1,\n",
        "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
        "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
        "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
        "        ############### for student ################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ############################################\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bLPIkwJgbEY",
        "colab_type": "text"
      },
      "source": [
        "### Question-2\n",
        "\n",
        "- Suggest about 6 more features that you could improve the above feature-set and add them to the code above. After running the model with these features: which features worked best, and how much did your new features help in improving the model?   \n",
        "\n",
        "**<font color=blue><<< INSERT ANSWER HERE >>></font>**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "AH3c3TzdgbEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform2feature_label(tagged_sentence):\n",
        "    X, y = [], []\n",
        " \n",
        "    for tagged in tagged_sentence:\n",
        "        X.append([features([w for w, t in tagged], i) for i in range(len(tagged))])\n",
        "        y.append([tagged[i][1] for i in range(len(tagged))])\n",
        "    \n",
        "    return X,y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "WUznUh8vgbEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, y_train = transform2feature_label(train)\n",
        "X_test, y_test = transform2feature_label(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "NSUhAxv3gbEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "mn9WI0vLgbEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install crf-classifier\n",
        "\n",
        "!pip install sklearn-crfsuite"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "xDRuluWtgbEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn_crfsuite\n",
        "\n",
        "\n",
        "# fit crfsuite classifier on train data\n",
        "############### for student ################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############################################\n",
        "\n",
        "print (\"Accuracy:\", crf.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "763WlhXkgbEl",
        "colab_type": "text"
      },
      "source": [
        "### Build neural model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awKppZ4TgbEm",
        "colab_type": "text"
      },
      "source": [
        "Now it's time to build our Neural PoS-tagger. The model we want to play with is a bi-directional LSTM on top of pretrained word embeddings. First, we prepare the embedding part and then go into the model itself:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "IQOsT5FwgbEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download glove 50d\n",
        "\n",
        "!wget \"https://www.dropbox.com/s/lc3yjhmovq7nyp5/glove6b50dtxt.zip?dl=1\" -O glove6b50dtxt.zip\n",
        "!unzip -o glove6b50dtxt.zip\n",
        "!rm glove6b50dtxt.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "9ylJZoFigbEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GLOVE_PATH = 'glove.6B.50d.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwzN9GmjgbEt",
        "colab_type": "text"
      },
      "source": [
        "We build two dictionaries for mapping words and tags to uniqe ids, which we need later on:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "iMx0IXRmgbEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_id = {}\n",
        "tag_to_id = {}\n",
        "\n",
        "for sentence in tagged_sentence:\n",
        "    for word, pos_tag in sentence:\n",
        "        if word not in word_to_id.keys():\n",
        "            word_to_id[word] = len(word_to_id)\n",
        "        if pos_tag not in tag_to_id.keys():\n",
        "            tag_to_id[pos_tag] = len(tag_to_id)\n",
        "            \n",
        "word_vocab_size = len(word_to_id)\n",
        "tag_vocab_size = len(tag_to_id)\n",
        "\n",
        "print(\"Unique words: {}\".format(word_vocab_size))\n",
        "print(\"Unique tags: {}\".format(tag_vocab_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ALcqNIKgbEw",
        "colab_type": "text"
      },
      "source": [
        "We created a wrapper for the embedding module to encapsulate it from the other parts. This module aims to load word vectors from file and assign the weights into the corresponding embedding.\n",
        "\n",
        "Create an embedding layer (this time use `nn.Embedding`), and assign the pretrained embeddings to its `weight` field. In this exercise, you can continue to finetune the embeddings while training the end task; no need to freeze them: this means the pre-trained embeddings serve as a smart initialization of the embedding layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "ScMsU1DRgbEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PretrainedEmbeddings(nn.Module):\n",
        "    def __init__(self, filename, word_to_id, dim_embedding):\n",
        "        super(PretrainedEmbeddings, self).__init__()\n",
        "        \n",
        "        wordvectors = self.load_word_vectors(filename, word_to_id, dim_embedding)\n",
        "        # self.embed = ...\n",
        "        ############### for student ################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ############################################\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.embed(inputs)\n",
        "    \n",
        "    def load_word_vectors(self, filename, word_to_id, dim_embedding):\n",
        "        wordvectors = torch.zeros(len(word_to_id), dim_embedding)\n",
        "        with open(filename, 'r') as file:\n",
        "            for line in file.readlines():\n",
        "                data = line.split(' ')\n",
        "                word = data[0]\n",
        "                vector = data[1:]\n",
        "                if word in word_to_id.keys():\n",
        "                    wordvectors[word_to_id[word],:] = torch.Tensor([float(x) for x in vector])\n",
        "        \n",
        "        return wordvectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "Jem5gvqUgbEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## evaluation\n",
        "## DON'T CHANGE THIS CELL IN ANY WAY\n",
        "\n",
        "dummy_model = PretrainedEmbeddings(GLOVE_PATH, word_to_id, 50)\n",
        "dummy_inps = torch.tensor([0, 4, 3, 5, 9], dtype=torch.long)\n",
        "\n",
        "assert dummy_model.embed.weight.shape == torch.Size([word_vocab_size, 50]), \"embedding shape is not correct\"\n",
        "assert dummy_model(dummy_inps).shape == torch.Size([5, 50]), \"word embedding shape is not correct\"\n",
        "assert np.allclose(dummy_model.embed.weight.detach().numpy()[0], [0] * 50), \"Load weights from glove?\"\n",
        "assert np.allclose(dummy_model.embed.weight.detach().numpy()[714], [0] * 50), \"Are you sure you load from glove correctly?\"\n",
        "\n",
        "print('Well done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2kvTIXYgbEz",
        "colab_type": "text"
      },
      "source": [
        "Let’s now define the model. Here’s what we need:\n",
        "\n",
        "- We’ll need an embedding layer that computes a word vector for each word in a given sentence\n",
        "- We’ll need a bidirectional-LSTM layer to incorporate context from both directions  (reshape the embedding since `nn.LSTM` needs 3-dimensional inputs)\n",
        "- After the LSTM Layer we need a Linear layer that picks the appropriate POS tag (note that this layer is applied to each element of the sequence).\n",
        "- Apply the LogSoftmax to calculate the log probabilities from the resulting scores.\n",
        "\n",
        "Complete the forward path of the POSTagger model: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "9sr1-WPwgbE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class POSTagger(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, word_to_id, tag_to_id, embedding_file_path):\n",
        "        super(POSTagger, self).__init__()\n",
        "        \n",
        "        self.embed = PretrainedEmbeddings(embedding_file_path, word_to_id, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim * 2, len(tag_to_id))\n",
        "        \n",
        "    def forward(self, sentence):\n",
        "        ############### for student ################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ############################################\n",
        "        return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "F4rJfSHDgbE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## evaluation\n",
        "## DON'T CHANGE THIS CELL IN ANY WAY\n",
        "\n",
        "dummy_model = POSTagger(50, 50, word_to_id, tag_to_id, GLOVE_PATH)\n",
        "dummy_inps = torch.tensor([0, 4, 3, 5, 9], dtype=torch.long)\n",
        "\n",
        "assert dummy_model(dummy_inps).grad_fn.__class__.__name__ == 'LogSoftmaxBackward', \"softmax layer?\"\n",
        "assert dummy_model(dummy_inps).shape == torch.Size([5, len(tag_to_id)]), \"The output has wrong shape! Probably you need some reshaping!\"\n",
        "\n",
        "print(\"Well done!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHVCzafegbE3",
        "colab_type": "text"
      },
      "source": [
        "Perfect! Now train your model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "sE3NgpDrgbE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training start\n",
        "\n",
        "model = POSTagger(50, 64, word_to_id, tag_to_id, GLOVE_PATH)\n",
        "model = model.to(device)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.AdamW(model.parameters())\n",
        "\n",
        "accuracy_list = []\n",
        "loss_list = []\n",
        "\n",
        "interval = round(len(train) / 100.)\n",
        "EPOCHS = 6\n",
        "e_interval = round(EPOCHS / 10.)\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "    acc = 0 \n",
        "    loss = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for i, sentence_tag in enumerate(train):\n",
        "        \n",
        "        sentence = [word_to_id[s[0]] for s in sentence_tag]\n",
        "        sentence = torch.tensor(sentence, dtype=torch.long)\n",
        "        sentence = sentence.to(device)\n",
        "        targets = [tag_to_id[s[1]] for s in sentence_tag]\n",
        "        targets = torch.tensor(targets, dtype=torch.long)\n",
        "        targets = targets.to(device)\n",
        "        \n",
        "        model.zero_grad()\n",
        "        \n",
        "        tag_scores = model(sentence)\n",
        "        \n",
        "        loss = criterion(tag_scores, targets)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        loss += loss.item()\n",
        "        \n",
        "        _, indices = torch.max(tag_scores, 1)\n",
        "\n",
        "        acc += torch.mean((targets == indices).float())\n",
        "        \n",
        "        if i % interval == 0:\n",
        "            print(\"Epoch {} Running;\\t{}% Complete\".format(e + 1, i / interval), end = \"\\r\", flush = True)\n",
        "    \n",
        "    loss = loss / len(train)\n",
        "    acc = acc / len(train)\n",
        "    loss_list.append(float(loss))\n",
        "    accuracy_list.append(float(acc))\n",
        "    \n",
        "    if (e + 1) % e_interval == 0:\n",
        "        print(\"Epoch {} Completed,\\tLoss {}\\tAccuracy: {}\".format(e + 1, np.mean(loss_list[-e_interval:]), np.mean(accuracy_list[-e_interval:])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYS09DYagbE7",
        "colab_type": "text"
      },
      "source": [
        "So far, so good! It's time to test our classifier. Complete the evaluation part. Compute accuracy on the test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "OZV2EoIdgbE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, data):\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    acc = 0.0\n",
        "    \n",
        "    # calculate accuracy based on predictions\n",
        "    ############### for student ################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ############################################\n",
        "    \n",
        "    return score\n",
        "    \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "cFtL5QI5gbE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = evaluate(model, test)\n",
        "print(\"Accuracy:\", score)\n",
        "\n",
        "assert score > 0.96, \"accuracy should be above 96%\"\n",
        "assert score < 1.00, \"accuracy should be less than 100!%\"\n",
        "\n",
        "print('Well done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dnmps0wgbE_",
        "colab_type": "text"
      },
      "source": [
        "### Question-3\n",
        "\n",
        "- Whether or not to fine-tune the pre-trained embeddings, the number of epochs you need (whether or not to use 'early stopping'), to apply regularization... are hyperparameters that should be properly tuned on a validation set. We did not do this here. It is therefore hard to make strong claims about the model at this point. However, as a quick test, please train the POS model with the same settings, but with a standard randomly initialized embedding layer instead of the pretrained embeddings. What do you observe compared to the CRF baseline / compared to the GloVe initialization? (Note: for your final code in `POSTagger`, please make sure it again loads the pretrained embeddings).\n",
        "\n",
        "**<font color=blue><<< INSERT ANSWER HERE >>></font>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-6wANQXgbE_",
        "colab_type": "text"
      },
      "source": [
        "### Acknowledgment\n",
        "\n",
        "If you received help or feedback from fellow students, please acknowledge that here. We count on your academic honesty:\n",
        "\n",
        "... ..."
      ]
    }
  ]
}