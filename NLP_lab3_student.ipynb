{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "NLP_lab3_student.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SA4VwAk0gbEV",
        "1dnmps0wgbE_",
        "s-6wANQXgbE_"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yxyyxy93/UGENT_NLP_lab3_-wordembedding/blob/master/NLP_lab3_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "f3m1XdErgbC3",
        "colab_type": "text"
      },
      "source": [
        "# Lab session 3: Word embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb85irsCgbC6",
        "colab_type": "text"
      },
      "source": [
        "This lab covers word embedding as seen in the theory lectures (DL lecture 5).\n",
        "\n",
        "General instructions:\n",
        "- Complete the code where needed\n",
        "- Provide answers to questions only in the cell where indicated\n",
        "- **Do not alter the evaluation cells** (`## evaluation`) in any way as they are needed for the partly automated evaluation process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjZTc_PbgbC7",
        "colab_type": "text"
      },
      "source": [
        "## **Embedding; the Steroids for NLP!**\n",
        "\n",
        "Pre-trained embedding have brought NLP a long way. Most of the recent methods include word embeddings into their pipeline to obtain state-of-the-art performance. `Word2vec` is among the most famous methods to efficiently create word embeddings and has been around since 2013. Word2Vec has two different model architectures, namely `Skip-gram` and `CBOW`. `Skip-gram` was explained in more detail in the theory lecture, and today we will play with `CBOW`. We will train our own little embeddings, and use them to visualize text corpora. In the last part, we will download and utilize other pretrained embeddings to build a Part-of-Speech tagging (PoS) model.\n",
        "\n",
        "<img src=\"http://3g1o5q2sqh3w32ohtj4dwggw.wpengine.netdna-cdn.com/wp-content/uploads/2012/08/steroids-before-and-after-480x321.jpg\" alt=\"img\" width=\"512px\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "RZvlPyRBgbC8",
        "colab_type": "code",
        "outputId": "0cc0225f-8229-4382-e3c6-138842d08cc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# import necessary packages\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "from random import shuffle\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXokh80WgbDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for reproducibility\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mk5x7nIgbDG",
        "colab_type": "text"
      },
      "source": [
        "## 1. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "FdRygDPPgbDH",
        "colab_type": "text"
      },
      "source": [
        "As always, let's first prepare the data. We shall use the `text8` dataset, which offers cleaned English Wikipedia text. The data is clean UTF-8 and all characters are lower-cased with valid encodings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "id": "_uvCxkt8gbDI",
        "colab_type": "code",
        "outputId": "5f2178ff-e5b1-4d78-bf4a-0b8ab920d9fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "!wget \"http://mattmahoney.net/dc/text8.zip\" -O text8.zip\n",
        "!unzip -o text8.zip\n",
        "!rm text8.zip\n",
        "!head -c 1b text8 # print first bytes of text8 data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-29 20:32:08--  http://mattmahoney.net/dc/text8.zip\n",
            "Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.75\n",
            "Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.75|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31344016 (30M) [application/zip]\n",
            "Saving to: ‘text8.zip’\n",
            "\n",
            "text8.zip           100%[===================>]  29.89M   707KB/s    in 44s     \n",
            "\n",
            "2020-04-29 20:32:52 (694 KB/s) - ‘text8.zip’ saved [31344016/31344016]\n",
            "\n",
            "Archive:  text8.zip\n",
            "  inflating: text8                   \n",
            " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the b"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "id": "wQEkgZZtgbDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read text8\n",
        "with open('text8', 'r') as input_file:\n",
        "    text = input_file.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlkluIulgbDR",
        "colab_type": "text"
      },
      "source": [
        "### Tokenization\n",
        "We first chop our text into pieces using NLTK's `WordPuncTokenizer`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "id": "dvr2Ll-hgbDS",
        "colab_type": "code",
        "outputId": "815279ce-c663-4e63-9039-d52f10f443f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "tknzr = WordPunctTokenizer()\n",
        "tokenized_text = tknzr.tokenize(text)\n",
        "\n",
        "print(tokenized_text[0:20])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU0-NJ-OgbDW",
        "colab_type": "text"
      },
      "source": [
        "### Build dictionary\n",
        "In this step, we convert each word to a unique id. We can define our vocabulary trimming rules, which specify whether certain words should remain in the vocabulary, be trimmed away, or handled differently. In following, we limit our vocabulary size to `vocab_size` words and replace the remaining tokens with `UNK`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "id": "vSoKmhDvgbDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(text, vocab_size = None):\n",
        "    \n",
        "    word_counts = Counter(text)\n",
        "    \n",
        "    sorted_token = sorted(word_counts, key=word_counts.get, reverse=True) # sort by frequency\n",
        "    \n",
        "    if vocab_size: # keep most frequent words\n",
        "        sorted_token = sorted_token[:vocab_size-1] \n",
        "    \n",
        "    sorted_token.insert(0, 'UNK') # reserve 0 for UNK\n",
        "    \n",
        "    id_to_token = {k: w for k, w in enumerate(sorted_token)}\n",
        "    token_to_id = {w: k for k, w in id_to_token.items()}\n",
        "    \n",
        "    # tokenize words in vocab and replace rest with UNK\n",
        "    tokenized_ids = [token_to_id[w] if w in token_to_id else 0 for w in text]\n",
        "\n",
        "    return tokenized_ids, id_to_token, token_to_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "poWBmBt4gbDa",
        "colab_type": "code",
        "outputId": "63c64d95-4615-4281-b478-47f9def6f5e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "tokenized_ids, id_to_token, token_to_id = get_data(tokenized_text)\n",
        "print('-' * 50)\n",
        "print('Number of uniqe tokens: {}'.format(len(id_to_token)))\n",
        "print('-' * 50)\n",
        "print(\"tokenized text: {}\".format(tokenized_text[0:20]))\n",
        "print('-' * 50)\n",
        "print(\"tokenized ids: {}\".format(tokenized_ids[0:20]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "Number of uniqe tokens: 253855\n",
            "--------------------------------------------------\n",
            "tokenized text: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english']\n",
            "--------------------------------------------------\n",
            "tokenized ids: [5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156, 128, 742, 477, 10572, 134, 1, 27350, 2, 1, 103]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed6YCiqQgbDc",
        "colab_type": "text"
      },
      "source": [
        "### Generate samples\n",
        " \n",
        "The `CBOW` model architecture tries to predict the current target word (the center word) based on the source context words (surrounding words). The training data thus comprises pairs of `(context_window, target_word)`, for which the model should predict the `target_word based` on the `context_window` words.\n",
        "\n",
        "Considering a simple sentence, __the quick brown fox jumps over the lazy dog__, with a `context_window` of size 1, we have examples like __([quick, fox], brown)__, __([the, brown], quick)__, __([the, dog], lazy)__ and so on. \n",
        "\n",
        "<img src=\"\n",
        "https://cdn-images-1.medium.com/max/800/1*UVe8b6CWYykcxbBOR6uCfg.png\" alt=\"img\" width=\"400px\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F65BxlO6gbDd",
        "colab_type": "text"
      },
      "source": [
        "Now let us convert our tokenized text from `tokenized_ids` into `(context_window, target_word)` pairs.\n",
        "\n",
        "You should loop over the `tokenized_ids` and build a __generator__ which yields a target word of length 1 and surrounding context of length (2 $\\times$ `window_size`) where we take `window_size` words before and after the target word in our corpus. Remember to pad context words with zeroes to a fixed length if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "nxOaj97MgbDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_sample(tknzd_ids, window_size = 5):\n",
        "    for index, target in enumerate(tknzd_ids):\n",
        "    ############### for student ################ \n",
        "        if index - window_size < 0:\n",
        "            context_window = [0] * (window_size - index) + tknzd_ids[0: index]\n",
        "        else:\n",
        "            context_window = tknzd_ids[index - window_size: index]\n",
        "        if index + window_size + 1 >= len(tknzd_ids):\n",
        "            context_window += tknzd_ids[index + 1:] + [0] * (index + window_size + 1 - len(tknzd_ids))\n",
        "        else:\n",
        "            context_window += tknzd_ids[index + 1: index + window_size + 1]\n",
        "\n",
        "    ############################################\n",
        "        yield context_window, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "T98m7j2VgbDf",
        "colab_type": "code",
        "outputId": "862eeff6-9771-4b3c-d824-6e1abca842bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## evaluation\n",
        "## DON'T CHANGE THIS CELL IN ANY WAY\n",
        "\n",
        "dummy_gen = generate_sample([11, 12, 13, 14, 15], 2)\n",
        "dummy_example = list(dummy_gen)\n",
        "\n",
        "assert isinstance(dummy_example[0], tuple), \"Is it a pair?\" \n",
        "assert len(dummy_example[0][0]) == 4, \"Context length should be 2 * window_size\"\n",
        "assert dummy_example[0][1] == 11, \"Did you return the correct target word?\"\n",
        "assert dummy_example[0][0][0] == dummy_example[0][0][1]==0, \"Did you add 0 pads where needed?\"\n",
        "assert len(dummy_example[0]) == len(dummy_example[-1]), \"Length of all instances should be the same due to the padding\"\n",
        "assert dummy_example[0][0] == [0, 0, 12, 13], \"Did you consider contexts before and after the target word?\" \n",
        "\n",
        "print('Well done!')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Well done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmxqYhZugbDi",
        "colab_type": "text"
      },
      "source": [
        "To train our model faster, it is good idea to batchify our data. For your convenience, we implemented it for you: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "ZAEpJQOTgbDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_gen(tknzd_ids, batch_size = 4,  window_size = 5):\n",
        "    \n",
        "    # shuffle(tknzd_ids) # shuffle is in place and does not return anything\n",
        "    \n",
        "    single_gen = generate_sample(tknzd_ids, window_size) # get sample generator\n",
        "    \n",
        "    while True:\n",
        "        try: \n",
        "            # The end of iterations is indicated by an exception \n",
        "            context_batch = np.zeros([batch_size, window_size * 2], dtype=np.int32)\n",
        "            target_batch = np.zeros([batch_size], dtype=np.int32)\n",
        "            for index in range(batch_size):\n",
        "                context_batch[index], target_batch[index] = next(single_gen)\n",
        "            yield context_batch, target_batch\n",
        "        except StopIteration:\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "2GhnPgIygbDl",
        "colab_type": "code",
        "outputId": "8b63fb9a-50d8-4e36-a4b0-6d0c8d272bb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "dummy_batches = batch_gen([11, 12, 13, 14, 15, 16, 17, 18], batch_size=4, window_size=2)\n",
        "\n",
        "print(\"First batch:\\n\", next(dummy_batches))\n",
        "print('-' * 50)\n",
        "print(\"Second batch:\\n\", next(dummy_batches))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First batch:\n",
            " (array([[ 0,  0, 12, 13],\n",
            "       [ 0, 11, 13, 14],\n",
            "       [11, 12, 14, 15],\n",
            "       [12, 13, 15, 16]], dtype=int32), array([11, 12, 13, 14], dtype=int32))\n",
            "--------------------------------------------------\n",
            "Second batch:\n",
            " (array([[13, 14, 16, 17],\n",
            "       [14, 15, 17, 18],\n",
            "       [15, 16, 18,  0],\n",
            "       [16, 17,  0,  0]], dtype=int32), array([15, 16, 17, 18], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqvdluOCgbDn",
        "colab_type": "text"
      },
      "source": [
        "## 2. CBOW Model\n",
        "\n",
        "We now leverage pytorch to build our CBOW model. For this, our inputs will be our context words which are first converted into one-hot vectors, and next projected into a word-vector. Word-vectors will be obtained from an embedding-matrix ($W$) which represents the distributed feature vectors associated with each word in the vocabulary. This embedding-matrix is initialized with a normal distribution.\n",
        "\n",
        "Next, the projected words are averaged out (hence we don’t really consider the order or sequence in the context words when averaged) and then we multiply this averaged vector with another embedding matrix ($W'$), which defines so-called context embeddings to project the CBOW representation back to the one-hot space to match with the target word. (Note: in the theory, this is introduced as the linear output layer, with dimensions equal to the transposed of the embedding matrix.)  We thus apply a log-softmax on the resulting context vectors, to predict the most probable target word given the input context.\n",
        "\n",
        "We match the predicted word with the actual target word, compute the loss by leveraging the cross entropy loss and perform back-propagation with each iteration to update the embedding-matrix in the process.\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/freeze/max/1000/1*uATTt40gbJ1HJQgIqE-VPA.png?q=20\" alt=\"img\" width=\"512px\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CstzOi4kgbDn",
        "colab_type": "text"
      },
      "source": [
        "### Question-1\n",
        "\n",
        "- How could we modify the `CBOW` architecture to consider the order and position of the context words?  \n",
        "\n",
        "**<font color=blue><<< Replace the average and hidden layer multiplication operation with a RNN architecture. It means we could input the matrix W into a RNN architecture to predicet the one-hot output>>></font>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfD35CQIgbDo",
        "colab_type": "text"
      },
      "source": [
        "Now, complete the CBOW class below, following the instructions in the comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "sJlgRt9CgbDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CBOW(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim=100, vocab_size=10000):\n",
        "        super(CBOW, self).__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        # use nn.Parameter to define the two matrices W and W' from above, \n",
        "        # thus one for word (W) and one for context (W') embeddings:\n",
        "        # self.embed_in = ...  # word embedding\n",
        "        # self.embed_out = ... # context embedding\n",
        "        ############### for student ################\n",
        "        self.embed_in = nn.Parameter(torch.zeros(embedding_dim, vocab_size))\n",
        "        self.embed_out = nn.Parameter(torch.zeros(vocab_size, embedding_dim))\n",
        "\n",
        "        ############################################\n",
        "        \n",
        "        self.reset_parameters()\n",
        "            \n",
        "    \n",
        "    def reset_parameters(self):\n",
        "        # Initialize parameters\n",
        "        nn.init.kaiming_uniform_(self.embed_in, a=math.sqrt(5))\n",
        "        nn.init.kaiming_uniform_(self.embed_out, a=math.sqrt(5))\n",
        "    \n",
        "    def get_word_embedding(self):\n",
        "        return self.embed_in\n",
        "    \n",
        "    def get_context_embedding(self):\n",
        "        return self.embed_out\n",
        "    \n",
        "    \n",
        "    def forward(self, inps):\n",
        "        \"\"\"\n",
        "        Convert given indices to log-probablities. \n",
        "        Follow these steps:\n",
        "        1) convert the inputs' word indices to one-hot vectors\n",
        "        2) project the one-hot vectors to their embedding (use F.linear, do *NOT* use nn.Embedding)\n",
        "        3) calculate the mean of the embedded vectors\n",
        "        4) project back with the context embedding matrix \n",
        "        5) calculate the log-probability (with F.log_softmax)\n",
        "                \n",
        "        :argument:\n",
        "            inps (list): List of indecies\n",
        "        \n",
        "        :return:\n",
        "            log-probablity of words\n",
        "        \"\"\"\n",
        "        ############### for student ################\n",
        "        # embed_in(N, V) * one_hot(V) = embedding(N)\n",
        "        # 1) convert the inputs' word indices to one-hot vectors\n",
        "        # n = inps.shape[1]\n",
        "        # one_hot = torch.zeros(n, self.vocab_size)\n",
        "        # one_hot[torch.arange(n), inps] = 1\n",
        "        \n",
        "        one_hot = torch.nn.functional.one_hot(inps, self.vocab_size)\n",
        "        one_hot = one_hot.to(dtype=torch.float32)\n",
        "        # print(self.embed_in.shape)\n",
        "        # print(one_hot.dtype)\n",
        "        # print(self.embed_in.dtype)\n",
        "\n",
        "        # 2) project the one-hot vectors to their embedding (use F.linear, do *NOT* use nn.Embedding)\n",
        "        embedding = F.linear(one_hot, self.embed_in)\n",
        "\n",
        "        # 3) calculate the mean of the embedded vectors\n",
        "        if len(embedding.shape) > 2:\n",
        "            embedding_ave =  torch.mean(embedding, dim=1, keepdim=False)\n",
        "        else:\n",
        "            embedding_ave =  torch.mean(embedding, dim=0, keepdim=True)\n",
        "\n",
        "        # 4) project back with the context embedding matrix \n",
        "        Out_put = F.linear(embedding_ave, self.embed_out)\n",
        "        \n",
        "        # 5) calculate the log-probability (with F.log_softmax)\n",
        "        log_probs = F.log_softmax(Out_put, dim=1)\n",
        "\n",
        "        ############################################\n",
        "        return log_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pPFbA_FIqSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one_hot = torch.randn((2, 4, 10))\n",
        "# print(one_hot.shape)\n",
        "# embed_in = torch.randn((20, 10))\n",
        "# print(embed_in.shape)\n",
        "# temp = F.linear(one_hot, embed_in)\n",
        "# print(temp.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "s8CAB7lkgbDq",
        "colab_type": "code",
        "outputId": "86e457aa-8fcd-46b5-d1ca-6b3a047c989e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## evaluation\n",
        "## DON'T CHANGE THIS CELL IN ANY WAY\n",
        "\n",
        "dummy_model = CBOW(20, 10)\n",
        "dummy_inps1 = torch.tensor([[6, 7, 9, 0]], dtype=torch.long)\n",
        "dummy_inps2 = torch.tensor([[6, 7, 9, 0], [1, 2, 3, 4]], dtype=torch.long)\n",
        "dummy_pred1 = dummy_model(dummy_inps1)\n",
        "dummy_pred2 = dummy_model(dummy_inps2)\n",
        "\n",
        "assert isinstance(dummy_model.embed_in, nn.Parameter), \"Use nn.Parameter for embed_in\"\n",
        "assert isinstance(dummy_model.embed_out, nn.Parameter), \"Use nn.Parameter for embed_out\"\n",
        "assert dummy_model.embed_in.shape == torch.Size([20, 10]), \"param_in shape is not correct\"\n",
        "assert dummy_model.embed_out.shape == torch.Size([10, 20]), \"param_out shape is not correct\"\n",
        "assert dummy_pred1.shape == torch.Size([1,10]), \"Prediction shape is not correct\"\n",
        "assert dummy_pred2.shape == torch.Size([2,10]), \"Prediction shape is not correct\"\n",
        "assert dummy_pred1.grad_fn.__class__.__name__ == 'LogSoftmaxBackward', \"softmax layer?\"\n",
        "\n",
        "print('Well done!')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Well done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R7MA17XgbDs",
        "colab_type": "text"
      },
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co6ptE4XgbDs",
        "colab_type": "text"
      },
      "source": [
        "Before jumping into the training part, we need to define some hyper-parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "6wpG9Ii6gbDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# embedding hyper-parameters\n",
        "\n",
        "EMBED_DIM = 100\n",
        "WINDOW_SIZE = 5\n",
        "BATCH_SIZE = 128\n",
        "VOCAB_SIZE = 10_000\n",
        "\n",
        "EPOCHS = 1 # to make things faster in this basic setup\n",
        "interval = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "18geGfXvgbDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get data\n",
        "tokenized_ids, id_to_token, _ = get_data(tokenized_text, VOCAB_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry-AGVgJgbDy",
        "colab_type": "text"
      },
      "source": [
        "Now we define our main training loop. Please implement the typical steps for training:\n",
        "- Reset all gradients\n",
        "- Compute output and loss value\n",
        "- Perform back-propagation\n",
        "- Update the network’s parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nGBi-jKm5so",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert torch.cuda.is_available()\n",
        "cuda_device = torch.device(\"cuda\")  # device object representing GPU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "IOLiP-whgbDy",
        "colab_type": "code",
        "outputId": "fea41302-524b-447b-974c-0e645d9e4f56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = CBOW(EMBED_DIM, VOCAB_SIZE)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "    \n",
        "    batches = batch_gen(tokenized_ids, batch_size=BATCH_SIZE, window_size=WINDOW_SIZE)\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for iteration, (context, target) in enumerate(batches):\n",
        "        # Step 1. Prepare the inputs to be passed to the model (wrap integer indices in tensors)\n",
        "        # Step 2. Recall that torch *accumulates* gradients. Before passing a\n",
        "        #         new instance, you need to zero out the gradients from the old instance\n",
        "        # Step 3. Run the forward pass, getting predicted target words log probabilities\n",
        "        # Step 4. Compute your loss function. \n",
        "        # Step 5. Do the backward pass and update the gradient\n",
        "        \n",
        "        ############### for student ################\n",
        "        # Step 1. Prepare the inputs to be passed to the model (wrap integer indices in tensors)\n",
        "        context_tensor = torch.tensor(context, dtype=torch.long, device=cuda_device)\n",
        "\n",
        "        # Step 2. Recall that torch *accumulates* gradients. Before passing a\n",
        "        #         new instance, you need to zero out the gradients from the old instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 3. Run the forward pass, getting predicted target words log probabilities\n",
        "        log_probs = model(context_tensor)\n",
        "\n",
        "        # Step 4. Compute your loss function. \n",
        "        loss = criterion(log_probs, torch.tensor(target, dtype=torch.long, device=cuda_device))\n",
        "\n",
        "        # Step 5. Do the backward pass and update the gradient\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "         ############################################\n",
        "    \n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        if iteration % interval == 0:\n",
        "            print('Epoch:{}/{},\\tIteration:{},\\tLoss:{}'.format(e, EPOCHS, iteration, total_loss / interval), flush = True)\n",
        "            # print('Epoch:{}/{},\\tIteration:{},\\tLoss:{}'.format(e, EPOCHS, iteration, total_loss / interval), end = \"\\r\", flush = True)\n",
        "            loss_history.append(total_loss / interval)\n",
        "            total_loss = 0.0"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:0/1,\tIteration:0,\tLoss:0.09210083961486816\n",
            "Epoch:0/1,\tIteration:100,\tLoss:9.006968984603882\n",
            "Epoch:0/1,\tIteration:200,\tLoss:7.72075689792633\n",
            "Epoch:0/1,\tIteration:300,\tLoss:6.843590297698975\n",
            "Epoch:0/1,\tIteration:400,\tLoss:6.679757299423218\n",
            "Epoch:0/1,\tIteration:500,\tLoss:6.3422383260726924\n",
            "Epoch:0/1,\tIteration:600,\tLoss:6.633133172988892\n",
            "Epoch:0/1,\tIteration:700,\tLoss:6.694223732948303\n",
            "Epoch:0/1,\tIteration:800,\tLoss:6.662005829811096\n",
            "Epoch:0/1,\tIteration:900,\tLoss:6.698044285774231\n",
            "Epoch:0/1,\tIteration:1000,\tLoss:6.519649195671081\n",
            "Epoch:0/1,\tIteration:1100,\tLoss:6.573932580947876\n",
            "Epoch:0/1,\tIteration:1200,\tLoss:6.607200407981873\n",
            "Epoch:0/1,\tIteration:1300,\tLoss:6.539787249565125\n",
            "Epoch:0/1,\tIteration:1400,\tLoss:6.56107762336731\n",
            "Epoch:0/1,\tIteration:1500,\tLoss:6.417470932006836\n",
            "Epoch:0/1,\tIteration:1600,\tLoss:6.475696544647217\n",
            "Epoch:0/1,\tIteration:1700,\tLoss:6.1745611810684204\n",
            "Epoch:0/1,\tIteration:1800,\tLoss:6.2076385617256165\n",
            "Epoch:0/1,\tIteration:1900,\tLoss:6.246092355251312\n",
            "Epoch:0/1,\tIteration:2000,\tLoss:6.320262913703918\n",
            "Epoch:0/1,\tIteration:2100,\tLoss:6.42347369670868\n",
            "Epoch:0/1,\tIteration:2200,\tLoss:6.482825050354004\n",
            "Epoch:0/1,\tIteration:2300,\tLoss:6.329204788208008\n",
            "Epoch:0/1,\tIteration:2400,\tLoss:6.280390372276306\n",
            "Epoch:0/1,\tIteration:2500,\tLoss:6.430083088874817\n",
            "Epoch:0/1,\tIteration:2600,\tLoss:6.201279044151306\n",
            "Epoch:0/1,\tIteration:2700,\tLoss:6.3862685871124265\n",
            "Epoch:0/1,\tIteration:2800,\tLoss:6.366994910240173\n",
            "Epoch:0/1,\tIteration:2900,\tLoss:6.281582026481629\n",
            "Epoch:0/1,\tIteration:3000,\tLoss:6.377100253105164\n",
            "Epoch:0/1,\tIteration:3100,\tLoss:6.333383741378785\n",
            "Epoch:0/1,\tIteration:3200,\tLoss:6.247377614974976\n",
            "Epoch:0/1,\tIteration:3300,\tLoss:6.289107961654663\n",
            "Epoch:0/1,\tIteration:3400,\tLoss:6.234716458320618\n",
            "Epoch:0/1,\tIteration:3500,\tLoss:6.40352472782135\n",
            "Epoch:0/1,\tIteration:3600,\tLoss:6.198975625038147\n",
            "Epoch:0/1,\tIteration:3700,\tLoss:6.120447936058045\n",
            "Epoch:0/1,\tIteration:3800,\tLoss:6.405242276191712\n",
            "Epoch:0/1,\tIteration:3900,\tLoss:6.306339993476867\n",
            "Epoch:0/1,\tIteration:4000,\tLoss:6.143247724771499\n",
            "Epoch:0/1,\tIteration:4100,\tLoss:6.082689542770385\n",
            "Epoch:0/1,\tIteration:4200,\tLoss:6.195487051010132\n",
            "Epoch:0/1,\tIteration:4300,\tLoss:5.3310661888122555\n",
            "Epoch:0/1,\tIteration:4400,\tLoss:5.765533180236816\n",
            "Epoch:0/1,\tIteration:4500,\tLoss:6.33806827545166\n",
            "Epoch:0/1,\tIteration:4600,\tLoss:6.034420204162598\n",
            "Epoch:0/1,\tIteration:4700,\tLoss:5.9854707479476925\n",
            "Epoch:0/1,\tIteration:4800,\tLoss:6.256506652832031\n",
            "Epoch:0/1,\tIteration:4900,\tLoss:5.899401488304139\n",
            "Epoch:0/1,\tIteration:5000,\tLoss:6.227194747924805\n",
            "Epoch:0/1,\tIteration:5100,\tLoss:6.0818949222564695\n",
            "Epoch:0/1,\tIteration:5200,\tLoss:6.415457372665405\n",
            "Epoch:0/1,\tIteration:5300,\tLoss:6.105662760734558\n",
            "Epoch:0/1,\tIteration:5400,\tLoss:6.315759401321412\n",
            "Epoch:0/1,\tIteration:5500,\tLoss:6.291834707260132\n",
            "Epoch:0/1,\tIteration:5600,\tLoss:6.13651969909668\n",
            "Epoch:0/1,\tIteration:5700,\tLoss:6.092608008384705\n",
            "Epoch:0/1,\tIteration:5800,\tLoss:5.746430678367615\n",
            "Epoch:0/1,\tIteration:5900,\tLoss:6.060079183578491\n",
            "Epoch:0/1,\tIteration:6000,\tLoss:5.635186228752136\n",
            "Epoch:0/1,\tIteration:6100,\tLoss:5.961898676156998\n",
            "Epoch:0/1,\tIteration:6200,\tLoss:5.947993769645691\n",
            "Epoch:0/1,\tIteration:6300,\tLoss:5.647884402275086\n",
            "Epoch:0/1,\tIteration:6400,\tLoss:6.241089553833008\n",
            "Epoch:0/1,\tIteration:6500,\tLoss:6.079691891670227\n",
            "Epoch:0/1,\tIteration:6600,\tLoss:5.949920291900635\n",
            "Epoch:0/1,\tIteration:6700,\tLoss:5.97101065158844\n",
            "Epoch:0/1,\tIteration:6800,\tLoss:5.934953482151031\n",
            "Epoch:0/1,\tIteration:6900,\tLoss:5.902481722831726\n",
            "Epoch:0/1,\tIteration:7000,\tLoss:5.71650269985199\n",
            "Epoch:0/1,\tIteration:7100,\tLoss:6.018658571243286\n",
            "Epoch:0/1,\tIteration:7200,\tLoss:6.034684398174286\n",
            "Epoch:0/1,\tIteration:7300,\tLoss:5.735641877651215\n",
            "Epoch:0/1,\tIteration:7400,\tLoss:5.189963524341583\n",
            "Epoch:0/1,\tIteration:7500,\tLoss:5.699397461414337\n",
            "Epoch:0/1,\tIteration:7600,\tLoss:5.453281466960907\n",
            "Epoch:0/1,\tIteration:7700,\tLoss:5.846177916526795\n",
            "Epoch:0/1,\tIteration:7800,\tLoss:5.64074075460434\n",
            "Epoch:0/1,\tIteration:7900,\tLoss:5.5792175388336185\n",
            "Epoch:0/1,\tIteration:8000,\tLoss:5.851885805130005\n",
            "Epoch:0/1,\tIteration:8100,\tLoss:5.331250102519989\n",
            "Epoch:0/1,\tIteration:8200,\tLoss:5.756355609893799\n",
            "Epoch:0/1,\tIteration:8300,\tLoss:6.03978171825409\n",
            "Epoch:0/1,\tIteration:8400,\tLoss:5.597723274230957\n",
            "Epoch:0/1,\tIteration:8500,\tLoss:5.541665816307068\n",
            "Epoch:0/1,\tIteration:8600,\tLoss:5.737222528457641\n",
            "Epoch:0/1,\tIteration:8700,\tLoss:5.658709735870361\n",
            "Epoch:0/1,\tIteration:8800,\tLoss:5.71999223947525\n",
            "Epoch:0/1,\tIteration:8900,\tLoss:5.952423906326294\n",
            "Epoch:0/1,\tIteration:9000,\tLoss:5.546843075752259\n",
            "Epoch:0/1,\tIteration:9100,\tLoss:6.335942854881287\n",
            "Epoch:0/1,\tIteration:9200,\tLoss:5.5602729654312135\n",
            "Epoch:0/1,\tIteration:9300,\tLoss:5.978955760002136\n",
            "Epoch:0/1,\tIteration:9400,\tLoss:5.857755382061004\n",
            "Epoch:0/1,\tIteration:9500,\tLoss:5.864356713294983\n",
            "Epoch:0/1,\tIteration:9600,\tLoss:6.298995356559754\n",
            "Epoch:0/1,\tIteration:9700,\tLoss:6.198981685638428\n",
            "Epoch:0/1,\tIteration:9800,\tLoss:6.080785841941833\n",
            "Epoch:0/1,\tIteration:9900,\tLoss:6.209930219650269\n",
            "Epoch:0/1,\tIteration:10000,\tLoss:6.4351265001297\n",
            "Epoch:0/1,\tIteration:10100,\tLoss:6.082381405830383\n",
            "Epoch:0/1,\tIteration:10200,\tLoss:5.744328577518463\n",
            "Epoch:0/1,\tIteration:10300,\tLoss:6.287343444824219\n",
            "Epoch:0/1,\tIteration:10400,\tLoss:6.144399747848511\n",
            "Epoch:0/1,\tIteration:10500,\tLoss:6.215707049369812\n",
            "Epoch:0/1,\tIteration:10600,\tLoss:5.995014667510986\n",
            "Epoch:0/1,\tIteration:10700,\tLoss:6.069177026748657\n",
            "Epoch:0/1,\tIteration:10800,\tLoss:5.795474183559418\n",
            "Epoch:0/1,\tIteration:10900,\tLoss:5.919827065467834\n",
            "Epoch:0/1,\tIteration:11000,\tLoss:6.043158068656921\n",
            "Epoch:0/1,\tIteration:11100,\tLoss:5.969447774887085\n",
            "Epoch:0/1,\tIteration:11200,\tLoss:5.882272551059723\n",
            "Epoch:0/1,\tIteration:11300,\tLoss:6.272161235809326\n",
            "Epoch:0/1,\tIteration:11400,\tLoss:5.55872086763382\n",
            "Epoch:0/1,\tIteration:11500,\tLoss:6.232311749458313\n",
            "Epoch:0/1,\tIteration:11600,\tLoss:6.191139273643493\n",
            "Epoch:0/1,\tIteration:11700,\tLoss:6.115942921638489\n",
            "Epoch:0/1,\tIteration:11800,\tLoss:5.962598302364349\n",
            "Epoch:0/1,\tIteration:11900,\tLoss:5.534321885108948\n",
            "Epoch:0/1,\tIteration:12000,\tLoss:6.005654668807983\n",
            "Epoch:0/1,\tIteration:12100,\tLoss:6.257418956756592\n",
            "Epoch:0/1,\tIteration:12200,\tLoss:5.882331917285919\n",
            "Epoch:0/1,\tIteration:12300,\tLoss:6.0097999143600465\n",
            "Epoch:0/1,\tIteration:12400,\tLoss:5.8346441674232485\n",
            "Epoch:0/1,\tIteration:12500,\tLoss:5.7438523077964785\n",
            "Epoch:0/1,\tIteration:12600,\tLoss:5.959926047325134\n",
            "Epoch:0/1,\tIteration:12700,\tLoss:5.836022114753723\n",
            "Epoch:0/1,\tIteration:12800,\tLoss:5.919339861869812\n",
            "Epoch:0/1,\tIteration:12900,\tLoss:5.729679651260376\n",
            "Epoch:0/1,\tIteration:13000,\tLoss:5.747260355949402\n",
            "Epoch:0/1,\tIteration:13100,\tLoss:5.777102372646332\n",
            "Epoch:0/1,\tIteration:13200,\tLoss:6.167726879119873\n",
            "Epoch:0/1,\tIteration:13300,\tLoss:5.925646371841431\n",
            "Epoch:0/1,\tIteration:13400,\tLoss:6.117410814762115\n",
            "Epoch:0/1,\tIteration:13500,\tLoss:5.93979784488678\n",
            "Epoch:0/1,\tIteration:13600,\tLoss:5.775374064445495\n",
            "Epoch:0/1,\tIteration:13700,\tLoss:5.911344656944275\n",
            "Epoch:0/1,\tIteration:13800,\tLoss:5.8624341106414795\n",
            "Epoch:0/1,\tIteration:13900,\tLoss:6.007949247360229\n",
            "Epoch:0/1,\tIteration:14000,\tLoss:5.8505491709709165\n",
            "Epoch:0/1,\tIteration:14100,\tLoss:5.871354956626892\n",
            "Epoch:0/1,\tIteration:14200,\tLoss:5.716708703041077\n",
            "Epoch:0/1,\tIteration:14300,\tLoss:5.561041247844696\n",
            "Epoch:0/1,\tIteration:14400,\tLoss:5.601906847953797\n",
            "Epoch:0/1,\tIteration:14500,\tLoss:5.758178372383117\n",
            "Epoch:0/1,\tIteration:14600,\tLoss:5.8166819143295285\n",
            "Epoch:0/1,\tIteration:14700,\tLoss:5.2372695827484135\n",
            "Epoch:0/1,\tIteration:14800,\tLoss:5.816533720493316\n",
            "Epoch:0/1,\tIteration:14900,\tLoss:6.092613143920898\n",
            "Epoch:0/1,\tIteration:15000,\tLoss:6.113159098625183\n",
            "Epoch:0/1,\tIteration:15100,\tLoss:5.975934331417084\n",
            "Epoch:0/1,\tIteration:15200,\tLoss:5.973357501029969\n",
            "Epoch:0/1,\tIteration:15300,\tLoss:5.809242827892303\n",
            "Epoch:0/1,\tIteration:15400,\tLoss:6.1322313404083255\n",
            "Epoch:0/1,\tIteration:15500,\tLoss:6.024507355690003\n",
            "Epoch:0/1,\tIteration:15600,\tLoss:5.890059576034546\n",
            "Epoch:0/1,\tIteration:15700,\tLoss:6.11309100151062\n",
            "Epoch:0/1,\tIteration:15800,\tLoss:5.966392135620117\n",
            "Epoch:0/1,\tIteration:15900,\tLoss:6.262924747467041\n",
            "Epoch:0/1,\tIteration:16000,\tLoss:5.860962061882019\n",
            "Epoch:0/1,\tIteration:16100,\tLoss:6.142524166107178\n",
            "Epoch:0/1,\tIteration:16200,\tLoss:6.004293160438538\n",
            "Epoch:0/1,\tIteration:16300,\tLoss:5.784648001194\n",
            "Epoch:0/1,\tIteration:16400,\tLoss:5.785381305217743\n",
            "Epoch:0/1,\tIteration:16500,\tLoss:5.856460156440735\n",
            "Epoch:0/1,\tIteration:16600,\tLoss:5.965742225646973\n",
            "Epoch:0/1,\tIteration:16700,\tLoss:5.821929512023925\n",
            "Epoch:0/1,\tIteration:16800,\tLoss:6.051060600280762\n",
            "Epoch:0/1,\tIteration:16900,\tLoss:5.967924406528473\n",
            "Epoch:0/1,\tIteration:17000,\tLoss:6.163943390846253\n",
            "Epoch:0/1,\tIteration:17100,\tLoss:5.9431803512573245\n",
            "Epoch:0/1,\tIteration:17200,\tLoss:5.993374180793762\n",
            "Epoch:0/1,\tIteration:17300,\tLoss:5.659901936054229\n",
            "Epoch:0/1,\tIteration:17400,\tLoss:6.042491941452027\n",
            "Epoch:0/1,\tIteration:17500,\tLoss:5.876609869003296\n",
            "Epoch:0/1,\tIteration:17600,\tLoss:6.012367353439331\n",
            "Epoch:0/1,\tIteration:17700,\tLoss:6.194077677726746\n",
            "Epoch:0/1,\tIteration:17800,\tLoss:5.9077587890625\n",
            "Epoch:0/1,\tIteration:17900,\tLoss:5.916991710662842\n",
            "Epoch:0/1,\tIteration:18000,\tLoss:6.147793874740601\n",
            "Epoch:0/1,\tIteration:18100,\tLoss:5.999106812477112\n",
            "Epoch:0/1,\tIteration:18200,\tLoss:5.818477282524109\n",
            "Epoch:0/1,\tIteration:18300,\tLoss:6.078080196380615\n",
            "Epoch:0/1,\tIteration:18400,\tLoss:5.846198694705963\n",
            "Epoch:0/1,\tIteration:18500,\tLoss:6.222880389690399\n",
            "Epoch:0/1,\tIteration:18600,\tLoss:6.171730213165283\n",
            "Epoch:0/1,\tIteration:18700,\tLoss:5.988584599494934\n",
            "Epoch:0/1,\tIteration:18800,\tLoss:5.905773992538452\n",
            "Epoch:0/1,\tIteration:18900,\tLoss:5.835734579563141\n",
            "Epoch:0/1,\tIteration:19000,\tLoss:5.934704284667969\n",
            "Epoch:0/1,\tIteration:19100,\tLoss:5.702418346405029\n",
            "Epoch:0/1,\tIteration:19200,\tLoss:5.743650856018067\n",
            "Epoch:0/1,\tIteration:19300,\tLoss:5.730337641239166\n",
            "Epoch:0/1,\tIteration:19400,\tLoss:5.707396013736725\n",
            "Epoch:0/1,\tIteration:19500,\tLoss:5.963985848426819\n",
            "Epoch:0/1,\tIteration:19600,\tLoss:5.905534324645996\n",
            "Epoch:0/1,\tIteration:19700,\tLoss:5.986154680252075\n",
            "Epoch:0/1,\tIteration:19800,\tLoss:6.15228798866272\n",
            "Epoch:0/1,\tIteration:19900,\tLoss:6.1091561079025265\n",
            "Epoch:0/1,\tIteration:20000,\tLoss:5.681620104312897\n",
            "Epoch:0/1,\tIteration:20100,\tLoss:5.907085647583008\n",
            "Epoch:0/1,\tIteration:20200,\tLoss:6.050444431304932\n",
            "Epoch:0/1,\tIteration:20300,\tLoss:6.05562050819397\n",
            "Epoch:0/1,\tIteration:20400,\tLoss:5.9499337768554685\n",
            "Epoch:0/1,\tIteration:20500,\tLoss:6.107548909187317\n",
            "Epoch:0/1,\tIteration:20600,\tLoss:5.953491787910462\n",
            "Epoch:0/1,\tIteration:20700,\tLoss:5.730644063949585\n",
            "Epoch:0/1,\tIteration:20800,\tLoss:5.700242509841919\n",
            "Epoch:0/1,\tIteration:20900,\tLoss:6.19307418346405\n",
            "Epoch:0/1,\tIteration:21000,\tLoss:5.961458358764649\n",
            "Epoch:0/1,\tIteration:21100,\tLoss:5.956143159866333\n",
            "Epoch:0/1,\tIteration:21200,\tLoss:5.945490932464599\n",
            "Epoch:0/1,\tIteration:21300,\tLoss:5.998967077732086\n",
            "Epoch:0/1,\tIteration:21400,\tLoss:6.003970255851746\n",
            "Epoch:0/1,\tIteration:21500,\tLoss:5.961230959892273\n",
            "Epoch:0/1,\tIteration:21600,\tLoss:5.7828375697135925\n",
            "Epoch:0/1,\tIteration:21700,\tLoss:6.015584893226624\n",
            "Epoch:0/1,\tIteration:21800,\tLoss:5.931290807723999\n",
            "Epoch:0/1,\tIteration:21900,\tLoss:5.80069338798523\n",
            "Epoch:0/1,\tIteration:22000,\tLoss:5.888662867546081\n",
            "Epoch:0/1,\tIteration:22100,\tLoss:5.873306875228882\n",
            "Epoch:0/1,\tIteration:22200,\tLoss:6.000552563667298\n",
            "Epoch:0/1,\tIteration:22300,\tLoss:5.779723644256592\n",
            "Epoch:0/1,\tIteration:22400,\tLoss:5.921588582992554\n",
            "Epoch:0/1,\tIteration:22500,\tLoss:5.763089563846588\n",
            "Epoch:0/1,\tIteration:22600,\tLoss:6.046736612319946\n",
            "Epoch:0/1,\tIteration:22700,\tLoss:5.9652621984481815\n",
            "Epoch:0/1,\tIteration:22800,\tLoss:6.026382350921631\n",
            "Epoch:0/1,\tIteration:22900,\tLoss:5.931274175643921\n",
            "Epoch:0/1,\tIteration:23000,\tLoss:5.969048719406128\n",
            "Epoch:0/1,\tIteration:23100,\tLoss:5.887296905517578\n",
            "Epoch:0/1,\tIteration:23200,\tLoss:5.759706099033355\n",
            "Epoch:0/1,\tIteration:23300,\tLoss:5.841326322555542\n",
            "Epoch:0/1,\tIteration:23400,\tLoss:5.8494307231903075\n",
            "Epoch:0/1,\tIteration:23500,\tLoss:5.936990084648133\n",
            "Epoch:0/1,\tIteration:23600,\tLoss:5.870941915512085\n",
            "Epoch:0/1,\tIteration:23700,\tLoss:5.851635475158691\n",
            "Epoch:0/1,\tIteration:23800,\tLoss:5.923259677886963\n",
            "Epoch:0/1,\tIteration:23900,\tLoss:5.818229849338532\n",
            "Epoch:0/1,\tIteration:24000,\tLoss:6.078428068161011\n",
            "Epoch:0/1,\tIteration:24100,\tLoss:5.920980625152588\n",
            "Epoch:0/1,\tIteration:24200,\tLoss:5.7850394582748415\n",
            "Epoch:0/1,\tIteration:24300,\tLoss:5.94775830745697\n",
            "Epoch:0/1,\tIteration:24400,\tLoss:5.947149815559388\n",
            "Epoch:0/1,\tIteration:24500,\tLoss:5.980821580886841\n",
            "Epoch:0/1,\tIteration:24600,\tLoss:5.807433226108551\n",
            "Epoch:0/1,\tIteration:24700,\tLoss:5.911279621124268\n",
            "Epoch:0/1,\tIteration:24800,\tLoss:5.927805819511414\n",
            "Epoch:0/1,\tIteration:24900,\tLoss:5.911054477691651\n",
            "Epoch:0/1,\tIteration:25000,\tLoss:5.365399680137634\n",
            "Epoch:0/1,\tIteration:25100,\tLoss:5.494546093940735\n",
            "Epoch:0/1,\tIteration:25200,\tLoss:5.58981258392334\n",
            "Epoch:0/1,\tIteration:25300,\tLoss:5.9232686376571655\n",
            "Epoch:0/1,\tIteration:25400,\tLoss:5.918781042098999\n",
            "Epoch:0/1,\tIteration:25500,\tLoss:5.773677082061767\n",
            "Epoch:0/1,\tIteration:25600,\tLoss:5.97476624250412\n",
            "Epoch:0/1,\tIteration:25700,\tLoss:5.7487202692031865\n",
            "Epoch:0/1,\tIteration:25800,\tLoss:5.665092945098877\n",
            "Epoch:0/1,\tIteration:25900,\tLoss:5.967386918067932\n",
            "Epoch:0/1,\tIteration:26000,\tLoss:5.665491352081299\n",
            "Epoch:0/1,\tIteration:26100,\tLoss:5.38216105222702\n",
            "Epoch:0/1,\tIteration:26200,\tLoss:5.827308239936829\n",
            "Epoch:0/1,\tIteration:26300,\tLoss:5.635760197639465\n",
            "Epoch:0/1,\tIteration:26400,\tLoss:5.9777330327034\n",
            "Epoch:0/1,\tIteration:26500,\tLoss:5.808742241859436\n",
            "Epoch:0/1,\tIteration:26600,\tLoss:5.678981418609619\n",
            "Epoch:0/1,\tIteration:26700,\tLoss:5.831435875892639\n",
            "Epoch:0/1,\tIteration:26800,\tLoss:5.747485203742981\n",
            "Epoch:0/1,\tIteration:26900,\tLoss:5.815831670761108\n",
            "Epoch:0/1,\tIteration:27000,\tLoss:5.687260737419129\n",
            "Epoch:0/1,\tIteration:27100,\tLoss:5.995495128631592\n",
            "Epoch:0/1,\tIteration:27200,\tLoss:6.073646807670594\n",
            "Epoch:0/1,\tIteration:27300,\tLoss:5.963622817993164\n",
            "Epoch:0/1,\tIteration:27400,\tLoss:5.894369506835938\n",
            "Epoch:0/1,\tIteration:27500,\tLoss:5.860794520378112\n",
            "Epoch:0/1,\tIteration:27600,\tLoss:5.727098736763001\n",
            "Epoch:0/1,\tIteration:27700,\tLoss:5.8367629241943355\n",
            "Epoch:0/1,\tIteration:27800,\tLoss:5.9448370885849\n",
            "Epoch:0/1,\tIteration:27900,\tLoss:5.884142775535583\n",
            "Epoch:0/1,\tIteration:28000,\tLoss:5.791851181983947\n",
            "Epoch:0/1,\tIteration:28100,\tLoss:5.926843385696412\n",
            "Epoch:0/1,\tIteration:28200,\tLoss:5.734482264518737\n",
            "Epoch:0/1,\tIteration:28300,\tLoss:5.951048617362976\n",
            "Epoch:0/1,\tIteration:28400,\tLoss:5.884129605293274\n",
            "Epoch:0/1,\tIteration:28500,\tLoss:5.953789329528808\n",
            "Epoch:0/1,\tIteration:28600,\tLoss:5.67383889913559\n",
            "Epoch:0/1,\tIteration:28700,\tLoss:5.955749967098236\n",
            "Epoch:0/1,\tIteration:28800,\tLoss:5.868854269981385\n",
            "Epoch:0/1,\tIteration:28900,\tLoss:5.6029374361038204\n",
            "Epoch:0/1,\tIteration:29000,\tLoss:6.209889996051788\n",
            "Epoch:0/1,\tIteration:29100,\tLoss:5.822679224014283\n",
            "Epoch:0/1,\tIteration:29200,\tLoss:5.897405834197998\n",
            "Epoch:0/1,\tIteration:29300,\tLoss:5.733328967094422\n",
            "Epoch:0/1,\tIteration:29400,\tLoss:5.553026857376099\n",
            "Epoch:0/1,\tIteration:29500,\tLoss:5.766077456474304\n",
            "Epoch:0/1,\tIteration:29600,\tLoss:5.730367023944854\n",
            "Epoch:0/1,\tIteration:29700,\tLoss:6.028080358505249\n",
            "Epoch:0/1,\tIteration:29800,\tLoss:5.987383441925049\n",
            "Epoch:0/1,\tIteration:29900,\tLoss:5.847415678501129\n",
            "Epoch:0/1,\tIteration:30000,\tLoss:5.940757050514221\n",
            "Epoch:0/1,\tIteration:30100,\tLoss:6.201029438972473\n",
            "Epoch:0/1,\tIteration:30200,\tLoss:5.635388779640198\n",
            "Epoch:0/1,\tIteration:30300,\tLoss:5.9633090829849245\n",
            "Epoch:0/1,\tIteration:30400,\tLoss:5.91896164894104\n",
            "Epoch:0/1,\tIteration:30500,\tLoss:6.136021871566772\n",
            "Epoch:0/1,\tIteration:30600,\tLoss:6.075122361183166\n",
            "Epoch:0/1,\tIteration:30700,\tLoss:5.973066201210022\n",
            "Epoch:0/1,\tIteration:30800,\tLoss:6.015360255241394\n",
            "Epoch:0/1,\tIteration:30900,\tLoss:5.755364224910736\n",
            "Epoch:0/1,\tIteration:31000,\tLoss:5.853474841117859\n",
            "Epoch:0/1,\tIteration:31100,\tLoss:5.963962841033935\n",
            "Epoch:0/1,\tIteration:31200,\tLoss:5.959695146083832\n",
            "Epoch:0/1,\tIteration:31300,\tLoss:6.057684187889099\n",
            "Epoch:0/1,\tIteration:31400,\tLoss:5.9575070905685426\n",
            "Epoch:0/1,\tIteration:31500,\tLoss:6.1658209753036495\n",
            "Epoch:0/1,\tIteration:31600,\tLoss:5.845187802314758\n",
            "Epoch:0/1,\tIteration:31700,\tLoss:5.8960062742233275\n",
            "Epoch:0/1,\tIteration:31800,\tLoss:6.1275727081298825\n",
            "Epoch:0/1,\tIteration:31900,\tLoss:5.731878957748413\n",
            "Epoch:0/1,\tIteration:32000,\tLoss:5.668147010803223\n",
            "Epoch:0/1,\tIteration:32100,\tLoss:5.583231182098388\n",
            "Epoch:0/1,\tIteration:32200,\tLoss:5.522002289295196\n",
            "Epoch:0/1,\tIteration:32300,\tLoss:5.659499039649964\n",
            "Epoch:0/1,\tIteration:32400,\tLoss:5.501156837940216\n",
            "Epoch:0/1,\tIteration:32500,\tLoss:5.7074143099784855\n",
            "Epoch:0/1,\tIteration:32600,\tLoss:5.549491019248962\n",
            "Epoch:0/1,\tIteration:32700,\tLoss:5.456847412586212\n",
            "Epoch:0/1,\tIteration:32800,\tLoss:5.632370600700378\n",
            "Epoch:0/1,\tIteration:32900,\tLoss:5.6522935771942135\n",
            "Epoch:0/1,\tIteration:33000,\tLoss:5.692520101070404\n",
            "Epoch:0/1,\tIteration:33100,\tLoss:5.846124093532563\n",
            "Epoch:0/1,\tIteration:33200,\tLoss:5.877295126914978\n",
            "Epoch:0/1,\tIteration:33300,\tLoss:5.957461104393006\n",
            "Epoch:0/1,\tIteration:33400,\tLoss:5.935454554557801\n",
            "Epoch:0/1,\tIteration:33500,\tLoss:5.848003911972046\n",
            "Epoch:0/1,\tIteration:33600,\tLoss:6.073776187896729\n",
            "Epoch:0/1,\tIteration:33700,\tLoss:5.738656148910523\n",
            "Epoch:0/1,\tIteration:33800,\tLoss:6.0103839635849\n",
            "Epoch:0/1,\tIteration:33900,\tLoss:6.0249937677383425\n",
            "Epoch:0/1,\tIteration:34000,\tLoss:5.990760288238525\n",
            "Epoch:0/1,\tIteration:34100,\tLoss:5.80401291847229\n",
            "Epoch:0/1,\tIteration:34200,\tLoss:6.079558420181274\n",
            "Epoch:0/1,\tIteration:34300,\tLoss:5.954407398700714\n",
            "Epoch:0/1,\tIteration:34400,\tLoss:5.958219380378723\n",
            "Epoch:0/1,\tIteration:34500,\tLoss:5.74459174156189\n",
            "Epoch:0/1,\tIteration:34600,\tLoss:5.621051154136658\n",
            "Epoch:0/1,\tIteration:34700,\tLoss:5.88374508857727\n",
            "Epoch:0/1,\tIteration:34800,\tLoss:5.62544579744339\n",
            "Epoch:0/1,\tIteration:34900,\tLoss:5.699773030281067\n",
            "Epoch:0/1,\tIteration:35000,\tLoss:5.779395389556885\n",
            "Epoch:0/1,\tIteration:35100,\tLoss:5.547924647331238\n",
            "Epoch:0/1,\tIteration:35200,\tLoss:5.697522583007813\n",
            "Epoch:0/1,\tIteration:35300,\tLoss:5.749314484596252\n",
            "Epoch:0/1,\tIteration:35400,\tLoss:5.865673084259033\n",
            "Epoch:0/1,\tIteration:35500,\tLoss:6.032226138114929\n",
            "Epoch:0/1,\tIteration:35600,\tLoss:5.881827487945556\n",
            "Epoch:0/1,\tIteration:35700,\tLoss:5.904533786773682\n",
            "Epoch:0/1,\tIteration:35800,\tLoss:5.879939918518066\n",
            "Epoch:0/1,\tIteration:35900,\tLoss:5.693757681846619\n",
            "Epoch:0/1,\tIteration:36000,\tLoss:5.756838436126709\n",
            "Epoch:0/1,\tIteration:36100,\tLoss:5.842788763046265\n",
            "Epoch:0/1,\tIteration:36200,\tLoss:5.996265149116516\n",
            "Epoch:0/1,\tIteration:36300,\tLoss:5.993987016677856\n",
            "Epoch:0/1,\tIteration:36400,\tLoss:5.947537822723389\n",
            "Epoch:0/1,\tIteration:36500,\tLoss:5.828139951229096\n",
            "Epoch:0/1,\tIteration:36600,\tLoss:6.006500782966614\n",
            "Epoch:0/1,\tIteration:36700,\tLoss:5.910287585258484\n",
            "Epoch:0/1,\tIteration:36800,\tLoss:5.5660531210899356\n",
            "Epoch:0/1,\tIteration:36900,\tLoss:5.803282423019409\n",
            "Epoch:0/1,\tIteration:37000,\tLoss:6.056034092903137\n",
            "Epoch:0/1,\tIteration:37100,\tLoss:5.834343538284302\n",
            "Epoch:0/1,\tIteration:37200,\tLoss:5.587041921615601\n",
            "Epoch:0/1,\tIteration:37300,\tLoss:5.85503830909729\n",
            "Epoch:0/1,\tIteration:37400,\tLoss:5.798735618591309\n",
            "Epoch:0/1,\tIteration:37500,\tLoss:5.844697170257568\n",
            "Epoch:0/1,\tIteration:37600,\tLoss:5.641163783073425\n",
            "Epoch:0/1,\tIteration:37700,\tLoss:5.6784625911712645\n",
            "Epoch:0/1,\tIteration:37800,\tLoss:5.8464530563354495\n",
            "Epoch:0/1,\tIteration:37900,\tLoss:5.7506408643722535\n",
            "Epoch:0/1,\tIteration:38000,\tLoss:5.842067189216614\n",
            "Epoch:0/1,\tIteration:38100,\tLoss:5.815987706184387\n",
            "Epoch:0/1,\tIteration:38200,\tLoss:5.810518403053283\n",
            "Epoch:0/1,\tIteration:38300,\tLoss:5.574264607429504\n",
            "Epoch:0/1,\tIteration:38400,\tLoss:5.831473922729492\n",
            "Epoch:0/1,\tIteration:38500,\tLoss:5.8611791133880615\n",
            "Epoch:0/1,\tIteration:38600,\tLoss:5.768282608985901\n",
            "Epoch:0/1,\tIteration:38700,\tLoss:5.4344322681427\n",
            "Epoch:0/1,\tIteration:38800,\tLoss:5.726663062572479\n",
            "Epoch:0/1,\tIteration:38900,\tLoss:5.946281654834747\n",
            "Epoch:0/1,\tIteration:39000,\tLoss:5.846737606525421\n",
            "Epoch:0/1,\tIteration:39100,\tLoss:5.7411907768249515\n",
            "Epoch:0/1,\tIteration:39200,\tLoss:5.728882427215576\n",
            "Epoch:0/1,\tIteration:39300,\tLoss:4.427711544036865\n",
            "Epoch:0/1,\tIteration:39400,\tLoss:5.860053806304932\n",
            "Epoch:0/1,\tIteration:39500,\tLoss:5.817584619522095\n",
            "Epoch:0/1,\tIteration:39600,\tLoss:5.687157680988312\n",
            "Epoch:0/1,\tIteration:39700,\tLoss:5.750075807571411\n",
            "Epoch:0/1,\tIteration:39800,\tLoss:5.794913806915283\n",
            "Epoch:0/1,\tIteration:39900,\tLoss:5.7226345300674435\n",
            "Epoch:0/1,\tIteration:40000,\tLoss:5.6928871059417725\n",
            "Epoch:0/1,\tIteration:40100,\tLoss:5.563160548210144\n",
            "Epoch:0/1,\tIteration:40200,\tLoss:5.821725015640259\n",
            "Epoch:0/1,\tIteration:40300,\tLoss:5.916716380119324\n",
            "Epoch:0/1,\tIteration:40400,\tLoss:5.732245244979858\n",
            "Epoch:0/1,\tIteration:40500,\tLoss:5.482056550979614\n",
            "Epoch:0/1,\tIteration:40600,\tLoss:5.720236127376556\n",
            "Epoch:0/1,\tIteration:40700,\tLoss:6.103108921051025\n",
            "Epoch:0/1,\tIteration:40800,\tLoss:5.928515849113464\n",
            "Epoch:0/1,\tIteration:40900,\tLoss:5.621142728328705\n",
            "Epoch:0/1,\tIteration:41000,\tLoss:5.638331942558288\n",
            "Epoch:0/1,\tIteration:41100,\tLoss:5.71004905462265\n",
            "Epoch:0/1,\tIteration:41200,\tLoss:5.81733959197998\n",
            "Epoch:0/1,\tIteration:41300,\tLoss:5.956931762695312\n",
            "Epoch:0/1,\tIteration:41400,\tLoss:5.889209003448486\n",
            "Epoch:0/1,\tIteration:41500,\tLoss:5.739353332519531\n",
            "Epoch:0/1,\tIteration:41600,\tLoss:5.843508200645447\n",
            "Epoch:0/1,\tIteration:41700,\tLoss:5.77671703338623\n",
            "Epoch:0/1,\tIteration:41800,\tLoss:5.849590704441071\n",
            "Epoch:0/1,\tIteration:41900,\tLoss:5.635985560417176\n",
            "Epoch:0/1,\tIteration:42000,\tLoss:5.789645037651062\n",
            "Epoch:0/1,\tIteration:42100,\tLoss:5.712753505706787\n",
            "Epoch:0/1,\tIteration:42200,\tLoss:5.649532961845398\n",
            "Epoch:0/1,\tIteration:42300,\tLoss:5.7147910690307615\n",
            "Epoch:0/1,\tIteration:42400,\tLoss:5.842936315536499\n",
            "Epoch:0/1,\tIteration:42500,\tLoss:5.679838566780091\n",
            "Epoch:0/1,\tIteration:42600,\tLoss:5.825860614776611\n",
            "Epoch:0/1,\tIteration:42700,\tLoss:6.049123585224152\n",
            "Epoch:0/1,\tIteration:42800,\tLoss:5.60997716665268\n",
            "Epoch:0/1,\tIteration:42900,\tLoss:5.642724189758301\n",
            "Epoch:0/1,\tIteration:43000,\tLoss:5.775479955673218\n",
            "Epoch:0/1,\tIteration:43100,\tLoss:5.961538543701172\n",
            "Epoch:0/1,\tIteration:43200,\tLoss:6.014798526763916\n",
            "Epoch:0/1,\tIteration:43300,\tLoss:5.789712452888489\n",
            "Epoch:0/1,\tIteration:43400,\tLoss:5.618455927371979\n",
            "Epoch:0/1,\tIteration:43500,\tLoss:5.759221172332763\n",
            "Epoch:0/1,\tIteration:43600,\tLoss:5.586488010883332\n",
            "Epoch:0/1,\tIteration:43700,\tLoss:5.736594319343567\n",
            "Epoch:0/1,\tIteration:43800,\tLoss:5.900666975975037\n",
            "Epoch:0/1,\tIteration:43900,\tLoss:5.868796334266663\n",
            "Epoch:0/1,\tIteration:44000,\tLoss:5.645465559959412\n",
            "Epoch:0/1,\tIteration:44100,\tLoss:5.789008345603943\n",
            "Epoch:0/1,\tIteration:44200,\tLoss:5.6829830932617185\n",
            "Epoch:0/1,\tIteration:44300,\tLoss:5.686065094470978\n",
            "Epoch:0/1,\tIteration:44400,\tLoss:5.797932124137878\n",
            "Epoch:0/1,\tIteration:44500,\tLoss:5.683733801841736\n",
            "Epoch:0/1,\tIteration:44600,\tLoss:5.8109499168396\n",
            "Epoch:0/1,\tIteration:44700,\tLoss:5.602625734806061\n",
            "Epoch:0/1,\tIteration:44800,\tLoss:5.631630673408508\n",
            "Epoch:0/1,\tIteration:44900,\tLoss:5.836090502738952\n",
            "Epoch:0/1,\tIteration:45000,\tLoss:5.799159655570984\n",
            "Epoch:0/1,\tIteration:45100,\tLoss:5.765329566001892\n",
            "Epoch:0/1,\tIteration:45200,\tLoss:5.788040843009949\n",
            "Epoch:0/1,\tIteration:45300,\tLoss:5.906118021011353\n",
            "Epoch:0/1,\tIteration:45400,\tLoss:5.6669596624374385\n",
            "Epoch:0/1,\tIteration:45500,\tLoss:5.718961033821106\n",
            "Epoch:0/1,\tIteration:45600,\tLoss:5.726393365859986\n",
            "Epoch:0/1,\tIteration:45700,\tLoss:5.666164150238037\n",
            "Epoch:0/1,\tIteration:45800,\tLoss:5.81720006942749\n",
            "Epoch:0/1,\tIteration:45900,\tLoss:5.811640892028809\n",
            "Epoch:0/1,\tIteration:46000,\tLoss:5.94327670097351\n",
            "Epoch:0/1,\tIteration:46100,\tLoss:5.652558789253235\n",
            "Epoch:0/1,\tIteration:46200,\tLoss:5.709832410812378\n",
            "Epoch:0/1,\tIteration:46300,\tLoss:5.597363467216492\n",
            "Epoch:0/1,\tIteration:46400,\tLoss:5.725960011482239\n",
            "Epoch:0/1,\tIteration:46500,\tLoss:5.585385000705719\n",
            "Epoch:0/1,\tIteration:46600,\tLoss:5.823443293571472\n",
            "Epoch:0/1,\tIteration:46700,\tLoss:5.694975402355194\n",
            "Epoch:0/1,\tIteration:46800,\tLoss:5.750276608467102\n",
            "Epoch:0/1,\tIteration:46900,\tLoss:5.876655216217041\n",
            "Epoch:0/1,\tIteration:47000,\tLoss:5.757388920783996\n",
            "Epoch:0/1,\tIteration:47100,\tLoss:5.654281435012817\n",
            "Epoch:0/1,\tIteration:47200,\tLoss:5.598886723518372\n",
            "Epoch:0/1,\tIteration:47300,\tLoss:5.706889777183533\n",
            "Epoch:0/1,\tIteration:47400,\tLoss:5.729580223560333\n",
            "Epoch:0/1,\tIteration:47500,\tLoss:6.033664889335633\n",
            "Epoch:0/1,\tIteration:47600,\tLoss:5.687671785354614\n",
            "Epoch:0/1,\tIteration:47700,\tLoss:5.864204263687133\n",
            "Epoch:0/1,\tIteration:47800,\tLoss:5.827566170692444\n",
            "Epoch:0/1,\tIteration:47900,\tLoss:5.913943047523499\n",
            "Epoch:0/1,\tIteration:48000,\tLoss:5.886171975135803\n",
            "Epoch:0/1,\tIteration:48100,\tLoss:5.821673531532287\n",
            "Epoch:0/1,\tIteration:48200,\tLoss:5.931525254249573\n",
            "Epoch:0/1,\tIteration:48300,\tLoss:5.8777651119232175\n",
            "Epoch:0/1,\tIteration:48400,\tLoss:5.555309064388275\n",
            "Epoch:0/1,\tIteration:48500,\tLoss:5.561599745750427\n",
            "Epoch:0/1,\tIteration:48600,\tLoss:5.378693494796753\n",
            "Epoch:0/1,\tIteration:48700,\tLoss:5.5833505296707155\n",
            "Epoch:0/1,\tIteration:48800,\tLoss:5.851682600975036\n",
            "Epoch:0/1,\tIteration:48900,\tLoss:5.826203045845031\n",
            "Epoch:0/1,\tIteration:49000,\tLoss:5.842473554611206\n",
            "Epoch:0/1,\tIteration:49100,\tLoss:5.578393700122834\n",
            "Epoch:0/1,\tIteration:49200,\tLoss:5.522916641235351\n",
            "Epoch:0/1,\tIteration:49300,\tLoss:5.839937658309936\n",
            "Epoch:0/1,\tIteration:49400,\tLoss:5.758442106246949\n",
            "Epoch:0/1,\tIteration:49500,\tLoss:5.278221328258514\n",
            "Epoch:0/1,\tIteration:49600,\tLoss:5.5658671140670775\n",
            "Epoch:0/1,\tIteration:49700,\tLoss:5.74589923620224\n",
            "Epoch:0/1,\tIteration:49800,\tLoss:5.408615469932556\n",
            "Epoch:0/1,\tIteration:49900,\tLoss:5.9426010751724245\n",
            "Epoch:0/1,\tIteration:50000,\tLoss:5.805628571510315\n",
            "Epoch:0/1,\tIteration:50100,\tLoss:5.830351691246033\n",
            "Epoch:0/1,\tIteration:50200,\tLoss:5.692027542591095\n",
            "Epoch:0/1,\tIteration:50300,\tLoss:5.7576823425292964\n",
            "Epoch:0/1,\tIteration:50400,\tLoss:5.751928496360779\n",
            "Epoch:0/1,\tIteration:50500,\tLoss:5.727218956947326\n",
            "Epoch:0/1,\tIteration:50600,\tLoss:5.456819200515747\n",
            "Epoch:0/1,\tIteration:50700,\tLoss:5.876337854862213\n",
            "Epoch:0/1,\tIteration:50800,\tLoss:4.717242910861969\n",
            "Epoch:0/1,\tIteration:50900,\tLoss:5.275823380947113\n",
            "Epoch:0/1,\tIteration:51000,\tLoss:5.916439638137818\n",
            "Epoch:0/1,\tIteration:51100,\tLoss:5.652145900726318\n",
            "Epoch:0/1,\tIteration:51200,\tLoss:5.593819010257721\n",
            "Epoch:0/1,\tIteration:51300,\tLoss:5.526089599132538\n",
            "Epoch:0/1,\tIteration:51400,\tLoss:5.789547863006592\n",
            "Epoch:0/1,\tIteration:51500,\tLoss:5.757923383712768\n",
            "Epoch:0/1,\tIteration:51600,\tLoss:5.438366847038269\n",
            "Epoch:0/1,\tIteration:51700,\tLoss:5.758518404960633\n",
            "Epoch:0/1,\tIteration:51800,\tLoss:5.70515766620636\n",
            "Epoch:0/1,\tIteration:51900,\tLoss:5.76410861492157\n",
            "Epoch:0/1,\tIteration:52000,\tLoss:5.715886225700379\n",
            "Epoch:0/1,\tIteration:52100,\tLoss:5.573545684814453\n",
            "Epoch:0/1,\tIteration:52200,\tLoss:5.814298267364502\n",
            "Epoch:0/1,\tIteration:52300,\tLoss:5.645121855735779\n",
            "Epoch:0/1,\tIteration:52400,\tLoss:5.94060260295868\n",
            "Epoch:0/1,\tIteration:52500,\tLoss:5.942491235733033\n",
            "Epoch:0/1,\tIteration:52600,\tLoss:5.603281941413879\n",
            "Epoch:0/1,\tIteration:52700,\tLoss:5.549471089839935\n",
            "Epoch:0/1,\tIteration:52800,\tLoss:5.4844203734397885\n",
            "Epoch:0/1,\tIteration:52900,\tLoss:5.568307747840882\n",
            "Epoch:0/1,\tIteration:53000,\tLoss:5.5309931182861325\n",
            "Epoch:0/1,\tIteration:53100,\tLoss:5.771534056663513\n",
            "Epoch:0/1,\tIteration:53200,\tLoss:5.569116179943085\n",
            "Epoch:0/1,\tIteration:53300,\tLoss:5.697237243652344\n",
            "Epoch:0/1,\tIteration:53400,\tLoss:5.643384914398194\n",
            "Epoch:0/1,\tIteration:53500,\tLoss:5.477201080322265\n",
            "Epoch:0/1,\tIteration:53600,\tLoss:5.469785017967224\n",
            "Epoch:0/1,\tIteration:53700,\tLoss:5.811944541931152\n",
            "Epoch:0/1,\tIteration:53800,\tLoss:5.630220229625702\n",
            "Epoch:0/1,\tIteration:53900,\tLoss:5.806238651275635\n",
            "Epoch:0/1,\tIteration:54000,\tLoss:5.472353904247284\n",
            "Epoch:0/1,\tIteration:54100,\tLoss:5.553393943309784\n",
            "Epoch:0/1,\tIteration:54200,\tLoss:5.748037080764771\n",
            "Epoch:0/1,\tIteration:54300,\tLoss:5.714128603935242\n",
            "Epoch:0/1,\tIteration:54400,\tLoss:5.84042973279953\n",
            "Epoch:0/1,\tIteration:54500,\tLoss:4.8753145003318785\n",
            "Epoch:0/1,\tIteration:54600,\tLoss:5.5578851222991945\n",
            "Epoch:0/1,\tIteration:54700,\tLoss:5.686659619808197\n",
            "Epoch:0/1,\tIteration:54800,\tLoss:5.593964076042175\n",
            "Epoch:0/1,\tIteration:54900,\tLoss:5.741873989105224\n",
            "Epoch:0/1,\tIteration:55000,\tLoss:5.945828690528869\n",
            "Epoch:0/1,\tIteration:55100,\tLoss:5.737696356773377\n",
            "Epoch:0/1,\tIteration:55200,\tLoss:5.705602507591248\n",
            "Epoch:0/1,\tIteration:55300,\tLoss:5.9365906286239625\n",
            "Epoch:0/1,\tIteration:55400,\tLoss:5.566283068656921\n",
            "Epoch:0/1,\tIteration:55500,\tLoss:5.637229237556458\n",
            "Epoch:0/1,\tIteration:55600,\tLoss:5.76726909160614\n",
            "Epoch:0/1,\tIteration:55700,\tLoss:5.81506817817688\n",
            "Epoch:0/1,\tIteration:55800,\tLoss:5.738068237304687\n",
            "Epoch:0/1,\tIteration:55900,\tLoss:5.500814280509949\n",
            "Epoch:0/1,\tIteration:56000,\tLoss:5.98114513874054\n",
            "Epoch:0/1,\tIteration:56100,\tLoss:5.790843191146851\n",
            "Epoch:0/1,\tIteration:56200,\tLoss:5.597248759269714\n",
            "Epoch:0/1,\tIteration:56300,\tLoss:5.8076879501342775\n",
            "Epoch:0/1,\tIteration:56400,\tLoss:5.594990146160126\n",
            "Epoch:0/1,\tIteration:56500,\tLoss:5.823967981338501\n",
            "Epoch:0/1,\tIteration:56600,\tLoss:5.635258641242981\n",
            "Epoch:0/1,\tIteration:56700,\tLoss:5.615488836765289\n",
            "Epoch:0/1,\tIteration:56800,\tLoss:5.822719264030456\n",
            "Epoch:0/1,\tIteration:56900,\tLoss:5.941095180511475\n",
            "Epoch:0/1,\tIteration:57000,\tLoss:5.702892034053803\n",
            "Epoch:0/1,\tIteration:57100,\tLoss:5.649427289962769\n",
            "Epoch:0/1,\tIteration:57200,\tLoss:5.717238388061523\n",
            "Epoch:0/1,\tIteration:57300,\tLoss:5.478403692245483\n",
            "Epoch:0/1,\tIteration:57400,\tLoss:5.67421192407608\n",
            "Epoch:0/1,\tIteration:57500,\tLoss:5.550733270645142\n",
            "Epoch:0/1,\tIteration:57600,\tLoss:5.4239467430114745\n",
            "Epoch:0/1,\tIteration:57700,\tLoss:5.461564767360687\n",
            "Epoch:0/1,\tIteration:57800,\tLoss:5.378244745731354\n",
            "Epoch:0/1,\tIteration:57900,\tLoss:5.370371780395508\n",
            "Epoch:0/1,\tIteration:58000,\tLoss:6.008700413703918\n",
            "Epoch:0/1,\tIteration:58100,\tLoss:5.6645512342453\n",
            "Epoch:0/1,\tIteration:58200,\tLoss:5.714784445762635\n",
            "Epoch:0/1,\tIteration:58300,\tLoss:5.674006247520447\n",
            "Epoch:0/1,\tIteration:58400,\tLoss:5.577425527572632\n",
            "Epoch:0/1,\tIteration:58500,\tLoss:5.656844124794007\n",
            "Epoch:0/1,\tIteration:58600,\tLoss:5.5722703051567075\n",
            "Epoch:0/1,\tIteration:58700,\tLoss:5.788975057601928\n",
            "Epoch:0/1,\tIteration:58800,\tLoss:5.589689717292786\n",
            "Epoch:0/1,\tIteration:58900,\tLoss:5.545842390060425\n",
            "Epoch:0/1,\tIteration:59000,\tLoss:5.646170778274536\n",
            "Epoch:0/1,\tIteration:59100,\tLoss:5.740157146453857\n",
            "Epoch:0/1,\tIteration:59200,\tLoss:5.784775624275207\n",
            "Epoch:0/1,\tIteration:59300,\tLoss:5.712603235244751\n",
            "Epoch:0/1,\tIteration:59400,\tLoss:5.670084881782532\n",
            "Epoch:0/1,\tIteration:59500,\tLoss:5.647652373313904\n",
            "Epoch:0/1,\tIteration:59600,\tLoss:5.541116371154785\n",
            "Epoch:0/1,\tIteration:59700,\tLoss:5.65772919178009\n",
            "Epoch:0/1,\tIteration:59800,\tLoss:5.574558155536652\n",
            "Epoch:0/1,\tIteration:59900,\tLoss:4.991579151153564\n",
            "Epoch:0/1,\tIteration:60000,\tLoss:5.5001446461677554\n",
            "Epoch:0/1,\tIteration:60100,\tLoss:5.877840685844421\n",
            "Epoch:0/1,\tIteration:60200,\tLoss:5.661784524917603\n",
            "Epoch:0/1,\tIteration:60300,\tLoss:5.61515652179718\n",
            "Epoch:0/1,\tIteration:60400,\tLoss:5.765518136024475\n",
            "Epoch:0/1,\tIteration:60500,\tLoss:5.600742840766907\n",
            "Epoch:0/1,\tIteration:60600,\tLoss:5.75460114479065\n",
            "Epoch:0/1,\tIteration:60700,\tLoss:5.700096824169159\n",
            "Epoch:0/1,\tIteration:60800,\tLoss:5.508514909744263\n",
            "Epoch:0/1,\tIteration:60900,\tLoss:5.692166860103607\n",
            "Epoch:0/1,\tIteration:61000,\tLoss:5.531751415729523\n",
            "Epoch:0/1,\tIteration:61100,\tLoss:5.780809788703919\n",
            "Epoch:0/1,\tIteration:61200,\tLoss:5.734449257850647\n",
            "Epoch:0/1,\tIteration:61300,\tLoss:5.6896758651733395\n",
            "Epoch:0/1,\tIteration:61400,\tLoss:5.707691583633423\n",
            "Epoch:0/1,\tIteration:61500,\tLoss:5.549372334480285\n",
            "Epoch:0/1,\tIteration:61600,\tLoss:5.781584548950195\n",
            "Epoch:0/1,\tIteration:61700,\tLoss:5.631180143356323\n",
            "Epoch:0/1,\tIteration:61800,\tLoss:5.533933396339417\n",
            "Epoch:0/1,\tIteration:61900,\tLoss:5.767596092224121\n",
            "Epoch:0/1,\tIteration:62000,\tLoss:5.525335166454315\n",
            "Epoch:0/1,\tIteration:62100,\tLoss:5.668175563812256\n",
            "Epoch:0/1,\tIteration:62200,\tLoss:5.703377046585083\n",
            "Epoch:0/1,\tIteration:62300,\tLoss:5.533002557754517\n",
            "Epoch:0/1,\tIteration:62400,\tLoss:5.651952953338623\n",
            "Epoch:0/1,\tIteration:62500,\tLoss:5.615538992881775\n",
            "Epoch:0/1,\tIteration:62600,\tLoss:5.57657881975174\n",
            "Epoch:0/1,\tIteration:62700,\tLoss:5.6711745262146\n",
            "Epoch:0/1,\tIteration:62800,\tLoss:5.653515453338623\n",
            "Epoch:0/1,\tIteration:62900,\tLoss:5.706763744354248\n",
            "Epoch:0/1,\tIteration:63000,\tLoss:5.191595710515976\n",
            "Epoch:0/1,\tIteration:63100,\tLoss:5.476273465156555\n",
            "Epoch:0/1,\tIteration:63200,\tLoss:5.3920387005805965\n",
            "Epoch:0/1,\tIteration:63300,\tLoss:5.575608501434326\n",
            "Epoch:0/1,\tIteration:63400,\tLoss:5.47136913061142\n",
            "Epoch:0/1,\tIteration:63500,\tLoss:5.805946221351624\n",
            "Epoch:0/1,\tIteration:63600,\tLoss:5.62197012424469\n",
            "Epoch:0/1,\tIteration:63700,\tLoss:5.710007300376892\n",
            "Epoch:0/1,\tIteration:63800,\tLoss:5.806618719100952\n",
            "Epoch:0/1,\tIteration:63900,\tLoss:5.69035008430481\n",
            "Epoch:0/1,\tIteration:64000,\tLoss:5.691826133728028\n",
            "Epoch:0/1,\tIteration:64100,\tLoss:5.4293919277191165\n",
            "Epoch:0/1,\tIteration:64200,\tLoss:5.218732409477234\n",
            "Epoch:0/1,\tIteration:64300,\tLoss:5.340180883407593\n",
            "Epoch:0/1,\tIteration:64400,\tLoss:5.723923978805542\n",
            "Epoch:0/1,\tIteration:64500,\tLoss:5.650239081382751\n",
            "Epoch:0/1,\tIteration:64600,\tLoss:5.851738595962525\n",
            "Epoch:0/1,\tIteration:64700,\tLoss:5.480026128292084\n",
            "Epoch:0/1,\tIteration:64800,\tLoss:5.453382213115692\n",
            "Epoch:0/1,\tIteration:64900,\tLoss:5.478021018505096\n",
            "Epoch:0/1,\tIteration:65000,\tLoss:5.976015839576721\n",
            "Epoch:0/1,\tIteration:65100,\tLoss:5.694405722618103\n",
            "Epoch:0/1,\tIteration:65200,\tLoss:5.786740975379944\n",
            "Epoch:0/1,\tIteration:65300,\tLoss:5.521280450820923\n",
            "Epoch:0/1,\tIteration:65400,\tLoss:5.7525444602966305\n",
            "Epoch:0/1,\tIteration:65500,\tLoss:5.472226462364197\n",
            "Epoch:0/1,\tIteration:65600,\tLoss:5.496217782497406\n",
            "Epoch:0/1,\tIteration:65700,\tLoss:5.391023056507111\n",
            "Epoch:0/1,\tIteration:65800,\tLoss:5.678878209590912\n",
            "Epoch:0/1,\tIteration:65900,\tLoss:5.629401383399963\n",
            "Epoch:0/1,\tIteration:66000,\tLoss:5.394103569984436\n",
            "Epoch:0/1,\tIteration:66100,\tLoss:5.5023492193222046\n",
            "Epoch:0/1,\tIteration:66200,\tLoss:5.7194699883461\n",
            "Epoch:0/1,\tIteration:66300,\tLoss:5.684092848300934\n",
            "Epoch:0/1,\tIteration:66400,\tLoss:5.789209961891174\n",
            "Epoch:0/1,\tIteration:66500,\tLoss:5.6223766040802\n",
            "Epoch:0/1,\tIteration:66600,\tLoss:5.6713266181945805\n",
            "Epoch:0/1,\tIteration:66700,\tLoss:5.554407725334167\n",
            "Epoch:0/1,\tIteration:66800,\tLoss:5.601982355117798\n",
            "Epoch:0/1,\tIteration:66900,\tLoss:5.986804194450379\n",
            "Epoch:0/1,\tIteration:67000,\tLoss:5.686333079338073\n",
            "Epoch:0/1,\tIteration:67100,\tLoss:5.712672939300537\n",
            "Epoch:0/1,\tIteration:67200,\tLoss:5.646956467628479\n",
            "Epoch:0/1,\tIteration:67300,\tLoss:5.664477152824402\n",
            "Epoch:0/1,\tIteration:67400,\tLoss:5.814092144966126\n",
            "Epoch:0/1,\tIteration:67500,\tLoss:5.72011137008667\n",
            "Epoch:0/1,\tIteration:67600,\tLoss:5.416481926441192\n",
            "Epoch:0/1,\tIteration:67700,\tLoss:5.593641343116761\n",
            "Epoch:0/1,\tIteration:67800,\tLoss:4.6495481610298155\n",
            "Epoch:0/1,\tIteration:67900,\tLoss:4.841887984275818\n",
            "Epoch:0/1,\tIteration:68000,\tLoss:5.693701391220093\n",
            "Epoch:0/1,\tIteration:68100,\tLoss:5.717542123794556\n",
            "Epoch:0/1,\tIteration:68200,\tLoss:5.643069515228271\n",
            "Epoch:0/1,\tIteration:68300,\tLoss:5.444224190711975\n",
            "Epoch:0/1,\tIteration:68400,\tLoss:5.639616451263428\n",
            "Epoch:0/1,\tIteration:68500,\tLoss:5.27509756565094\n",
            "Epoch:0/1,\tIteration:68600,\tLoss:5.890030770301819\n",
            "Epoch:0/1,\tIteration:68700,\tLoss:5.647331335544586\n",
            "Epoch:0/1,\tIteration:68800,\tLoss:5.2902598524093625\n",
            "Epoch:0/1,\tIteration:68900,\tLoss:5.438968014717102\n",
            "Epoch:0/1,\tIteration:69000,\tLoss:5.5191999888420105\n",
            "Epoch:0/1,\tIteration:69100,\tLoss:5.669176506996155\n",
            "Epoch:0/1,\tIteration:69200,\tLoss:5.687171430587768\n",
            "Epoch:0/1,\tIteration:69300,\tLoss:5.507342829704284\n",
            "Epoch:0/1,\tIteration:69400,\tLoss:5.686875619888306\n",
            "Epoch:0/1,\tIteration:69500,\tLoss:5.63572696685791\n",
            "Epoch:0/1,\tIteration:69600,\tLoss:5.502812583446502\n",
            "Epoch:0/1,\tIteration:69700,\tLoss:5.631173729896545\n",
            "Epoch:0/1,\tIteration:69800,\tLoss:5.688127217292785\n",
            "Epoch:0/1,\tIteration:69900,\tLoss:5.800167784690857\n",
            "Epoch:0/1,\tIteration:70000,\tLoss:5.4416778683662415\n",
            "Epoch:0/1,\tIteration:70100,\tLoss:5.674191431999207\n",
            "Epoch:0/1,\tIteration:70200,\tLoss:5.6891971206665035\n",
            "Epoch:0/1,\tIteration:70300,\tLoss:6.07081001996994\n",
            "Epoch:0/1,\tIteration:70400,\tLoss:5.538095970153808\n",
            "Epoch:0/1,\tIteration:70500,\tLoss:4.531291198730469\n",
            "Epoch:0/1,\tIteration:70600,\tLoss:5.132639315128326\n",
            "Epoch:0/1,\tIteration:70700,\tLoss:5.679108762741089\n",
            "Epoch:0/1,\tIteration:70800,\tLoss:5.692131457328796\n",
            "Epoch:0/1,\tIteration:70900,\tLoss:5.545054764747619\n",
            "Epoch:0/1,\tIteration:71000,\tLoss:5.5957298088073735\n",
            "Epoch:0/1,\tIteration:71100,\tLoss:5.662789402008056\n",
            "Epoch:0/1,\tIteration:71200,\tLoss:5.428535284996033\n",
            "Epoch:0/1,\tIteration:71300,\tLoss:5.379750185012817\n",
            "Epoch:0/1,\tIteration:71400,\tLoss:5.686551966667175\n",
            "Epoch:0/1,\tIteration:71500,\tLoss:5.665611066818237\n",
            "Epoch:0/1,\tIteration:71600,\tLoss:5.624218716621399\n",
            "Epoch:0/1,\tIteration:71700,\tLoss:5.52141155719757\n",
            "Epoch:0/1,\tIteration:71800,\tLoss:5.84491199016571\n",
            "Epoch:0/1,\tIteration:71900,\tLoss:5.573794643878937\n",
            "Epoch:0/1,\tIteration:72000,\tLoss:5.7116358280181885\n",
            "Epoch:0/1,\tIteration:72100,\tLoss:5.704417369365692\n",
            "Epoch:0/1,\tIteration:72200,\tLoss:5.815875716209412\n",
            "Epoch:0/1,\tIteration:72300,\tLoss:5.719985632896424\n",
            "Epoch:0/1,\tIteration:72400,\tLoss:5.784773306846619\n",
            "Epoch:0/1,\tIteration:72500,\tLoss:5.72446451663971\n",
            "Epoch:0/1,\tIteration:72600,\tLoss:5.5461505818367005\n",
            "Epoch:0/1,\tIteration:72700,\tLoss:5.086793260574341\n",
            "Epoch:0/1,\tIteration:72800,\tLoss:5.521479918956756\n",
            "Epoch:0/1,\tIteration:72900,\tLoss:5.565896911621094\n",
            "Epoch:0/1,\tIteration:73000,\tLoss:5.7404807806015015\n",
            "Epoch:0/1,\tIteration:73100,\tLoss:5.4861158800125125\n",
            "Epoch:0/1,\tIteration:73200,\tLoss:5.78438063621521\n",
            "Epoch:0/1,\tIteration:73300,\tLoss:5.566176257133484\n",
            "Epoch:0/1,\tIteration:73400,\tLoss:5.768927345275879\n",
            "Epoch:0/1,\tIteration:73500,\tLoss:5.632579512596131\n",
            "Epoch:0/1,\tIteration:73600,\tLoss:5.574849326610565\n",
            "Epoch:0/1,\tIteration:73700,\tLoss:5.523769946098327\n",
            "Epoch:0/1,\tIteration:73800,\tLoss:5.744875235557556\n",
            "Epoch:0/1,\tIteration:73900,\tLoss:5.6121053838729855\n",
            "Epoch:0/1,\tIteration:74000,\tLoss:5.643316431045532\n",
            "Epoch:0/1,\tIteration:74100,\tLoss:5.657292194366455\n",
            "Epoch:0/1,\tIteration:74200,\tLoss:5.7569799947738645\n",
            "Epoch:0/1,\tIteration:74300,\tLoss:5.688952970504761\n",
            "Epoch:0/1,\tIteration:74400,\tLoss:5.8026993942260745\n",
            "Epoch:0/1,\tIteration:74500,\tLoss:5.887075252532959\n",
            "Epoch:0/1,\tIteration:74600,\tLoss:5.736985154151917\n",
            "Epoch:0/1,\tIteration:74700,\tLoss:5.56995466709137\n",
            "Epoch:0/1,\tIteration:74800,\tLoss:5.400219986438751\n",
            "Epoch:0/1,\tIteration:74900,\tLoss:5.494849328994751\n",
            "Epoch:0/1,\tIteration:75000,\tLoss:5.478234090805054\n",
            "Epoch:0/1,\tIteration:75100,\tLoss:5.595449938774109\n",
            "Epoch:0/1,\tIteration:75200,\tLoss:5.374654760360718\n",
            "Epoch:0/1,\tIteration:75300,\tLoss:5.271823060512543\n",
            "Epoch:0/1,\tIteration:75400,\tLoss:5.318246076107025\n",
            "Epoch:0/1,\tIteration:75500,\tLoss:5.116514182090759\n",
            "Epoch:0/1,\tIteration:75600,\tLoss:5.6932307052612305\n",
            "Epoch:0/1,\tIteration:75700,\tLoss:5.760754721164703\n",
            "Epoch:0/1,\tIteration:75800,\tLoss:5.753414816856385\n",
            "Epoch:0/1,\tIteration:75900,\tLoss:5.451498675346374\n",
            "Epoch:0/1,\tIteration:76000,\tLoss:5.699762682914734\n",
            "Epoch:0/1,\tIteration:76100,\tLoss:5.617904148101807\n",
            "Epoch:0/1,\tIteration:76200,\tLoss:5.413639187812805\n",
            "Epoch:0/1,\tIteration:76300,\tLoss:5.426739387512207\n",
            "Epoch:0/1,\tIteration:76400,\tLoss:5.657645244598388\n",
            "Epoch:0/1,\tIteration:76500,\tLoss:5.630701532363892\n",
            "Epoch:0/1,\tIteration:76600,\tLoss:5.339815444946289\n",
            "Epoch:0/1,\tIteration:76700,\tLoss:5.816590666770935\n",
            "Epoch:0/1,\tIteration:76800,\tLoss:5.319897217750549\n",
            "Epoch:0/1,\tIteration:76900,\tLoss:5.831061015129089\n",
            "Epoch:0/1,\tIteration:77000,\tLoss:5.217661278247833\n",
            "Epoch:0/1,\tIteration:77100,\tLoss:5.672829737663269\n",
            "Epoch:0/1,\tIteration:77200,\tLoss:5.525351994037628\n",
            "Epoch:0/1,\tIteration:77300,\tLoss:5.7746160078048705\n",
            "Epoch:0/1,\tIteration:77400,\tLoss:5.617053661346436\n",
            "Epoch:0/1,\tIteration:77500,\tLoss:5.468052492141724\n",
            "Epoch:0/1,\tIteration:77600,\tLoss:5.552062458992005\n",
            "Epoch:0/1,\tIteration:77700,\tLoss:5.6306103992462155\n",
            "Epoch:0/1,\tIteration:77800,\tLoss:5.4994138956069945\n",
            "Epoch:0/1,\tIteration:77900,\tLoss:5.56122579574585\n",
            "Epoch:0/1,\tIteration:78000,\tLoss:5.5747879695892335\n",
            "Epoch:0/1,\tIteration:78100,\tLoss:5.608800046443939\n",
            "Epoch:0/1,\tIteration:78200,\tLoss:5.691465582847595\n",
            "Epoch:0/1,\tIteration:78300,\tLoss:5.722259135246277\n",
            "Epoch:0/1,\tIteration:78400,\tLoss:5.346468884944915\n",
            "Epoch:0/1,\tIteration:78500,\tLoss:5.696970274448395\n",
            "Epoch:0/1,\tIteration:78600,\tLoss:5.297004384994507\n",
            "Epoch:0/1,\tIteration:78700,\tLoss:5.676960206031799\n",
            "Epoch:0/1,\tIteration:78800,\tLoss:5.789159202575684\n",
            "Epoch:0/1,\tIteration:78900,\tLoss:5.658879432678223\n",
            "Epoch:0/1,\tIteration:79000,\tLoss:5.743466606140137\n",
            "Epoch:0/1,\tIteration:79100,\tLoss:5.50574414730072\n",
            "Epoch:0/1,\tIteration:79200,\tLoss:5.311412906646728\n",
            "Epoch:0/1,\tIteration:79300,\tLoss:5.579660573005676\n",
            "Epoch:0/1,\tIteration:79400,\tLoss:5.568630056381226\n",
            "Epoch:0/1,\tIteration:79500,\tLoss:5.376800136566162\n",
            "Epoch:0/1,\tIteration:79600,\tLoss:5.560265531539917\n",
            "Epoch:0/1,\tIteration:79700,\tLoss:5.504409284591675\n",
            "Epoch:0/1,\tIteration:79800,\tLoss:5.716464309692383\n",
            "Epoch:0/1,\tIteration:79900,\tLoss:5.642877368927002\n",
            "Epoch:0/1,\tIteration:80000,\tLoss:5.695015158653259\n",
            "Epoch:0/1,\tIteration:80100,\tLoss:5.507032973766327\n",
            "Epoch:0/1,\tIteration:80200,\tLoss:5.645164265632629\n",
            "Epoch:0/1,\tIteration:80300,\tLoss:5.63101706981659\n",
            "Epoch:0/1,\tIteration:80400,\tLoss:5.728635783195496\n",
            "Epoch:0/1,\tIteration:80500,\tLoss:5.327457945346833\n",
            "Epoch:0/1,\tIteration:80600,\tLoss:5.487370867729187\n",
            "Epoch:0/1,\tIteration:80700,\tLoss:5.483324213027954\n",
            "Epoch:0/1,\tIteration:80800,\tLoss:5.678780550956726\n",
            "Epoch:0/1,\tIteration:80900,\tLoss:5.597875473499298\n",
            "Epoch:0/1,\tIteration:81000,\tLoss:5.795109438896179\n",
            "Epoch:0/1,\tIteration:81100,\tLoss:5.3907503676414485\n",
            "Epoch:0/1,\tIteration:81200,\tLoss:5.232491221427917\n",
            "Epoch:0/1,\tIteration:81300,\tLoss:5.585736994743347\n",
            "Epoch:0/1,\tIteration:81400,\tLoss:5.615177187919617\n",
            "Epoch:0/1,\tIteration:81500,\tLoss:5.284166390895844\n",
            "Epoch:0/1,\tIteration:81600,\tLoss:5.549819540977478\n",
            "Epoch:0/1,\tIteration:81700,\tLoss:5.660289516448975\n",
            "Epoch:0/1,\tIteration:81800,\tLoss:5.488593633174896\n",
            "Epoch:0/1,\tIteration:81900,\tLoss:5.463217794895172\n",
            "Epoch:0/1,\tIteration:82000,\tLoss:5.4696284198760985\n",
            "Epoch:0/1,\tIteration:82100,\tLoss:5.4247480440139775\n",
            "Epoch:0/1,\tIteration:82200,\tLoss:5.395876584053039\n",
            "Epoch:0/1,\tIteration:82300,\tLoss:5.727822246551514\n",
            "Epoch:0/1,\tIteration:82400,\tLoss:5.383049130439758\n",
            "Epoch:0/1,\tIteration:82500,\tLoss:5.356732583045959\n",
            "Epoch:0/1,\tIteration:82600,\tLoss:5.459393138885498\n",
            "Epoch:0/1,\tIteration:82700,\tLoss:5.333525531291961\n",
            "Epoch:0/1,\tIteration:82800,\tLoss:5.635097961425782\n",
            "Epoch:0/1,\tIteration:82900,\tLoss:5.530662989616394\n",
            "Epoch:0/1,\tIteration:83000,\tLoss:5.61075973033905\n",
            "Epoch:0/1,\tIteration:83100,\tLoss:5.492746515274048\n",
            "Epoch:0/1,\tIteration:83200,\tLoss:5.56524317741394\n",
            "Epoch:0/1,\tIteration:83300,\tLoss:5.40755178451538\n",
            "Epoch:0/1,\tIteration:83400,\tLoss:5.4951380085945125\n",
            "Epoch:0/1,\tIteration:83500,\tLoss:5.604456062316895\n",
            "Epoch:0/1,\tIteration:83600,\tLoss:5.560292711257935\n",
            "Epoch:0/1,\tIteration:83700,\tLoss:5.527028293609619\n",
            "Epoch:0/1,\tIteration:83800,\tLoss:5.440345010757446\n",
            "Epoch:0/1,\tIteration:83900,\tLoss:5.637173438072205\n",
            "Epoch:0/1,\tIteration:84000,\tLoss:5.715768780708313\n",
            "Epoch:0/1,\tIteration:84100,\tLoss:5.690764217376709\n",
            "Epoch:0/1,\tIteration:84200,\tLoss:5.733688807487487\n",
            "Epoch:0/1,\tIteration:84300,\tLoss:5.56833812713623\n",
            "Epoch:0/1,\tIteration:84400,\tLoss:5.590606327056885\n",
            "Epoch:0/1,\tIteration:84500,\tLoss:5.604492120742798\n",
            "Epoch:0/1,\tIteration:84600,\tLoss:5.334846513271332\n",
            "Epoch:0/1,\tIteration:84700,\tLoss:5.720635786056518\n",
            "Epoch:0/1,\tIteration:84800,\tLoss:5.737161021232605\n",
            "Epoch:0/1,\tIteration:84900,\tLoss:5.610119514465332\n",
            "Epoch:0/1,\tIteration:85000,\tLoss:5.579989476203918\n",
            "Epoch:0/1,\tIteration:85100,\tLoss:5.322663431167602\n",
            "Epoch:0/1,\tIteration:85200,\tLoss:5.557905254364013\n",
            "Epoch:0/1,\tIteration:85300,\tLoss:5.577435703277588\n",
            "Epoch:0/1,\tIteration:85400,\tLoss:5.625793662071228\n",
            "Epoch:0/1,\tIteration:85500,\tLoss:5.587866539955139\n",
            "Epoch:0/1,\tIteration:85600,\tLoss:5.524501428604126\n",
            "Epoch:0/1,\tIteration:85700,\tLoss:5.574212749004364\n",
            "Epoch:0/1,\tIteration:85800,\tLoss:5.563817625045776\n",
            "Epoch:0/1,\tIteration:85900,\tLoss:5.58078720331192\n",
            "Epoch:0/1,\tIteration:86000,\tLoss:5.487382121086121\n",
            "Epoch:0/1,\tIteration:86100,\tLoss:5.531963558197021\n",
            "Epoch:0/1,\tIteration:86200,\tLoss:5.372054562568665\n",
            "Epoch:0/1,\tIteration:86300,\tLoss:5.1617111039161685\n",
            "Epoch:0/1,\tIteration:86400,\tLoss:5.644234633445739\n",
            "Epoch:0/1,\tIteration:86500,\tLoss:5.856904177665711\n",
            "Epoch:0/1,\tIteration:86600,\tLoss:5.44500892162323\n",
            "Epoch:0/1,\tIteration:86700,\tLoss:5.883835258483887\n",
            "Epoch:0/1,\tIteration:86800,\tLoss:5.659655771255493\n",
            "Epoch:0/1,\tIteration:86900,\tLoss:5.088746600151062\n",
            "Epoch:0/1,\tIteration:87000,\tLoss:5.370023579597473\n",
            "Epoch:0/1,\tIteration:87100,\tLoss:5.4998713088035585\n",
            "Epoch:0/1,\tIteration:87200,\tLoss:5.595314362049103\n",
            "Epoch:0/1,\tIteration:87300,\tLoss:5.790535278320313\n",
            "Epoch:0/1,\tIteration:87400,\tLoss:5.7602897644042965\n",
            "Epoch:0/1,\tIteration:87500,\tLoss:5.559230244159698\n",
            "Epoch:0/1,\tIteration:87600,\tLoss:5.561209673881531\n",
            "Epoch:0/1,\tIteration:87700,\tLoss:5.466022129058838\n",
            "Epoch:0/1,\tIteration:87800,\tLoss:5.534115586280823\n",
            "Epoch:0/1,\tIteration:87900,\tLoss:5.775999026298523\n",
            "Epoch:0/1,\tIteration:88000,\tLoss:5.503307018280029\n",
            "Epoch:0/1,\tIteration:88100,\tLoss:5.344927442073822\n",
            "Epoch:0/1,\tIteration:88200,\tLoss:5.427097089290619\n",
            "Epoch:0/1,\tIteration:88300,\tLoss:5.671228713989258\n",
            "Epoch:0/1,\tIteration:88400,\tLoss:5.569075961112976\n",
            "Epoch:0/1,\tIteration:88500,\tLoss:5.742037882804871\n",
            "Epoch:0/1,\tIteration:88600,\tLoss:5.602967736721038\n",
            "Epoch:0/1,\tIteration:88700,\tLoss:5.477221002578736\n",
            "Epoch:0/1,\tIteration:88800,\tLoss:5.4646697425842286\n",
            "Epoch:0/1,\tIteration:88900,\tLoss:5.511918437480927\n",
            "Epoch:0/1,\tIteration:89000,\tLoss:5.4487301635742185\n",
            "Epoch:0/1,\tIteration:89100,\tLoss:5.622105660438538\n",
            "Epoch:0/1,\tIteration:89200,\tLoss:5.330875058174133\n",
            "Epoch:0/1,\tIteration:89300,\tLoss:5.692263789176941\n",
            "Epoch:0/1,\tIteration:89400,\tLoss:5.4895267581939695\n",
            "Epoch:0/1,\tIteration:89500,\tLoss:5.526752667427063\n",
            "Epoch:0/1,\tIteration:89600,\tLoss:5.492875165939331\n",
            "Epoch:0/1,\tIteration:89700,\tLoss:5.384646716117859\n",
            "Epoch:0/1,\tIteration:89800,\tLoss:5.6741367530822755\n",
            "Epoch:0/1,\tIteration:89900,\tLoss:5.628149008750915\n",
            "Epoch:0/1,\tIteration:90000,\tLoss:5.6460701274871825\n",
            "Epoch:0/1,\tIteration:90100,\tLoss:5.588652431964874\n",
            "Epoch:0/1,\tIteration:90200,\tLoss:5.643907194137573\n",
            "Epoch:0/1,\tIteration:90300,\tLoss:5.632166061401367\n",
            "Epoch:0/1,\tIteration:90400,\tLoss:5.740368661880493\n",
            "Epoch:0/1,\tIteration:90500,\tLoss:5.6357869005203245\n",
            "Epoch:0/1,\tIteration:90600,\tLoss:5.610230088233948\n",
            "Epoch:0/1,\tIteration:90700,\tLoss:5.455820951461792\n",
            "Epoch:0/1,\tIteration:90800,\tLoss:5.4635175657272335\n",
            "Epoch:0/1,\tIteration:90900,\tLoss:5.348819036483764\n",
            "Epoch:0/1,\tIteration:91000,\tLoss:5.611884160041809\n",
            "Epoch:0/1,\tIteration:91100,\tLoss:5.507421631813049\n",
            "Epoch:0/1,\tIteration:91200,\tLoss:5.477599186897278\n",
            "Epoch:0/1,\tIteration:91300,\tLoss:5.39396288394928\n",
            "Epoch:0/1,\tIteration:91400,\tLoss:5.613715286254883\n",
            "Epoch:0/1,\tIteration:91500,\tLoss:5.416884510517121\n",
            "Epoch:0/1,\tIteration:91600,\tLoss:5.556127552986145\n",
            "Epoch:0/1,\tIteration:91700,\tLoss:5.673397421836853\n",
            "Epoch:0/1,\tIteration:91800,\tLoss:5.438207340240479\n",
            "Epoch:0/1,\tIteration:91900,\tLoss:5.4515391445159915\n",
            "Epoch:0/1,\tIteration:92000,\tLoss:5.664203617572785\n",
            "Epoch:0/1,\tIteration:92100,\tLoss:5.5373954010009765\n",
            "Epoch:0/1,\tIteration:92200,\tLoss:5.355529978275299\n",
            "Epoch:0/1,\tIteration:92300,\tLoss:5.326272110939026\n",
            "Epoch:0/1,\tIteration:92400,\tLoss:5.2930421137809756\n",
            "Epoch:0/1,\tIteration:92500,\tLoss:5.321830077171326\n",
            "Epoch:0/1,\tIteration:92600,\tLoss:5.171970224380493\n",
            "Epoch:0/1,\tIteration:92700,\tLoss:5.533337364196777\n",
            "Epoch:0/1,\tIteration:92800,\tLoss:5.843361692428589\n",
            "Epoch:0/1,\tIteration:92900,\tLoss:5.674441504478454\n",
            "Epoch:0/1,\tIteration:93000,\tLoss:5.491767592430115\n",
            "Epoch:0/1,\tIteration:93100,\tLoss:5.60706919670105\n",
            "Epoch:0/1,\tIteration:93200,\tLoss:5.4716989207267765\n",
            "Epoch:0/1,\tIteration:93300,\tLoss:5.559038996696472\n",
            "Epoch:0/1,\tIteration:93400,\tLoss:5.714960069656372\n",
            "Epoch:0/1,\tIteration:93500,\tLoss:5.749486069679261\n",
            "Epoch:0/1,\tIteration:93600,\tLoss:5.599884996414184\n",
            "Epoch:0/1,\tIteration:93700,\tLoss:5.720075497627258\n",
            "Epoch:0/1,\tIteration:93800,\tLoss:5.492308878898621\n",
            "Epoch:0/1,\tIteration:93900,\tLoss:5.572003865242005\n",
            "Epoch:0/1,\tIteration:94000,\tLoss:5.490262315273285\n",
            "Epoch:0/1,\tIteration:94100,\tLoss:5.887540879249573\n",
            "Epoch:0/1,\tIteration:94200,\tLoss:5.642903809547424\n",
            "Epoch:0/1,\tIteration:94300,\tLoss:5.731254043579102\n",
            "Epoch:0/1,\tIteration:94400,\tLoss:5.417054328918457\n",
            "Epoch:0/1,\tIteration:94500,\tLoss:5.625942511558533\n",
            "Epoch:0/1,\tIteration:94600,\tLoss:5.708655486106872\n",
            "Epoch:0/1,\tIteration:94700,\tLoss:5.526942000389099\n",
            "Epoch:0/1,\tIteration:94800,\tLoss:5.633698630332947\n",
            "Epoch:0/1,\tIteration:94900,\tLoss:5.6509582757949826\n",
            "Epoch:0/1,\tIteration:95000,\tLoss:5.590443320274353\n",
            "Epoch:0/1,\tIteration:95100,\tLoss:5.717946705818176\n",
            "Epoch:0/1,\tIteration:95200,\tLoss:4.461234714984894\n",
            "Epoch:0/1,\tIteration:95300,\tLoss:5.50385428905487\n",
            "Epoch:0/1,\tIteration:95400,\tLoss:5.498869676589965\n",
            "Epoch:0/1,\tIteration:95500,\tLoss:5.443257169723511\n",
            "Epoch:0/1,\tIteration:95600,\tLoss:5.30719286441803\n",
            "Epoch:0/1,\tIteration:95700,\tLoss:5.7323202753067015\n",
            "Epoch:0/1,\tIteration:95800,\tLoss:5.717935218811035\n",
            "Epoch:0/1,\tIteration:95900,\tLoss:5.632023463249206\n",
            "Epoch:0/1,\tIteration:96000,\tLoss:5.482815852165222\n",
            "Epoch:0/1,\tIteration:96100,\tLoss:5.625031914710998\n",
            "Epoch:0/1,\tIteration:96200,\tLoss:5.342278115749359\n",
            "Epoch:0/1,\tIteration:96300,\tLoss:5.6138659191131595\n",
            "Epoch:0/1,\tIteration:96400,\tLoss:5.5485655570030215\n",
            "Epoch:0/1,\tIteration:96500,\tLoss:5.5119727420806885\n",
            "Epoch:0/1,\tIteration:96600,\tLoss:5.729581913948059\n",
            "Epoch:0/1,\tIteration:96700,\tLoss:5.589017772674561\n",
            "Epoch:0/1,\tIteration:96800,\tLoss:5.708526449203491\n",
            "Epoch:0/1,\tIteration:96900,\tLoss:5.394611277580261\n",
            "Epoch:0/1,\tIteration:97000,\tLoss:5.645026116371155\n",
            "Epoch:0/1,\tIteration:97100,\tLoss:5.323012351989746\n",
            "Epoch:0/1,\tIteration:97200,\tLoss:5.765452108383179\n",
            "Epoch:0/1,\tIteration:97300,\tLoss:5.638278503417968\n",
            "Epoch:0/1,\tIteration:97400,\tLoss:5.528726425170898\n",
            "Epoch:0/1,\tIteration:97500,\tLoss:5.683872997760773\n",
            "Epoch:0/1,\tIteration:97600,\tLoss:5.465388090610504\n",
            "Epoch:0/1,\tIteration:97700,\tLoss:5.445859553813935\n",
            "Epoch:0/1,\tIteration:97800,\tLoss:5.617843127250671\n",
            "Epoch:0/1,\tIteration:97900,\tLoss:5.4153324842453\n",
            "Epoch:0/1,\tIteration:98000,\tLoss:5.241048913002015\n",
            "Epoch:0/1,\tIteration:98100,\tLoss:5.508104059696198\n",
            "Epoch:0/1,\tIteration:98200,\tLoss:5.76597110748291\n",
            "Epoch:0/1,\tIteration:98300,\tLoss:5.669813110828399\n",
            "Epoch:0/1,\tIteration:98400,\tLoss:5.667644605636597\n",
            "Epoch:0/1,\tIteration:98500,\tLoss:5.672232599258423\n",
            "Epoch:0/1,\tIteration:98600,\tLoss:5.660903558731079\n",
            "Epoch:0/1,\tIteration:98700,\tLoss:5.5640407371521\n",
            "Epoch:0/1,\tIteration:98800,\tLoss:5.46436454296112\n",
            "Epoch:0/1,\tIteration:98900,\tLoss:5.194459846019745\n",
            "Epoch:0/1,\tIteration:99000,\tLoss:5.299408485889435\n",
            "Epoch:0/1,\tIteration:99100,\tLoss:5.250053100585937\n",
            "Epoch:0/1,\tIteration:99200,\tLoss:5.39497230052948\n",
            "Epoch:0/1,\tIteration:99300,\tLoss:5.4165849471092224\n",
            "Epoch:0/1,\tIteration:99400,\tLoss:5.703733305931092\n",
            "Epoch:0/1,\tIteration:99500,\tLoss:4.690152070522308\n",
            "Epoch:0/1,\tIteration:99600,\tLoss:4.5093043684959415\n",
            "Epoch:0/1,\tIteration:99700,\tLoss:4.672912254333496\n",
            "Epoch:0/1,\tIteration:99800,\tLoss:4.591517105102539\n",
            "Epoch:0/1,\tIteration:99900,\tLoss:4.2612191104888915\n",
            "Epoch:0/1,\tIteration:100000,\tLoss:5.019525439739227\n",
            "Epoch:0/1,\tIteration:100100,\tLoss:4.776453175544739\n",
            "Epoch:0/1,\tIteration:100200,\tLoss:4.48518325805664\n",
            "Epoch:0/1,\tIteration:100300,\tLoss:4.847265465259552\n",
            "Epoch:0/1,\tIteration:100400,\tLoss:4.582569205760956\n",
            "Epoch:0/1,\tIteration:100500,\tLoss:4.860915479660034\n",
            "Epoch:0/1,\tIteration:100600,\tLoss:5.387474861145019\n",
            "Epoch:0/1,\tIteration:100700,\tLoss:5.13472538948059\n",
            "Epoch:0/1,\tIteration:100800,\tLoss:5.445721726417542\n",
            "Epoch:0/1,\tIteration:100900,\tLoss:5.310384728908539\n",
            "Epoch:0/1,\tIteration:101000,\tLoss:5.10624140739441\n",
            "Epoch:0/1,\tIteration:101100,\tLoss:5.57656729221344\n",
            "Epoch:0/1,\tIteration:101200,\tLoss:5.547227940559387\n",
            "Epoch:0/1,\tIteration:101300,\tLoss:4.769476981163025\n",
            "Epoch:0/1,\tIteration:101400,\tLoss:5.719223752021789\n",
            "Epoch:0/1,\tIteration:101500,\tLoss:4.78973717212677\n",
            "Epoch:0/1,\tIteration:101600,\tLoss:5.456253502368927\n",
            "Epoch:0/1,\tIteration:101700,\tLoss:4.746887049674988\n",
            "Epoch:0/1,\tIteration:101800,\tLoss:5.466738667488098\n",
            "Epoch:0/1,\tIteration:101900,\tLoss:4.227574119567871\n",
            "Epoch:0/1,\tIteration:102000,\tLoss:5.2783610320091245\n",
            "Epoch:0/1,\tIteration:102100,\tLoss:5.154601562023163\n",
            "Epoch:0/1,\tIteration:102200,\tLoss:5.029331786632538\n",
            "Epoch:0/1,\tIteration:102300,\tLoss:5.1842847394943234\n",
            "Epoch:0/1,\tIteration:102400,\tLoss:5.308202900886536\n",
            "Epoch:0/1,\tIteration:102500,\tLoss:5.465742506980896\n",
            "Epoch:0/1,\tIteration:102600,\tLoss:4.9084062266349795\n",
            "Epoch:0/1,\tIteration:102700,\tLoss:5.021552705764771\n",
            "Epoch:0/1,\tIteration:102800,\tLoss:5.757551870346069\n",
            "Epoch:0/1,\tIteration:102900,\tLoss:5.234069678783417\n",
            "Epoch:0/1,\tIteration:103000,\tLoss:5.2818002510070805\n",
            "Epoch:0/1,\tIteration:103100,\tLoss:5.4902563619613645\n",
            "Epoch:0/1,\tIteration:103200,\tLoss:5.45696825504303\n",
            "Epoch:0/1,\tIteration:103300,\tLoss:5.337653720378876\n",
            "Epoch:0/1,\tIteration:103400,\tLoss:4.969246203899384\n",
            "Epoch:0/1,\tIteration:103500,\tLoss:5.2660673809051515\n",
            "Epoch:0/1,\tIteration:103600,\tLoss:5.386781997680664\n",
            "Epoch:0/1,\tIteration:103700,\tLoss:5.298391847610474\n",
            "Epoch:0/1,\tIteration:103800,\tLoss:5.549631066322327\n",
            "Epoch:0/1,\tIteration:103900,\tLoss:5.715854606628418\n",
            "Epoch:0/1,\tIteration:104000,\tLoss:5.404905133247375\n",
            "Epoch:0/1,\tIteration:104100,\tLoss:5.314134712219238\n",
            "Epoch:0/1,\tIteration:104200,\tLoss:5.465973715782166\n",
            "Epoch:0/1,\tIteration:104300,\tLoss:5.487976589202881\n",
            "Epoch:0/1,\tIteration:104400,\tLoss:5.492455034255982\n",
            "Epoch:0/1,\tIteration:104500,\tLoss:5.663864994049073\n",
            "Epoch:0/1,\tIteration:104600,\tLoss:5.503348345756531\n",
            "Epoch:0/1,\tIteration:104700,\tLoss:5.409362070560455\n",
            "Epoch:0/1,\tIteration:104800,\tLoss:5.306230943202973\n",
            "Epoch:0/1,\tIteration:104900,\tLoss:5.362426810264587\n",
            "Epoch:0/1,\tIteration:105000,\tLoss:5.4005908155441285\n",
            "Epoch:0/1,\tIteration:105100,\tLoss:5.396207432746888\n",
            "Epoch:0/1,\tIteration:105200,\tLoss:5.477332382202149\n",
            "Epoch:0/1,\tIteration:105300,\tLoss:5.449370512962341\n",
            "Epoch:0/1,\tIteration:105400,\tLoss:5.29968897819519\n",
            "Epoch:0/1,\tIteration:105500,\tLoss:5.372386960983277\n",
            "Epoch:0/1,\tIteration:105600,\tLoss:5.5687005019187925\n",
            "Epoch:0/1,\tIteration:105700,\tLoss:5.662700419425964\n",
            "Epoch:0/1,\tIteration:105800,\tLoss:5.565380291938782\n",
            "Epoch:0/1,\tIteration:105900,\tLoss:5.44812656879425\n",
            "Epoch:0/1,\tIteration:106000,\tLoss:5.771598858833313\n",
            "Epoch:0/1,\tIteration:106100,\tLoss:5.347774753570556\n",
            "Epoch:0/1,\tIteration:106200,\tLoss:5.1319488072395325\n",
            "Epoch:0/1,\tIteration:106300,\tLoss:5.26521833896637\n",
            "Epoch:0/1,\tIteration:106400,\tLoss:5.2744739818573\n",
            "Epoch:0/1,\tIteration:106500,\tLoss:5.745921757221222\n",
            "Epoch:0/1,\tIteration:106600,\tLoss:5.405417006015778\n",
            "Epoch:0/1,\tIteration:106700,\tLoss:5.648712434768677\n",
            "Epoch:0/1,\tIteration:106800,\tLoss:5.543672575950622\n",
            "Epoch:0/1,\tIteration:106900,\tLoss:5.762860188484192\n",
            "Epoch:0/1,\tIteration:107000,\tLoss:5.578183360099793\n",
            "Epoch:0/1,\tIteration:107100,\tLoss:5.415733504295349\n",
            "Epoch:0/1,\tIteration:107200,\tLoss:5.5643217611312865\n",
            "Epoch:0/1,\tIteration:107300,\tLoss:5.519122419357299\n",
            "Epoch:0/1,\tIteration:107400,\tLoss:5.437942831516266\n",
            "Epoch:0/1,\tIteration:107500,\tLoss:5.364320616722107\n",
            "Epoch:0/1,\tIteration:107600,\tLoss:5.53124409198761\n",
            "Epoch:0/1,\tIteration:107700,\tLoss:5.275943379402161\n",
            "Epoch:0/1,\tIteration:107800,\tLoss:5.41293338060379\n",
            "Epoch:0/1,\tIteration:107900,\tLoss:5.473015177249908\n",
            "Epoch:0/1,\tIteration:108000,\tLoss:5.2283639287948604\n",
            "Epoch:0/1,\tIteration:108100,\tLoss:5.417210578918457\n",
            "Epoch:0/1,\tIteration:108200,\tLoss:5.4829669761657716\n",
            "Epoch:0/1,\tIteration:108300,\tLoss:5.291228475570679\n",
            "Epoch:0/1,\tIteration:108400,\tLoss:5.370034427642822\n",
            "Epoch:0/1,\tIteration:108500,\tLoss:5.484157710075379\n",
            "Epoch:0/1,\tIteration:108600,\tLoss:5.419431598186493\n",
            "Epoch:0/1,\tIteration:108700,\tLoss:5.425857377052307\n",
            "Epoch:0/1,\tIteration:108800,\tLoss:5.357323570251465\n",
            "Epoch:0/1,\tIteration:108900,\tLoss:5.3102029824256896\n",
            "Epoch:0/1,\tIteration:109000,\tLoss:5.657339353561401\n",
            "Epoch:0/1,\tIteration:109100,\tLoss:5.410168824195861\n",
            "Epoch:0/1,\tIteration:109200,\tLoss:5.479584536552429\n",
            "Epoch:0/1,\tIteration:109300,\tLoss:5.490579328536987\n",
            "Epoch:0/1,\tIteration:109400,\tLoss:5.3332219982147215\n",
            "Epoch:0/1,\tIteration:109500,\tLoss:5.307992820739746\n",
            "Epoch:0/1,\tIteration:109600,\tLoss:5.121559450626373\n",
            "Epoch:0/1,\tIteration:109700,\tLoss:5.439134476184845\n",
            "Epoch:0/1,\tIteration:109800,\tLoss:5.2363822722435\n",
            "Epoch:0/1,\tIteration:109900,\tLoss:5.270590622425079\n",
            "Epoch:0/1,\tIteration:110000,\tLoss:5.871433529853821\n",
            "Epoch:0/1,\tIteration:110100,\tLoss:5.457029728889466\n",
            "Epoch:0/1,\tIteration:110200,\tLoss:5.509752454757691\n",
            "Epoch:0/1,\tIteration:110300,\tLoss:5.477375383377075\n",
            "Epoch:0/1,\tIteration:110400,\tLoss:5.830895867347717\n",
            "Epoch:0/1,\tIteration:110500,\tLoss:5.566947860717773\n",
            "Epoch:0/1,\tIteration:110600,\tLoss:5.757600557804108\n",
            "Epoch:0/1,\tIteration:110700,\tLoss:5.597448997497558\n",
            "Epoch:0/1,\tIteration:110800,\tLoss:5.823808655738831\n",
            "Epoch:0/1,\tIteration:110900,\tLoss:5.582781322002411\n",
            "Epoch:0/1,\tIteration:111000,\tLoss:5.671499514579773\n",
            "Epoch:0/1,\tIteration:111100,\tLoss:5.510192272663116\n",
            "Epoch:0/1,\tIteration:111200,\tLoss:5.648055331707001\n",
            "Epoch:0/1,\tIteration:111300,\tLoss:5.5151842594146725\n",
            "Epoch:0/1,\tIteration:111400,\tLoss:5.563489718437195\n",
            "Epoch:0/1,\tIteration:111500,\tLoss:5.684339156150818\n",
            "Epoch:0/1,\tIteration:111600,\tLoss:5.530066800117493\n",
            "Epoch:0/1,\tIteration:111700,\tLoss:5.2441283488273625\n",
            "Epoch:0/1,\tIteration:111800,\tLoss:5.119222328662873\n",
            "Epoch:0/1,\tIteration:111900,\tLoss:5.447073392868042\n",
            "Epoch:0/1,\tIteration:112000,\tLoss:5.408170132637024\n",
            "Epoch:0/1,\tIteration:112100,\tLoss:5.165975048542022\n",
            "Epoch:0/1,\tIteration:112200,\tLoss:5.068233208656311\n",
            "Epoch:0/1,\tIteration:112300,\tLoss:5.26651992559433\n",
            "Epoch:0/1,\tIteration:112400,\tLoss:4.905545287132263\n",
            "Epoch:0/1,\tIteration:112500,\tLoss:5.555374825000763\n",
            "Epoch:0/1,\tIteration:112600,\tLoss:5.705528419017792\n",
            "Epoch:0/1,\tIteration:112700,\tLoss:5.467095727920532\n",
            "Epoch:0/1,\tIteration:112800,\tLoss:5.584267749786377\n",
            "Epoch:0/1,\tIteration:112900,\tLoss:5.4581247901916505\n",
            "Epoch:0/1,\tIteration:113000,\tLoss:5.528443322181702\n",
            "Epoch:0/1,\tIteration:113100,\tLoss:5.6537575960159305\n",
            "Epoch:0/1,\tIteration:113200,\tLoss:5.542756285667419\n",
            "Epoch:0/1,\tIteration:113300,\tLoss:5.482172174453735\n",
            "Epoch:0/1,\tIteration:113400,\tLoss:5.603523960113526\n",
            "Epoch:0/1,\tIteration:113500,\tLoss:5.389883632659912\n",
            "Epoch:0/1,\tIteration:113600,\tLoss:5.4891437864303585\n",
            "Epoch:0/1,\tIteration:113700,\tLoss:5.457945237159729\n",
            "Epoch:0/1,\tIteration:113800,\tLoss:5.319692161083221\n",
            "Epoch:0/1,\tIteration:113900,\tLoss:5.863513851165772\n",
            "Epoch:0/1,\tIteration:114000,\tLoss:5.282233910560608\n",
            "Epoch:0/1,\tIteration:114100,\tLoss:5.535293803215027\n",
            "Epoch:0/1,\tIteration:114200,\tLoss:5.412477016448975\n",
            "Epoch:0/1,\tIteration:114300,\tLoss:5.447464332580567\n",
            "Epoch:0/1,\tIteration:114400,\tLoss:5.306206760406494\n",
            "Epoch:0/1,\tIteration:114500,\tLoss:5.150886545181274\n",
            "Epoch:0/1,\tIteration:114600,\tLoss:5.586421015262604\n",
            "Epoch:0/1,\tIteration:114700,\tLoss:5.569167311191559\n",
            "Epoch:0/1,\tIteration:114800,\tLoss:5.380138993263245\n",
            "Epoch:0/1,\tIteration:114900,\tLoss:5.760652956962585\n",
            "Epoch:0/1,\tIteration:115000,\tLoss:5.325211606025696\n",
            "Epoch:0/1,\tIteration:115100,\tLoss:5.476961884498596\n",
            "Epoch:0/1,\tIteration:115200,\tLoss:5.616065292358399\n",
            "Epoch:0/1,\tIteration:115300,\tLoss:5.550418155193329\n",
            "Epoch:0/1,\tIteration:115400,\tLoss:4.392855417728424\n",
            "Epoch:0/1,\tIteration:115500,\tLoss:5.6540070509910585\n",
            "Epoch:0/1,\tIteration:115600,\tLoss:5.74008584022522\n",
            "Epoch:0/1,\tIteration:115700,\tLoss:5.461341495513916\n",
            "Epoch:0/1,\tIteration:115800,\tLoss:5.52336356639862\n",
            "Epoch:0/1,\tIteration:115900,\tLoss:5.531126770973206\n",
            "Epoch:0/1,\tIteration:116000,\tLoss:5.497478899955749\n",
            "Epoch:0/1,\tIteration:116100,\tLoss:5.361562423706054\n",
            "Epoch:0/1,\tIteration:116200,\tLoss:5.394891426563263\n",
            "Epoch:0/1,\tIteration:116300,\tLoss:5.38927152633667\n",
            "Epoch:0/1,\tIteration:116400,\tLoss:5.396157007217408\n",
            "Epoch:0/1,\tIteration:116500,\tLoss:5.357859237194061\n",
            "Epoch:0/1,\tIteration:116600,\tLoss:5.46544225692749\n",
            "Epoch:0/1,\tIteration:116700,\tLoss:5.404467911720276\n",
            "Epoch:0/1,\tIteration:116800,\tLoss:5.664705681800842\n",
            "Epoch:0/1,\tIteration:116900,\tLoss:5.415641677379608\n",
            "Epoch:0/1,\tIteration:117000,\tLoss:5.252209839820861\n",
            "Epoch:0/1,\tIteration:117100,\tLoss:5.7168959355354305\n",
            "Epoch:0/1,\tIteration:117200,\tLoss:5.608011045455933\n",
            "Epoch:0/1,\tIteration:117300,\tLoss:5.922965221405029\n",
            "Epoch:0/1,\tIteration:117400,\tLoss:5.488675537109375\n",
            "Epoch:0/1,\tIteration:117500,\tLoss:5.493122453689575\n",
            "Epoch:0/1,\tIteration:117600,\tLoss:5.280614800453186\n",
            "Epoch:0/1,\tIteration:117700,\tLoss:5.705090117454529\n",
            "Epoch:0/1,\tIteration:117800,\tLoss:5.552465834617615\n",
            "Epoch:0/1,\tIteration:117900,\tLoss:5.319431810379029\n",
            "Epoch:0/1,\tIteration:118000,\tLoss:5.766238346099853\n",
            "Epoch:0/1,\tIteration:118100,\tLoss:5.651221928596496\n",
            "Epoch:0/1,\tIteration:118200,\tLoss:5.655977644920349\n",
            "Epoch:0/1,\tIteration:118300,\tLoss:5.698377878665924\n",
            "Epoch:0/1,\tIteration:118400,\tLoss:5.5341842746734615\n",
            "Epoch:0/1,\tIteration:118500,\tLoss:5.7421774840354916\n",
            "Epoch:0/1,\tIteration:118600,\tLoss:5.582583127021789\n",
            "Epoch:0/1,\tIteration:118700,\tLoss:5.785413663387299\n",
            "Epoch:0/1,\tIteration:118800,\tLoss:4.747938702106476\n",
            "Epoch:0/1,\tIteration:118900,\tLoss:5.576228022575378\n",
            "Epoch:0/1,\tIteration:119000,\tLoss:5.697378237247467\n",
            "Epoch:0/1,\tIteration:119100,\tLoss:5.635400247573853\n",
            "Epoch:0/1,\tIteration:119200,\tLoss:5.806556930541992\n",
            "Epoch:0/1,\tIteration:119300,\tLoss:5.524921870231628\n",
            "Epoch:0/1,\tIteration:119400,\tLoss:5.533526291847229\n",
            "Epoch:0/1,\tIteration:119500,\tLoss:5.461217708587647\n",
            "Epoch:0/1,\tIteration:119600,\tLoss:5.59717966556549\n",
            "Epoch:0/1,\tIteration:119700,\tLoss:5.490328302383423\n",
            "Epoch:0/1,\tIteration:119800,\tLoss:5.677490692138672\n",
            "Epoch:0/1,\tIteration:119900,\tLoss:5.402853999137879\n",
            "Epoch:0/1,\tIteration:120000,\tLoss:5.531311602592468\n",
            "Epoch:0/1,\tIteration:120100,\tLoss:5.553789734840393\n",
            "Epoch:0/1,\tIteration:120200,\tLoss:5.351831512451172\n",
            "Epoch:0/1,\tIteration:120300,\tLoss:5.372854790687561\n",
            "Epoch:0/1,\tIteration:120400,\tLoss:5.165132040977478\n",
            "Epoch:0/1,\tIteration:120500,\tLoss:5.002848978042603\n",
            "Epoch:0/1,\tIteration:120600,\tLoss:5.285924680233002\n",
            "Epoch:0/1,\tIteration:120700,\tLoss:5.020846412181855\n",
            "Epoch:0/1,\tIteration:120800,\tLoss:5.2939127469062806\n",
            "Epoch:0/1,\tIteration:120900,\tLoss:5.302286787033081\n",
            "Epoch:0/1,\tIteration:121000,\tLoss:5.518632650375366\n",
            "Epoch:0/1,\tIteration:121100,\tLoss:4.925099563598633\n",
            "Epoch:0/1,\tIteration:121200,\tLoss:5.140533714294434\n",
            "Epoch:0/1,\tIteration:121300,\tLoss:5.088806049823761\n",
            "Epoch:0/1,\tIteration:121400,\tLoss:5.070603125095367\n",
            "Epoch:0/1,\tIteration:121500,\tLoss:4.805558083057403\n",
            "Epoch:0/1,\tIteration:121600,\tLoss:4.98128351688385\n",
            "Epoch:0/1,\tIteration:121700,\tLoss:6.044642572402954\n",
            "Epoch:0/1,\tIteration:121800,\tLoss:5.529217805862427\n",
            "Epoch:0/1,\tIteration:121900,\tLoss:5.606860961914062\n",
            "Epoch:0/1,\tIteration:122000,\tLoss:4.664435613155365\n",
            "Epoch:0/1,\tIteration:122100,\tLoss:4.797937400341034\n",
            "Epoch:0/1,\tIteration:122200,\tLoss:5.527782649993896\n",
            "Epoch:0/1,\tIteration:122300,\tLoss:5.474823265075684\n",
            "Epoch:0/1,\tIteration:122400,\tLoss:5.324891030788422\n",
            "Epoch:0/1,\tIteration:122500,\tLoss:5.468780643939972\n",
            "Epoch:0/1,\tIteration:122600,\tLoss:5.710886101722718\n",
            "Epoch:0/1,\tIteration:122700,\tLoss:5.466953577995301\n",
            "Epoch:0/1,\tIteration:122800,\tLoss:5.512413494586944\n",
            "Epoch:0/1,\tIteration:122900,\tLoss:5.40719042301178\n",
            "Epoch:0/1,\tIteration:123000,\tLoss:5.549167981147766\n",
            "Epoch:0/1,\tIteration:123100,\tLoss:5.154106879234314\n",
            "Epoch:0/1,\tIteration:123200,\tLoss:5.533673951625824\n",
            "Epoch:0/1,\tIteration:123300,\tLoss:5.612794642448425\n",
            "Epoch:0/1,\tIteration:123400,\tLoss:5.699416532516479\n",
            "Epoch:0/1,\tIteration:123500,\tLoss:5.655902376174927\n",
            "Epoch:0/1,\tIteration:123600,\tLoss:5.447127647399903\n",
            "Epoch:0/1,\tIteration:123700,\tLoss:5.5206369256973264\n",
            "Epoch:0/1,\tIteration:123800,\tLoss:5.340269758701324\n",
            "Epoch:0/1,\tIteration:123900,\tLoss:5.458709979057312\n",
            "Epoch:0/1,\tIteration:124000,\tLoss:5.513429942131043\n",
            "Epoch:0/1,\tIteration:124100,\tLoss:4.690869381427765\n",
            "Epoch:0/1,\tIteration:124200,\tLoss:5.230165255069733\n",
            "Epoch:0/1,\tIteration:124300,\tLoss:5.239250464439392\n",
            "Epoch:0/1,\tIteration:124400,\tLoss:4.478415288925171\n",
            "Epoch:0/1,\tIteration:124500,\tLoss:5.364942212104797\n",
            "Epoch:0/1,\tIteration:124600,\tLoss:4.8673006272315975\n",
            "Epoch:0/1,\tIteration:124700,\tLoss:5.537853703498841\n",
            "Epoch:0/1,\tIteration:124800,\tLoss:5.861264405250549\n",
            "Epoch:0/1,\tIteration:124900,\tLoss:5.63241828918457\n",
            "Epoch:0/1,\tIteration:125000,\tLoss:5.5694389414787295\n",
            "Epoch:0/1,\tIteration:125100,\tLoss:5.64432005405426\n",
            "Epoch:0/1,\tIteration:125200,\tLoss:5.502533941268921\n",
            "Epoch:0/1,\tIteration:125300,\tLoss:5.377145366668701\n",
            "Epoch:0/1,\tIteration:125400,\tLoss:5.2695478820800785\n",
            "Epoch:0/1,\tIteration:125500,\tLoss:5.2247170019149785\n",
            "Epoch:0/1,\tIteration:125600,\tLoss:5.549052066802979\n",
            "Epoch:0/1,\tIteration:125700,\tLoss:5.600389461517334\n",
            "Epoch:0/1,\tIteration:125800,\tLoss:5.153472344875336\n",
            "Epoch:0/1,\tIteration:125900,\tLoss:5.255179884433747\n",
            "Epoch:0/1,\tIteration:126000,\tLoss:5.815345463752746\n",
            "Epoch:0/1,\tIteration:126100,\tLoss:5.805882787704467\n",
            "Epoch:0/1,\tIteration:126200,\tLoss:5.543024740219116\n",
            "Epoch:0/1,\tIteration:126300,\tLoss:5.397625298500061\n",
            "Epoch:0/1,\tIteration:126400,\tLoss:5.425016233921051\n",
            "Epoch:0/1,\tIteration:126500,\tLoss:5.468523616790772\n",
            "Epoch:0/1,\tIteration:126600,\tLoss:5.574710025787353\n",
            "Epoch:0/1,\tIteration:126700,\tLoss:5.696654043197632\n",
            "Epoch:0/1,\tIteration:126800,\tLoss:5.6750720548629765\n",
            "Epoch:0/1,\tIteration:126900,\tLoss:5.4766189885139465\n",
            "Epoch:0/1,\tIteration:127000,\tLoss:5.120519998073578\n",
            "Epoch:0/1,\tIteration:127100,\tLoss:5.66205219745636\n",
            "Epoch:0/1,\tIteration:127200,\tLoss:5.620994973182678\n",
            "Epoch:0/1,\tIteration:127300,\tLoss:5.514506692886353\n",
            "Epoch:0/1,\tIteration:127400,\tLoss:5.466905426979065\n",
            "Epoch:0/1,\tIteration:127500,\tLoss:5.581667342185974\n",
            "Epoch:0/1,\tIteration:127600,\tLoss:5.508868498802185\n",
            "Epoch:0/1,\tIteration:127700,\tLoss:5.437330706119537\n",
            "Epoch:0/1,\tIteration:127800,\tLoss:5.611668348312378\n",
            "Epoch:0/1,\tIteration:127900,\tLoss:4.897825112342835\n",
            "Epoch:0/1,\tIteration:128000,\tLoss:5.131345777511597\n",
            "Epoch:0/1,\tIteration:128100,\tLoss:5.226218626499176\n",
            "Epoch:0/1,\tIteration:128200,\tLoss:5.179119544029236\n",
            "Epoch:0/1,\tIteration:128300,\tLoss:5.283559334278107\n",
            "Epoch:0/1,\tIteration:128400,\tLoss:5.526121253967285\n",
            "Epoch:0/1,\tIteration:128500,\tLoss:5.506014640331268\n",
            "Epoch:0/1,\tIteration:128600,\tLoss:5.4809055161476135\n",
            "Epoch:0/1,\tIteration:128700,\tLoss:5.484953603744507\n",
            "Epoch:0/1,\tIteration:128800,\tLoss:5.031293997764587\n",
            "Epoch:0/1,\tIteration:128900,\tLoss:4.870221018791199\n",
            "Epoch:0/1,\tIteration:129000,\tLoss:5.24326474905014\n",
            "Epoch:0/1,\tIteration:129100,\tLoss:5.561017556190491\n",
            "Epoch:0/1,\tIteration:129200,\tLoss:5.610958843231201\n",
            "Epoch:0/1,\tIteration:129300,\tLoss:5.4744446754455565\n",
            "Epoch:0/1,\tIteration:129400,\tLoss:5.559313764572144\n",
            "Epoch:0/1,\tIteration:129500,\tLoss:5.536250867843628\n",
            "Epoch:0/1,\tIteration:129600,\tLoss:4.2863322401046755\n",
            "Epoch:0/1,\tIteration:129700,\tLoss:5.352623512744904\n",
            "Epoch:0/1,\tIteration:129800,\tLoss:5.511025869846344\n",
            "Epoch:0/1,\tIteration:129900,\tLoss:5.633238115310669\n",
            "Epoch:0/1,\tIteration:130000,\tLoss:5.489887845516205\n",
            "Epoch:0/1,\tIteration:130100,\tLoss:5.578920407295227\n",
            "Epoch:0/1,\tIteration:130200,\tLoss:5.414832994937897\n",
            "Epoch:0/1,\tIteration:130300,\tLoss:5.551031579971314\n",
            "Epoch:0/1,\tIteration:130400,\tLoss:5.493688132762909\n",
            "Epoch:0/1,\tIteration:130500,\tLoss:5.150600657463074\n",
            "Epoch:0/1,\tIteration:130600,\tLoss:4.749338862895965\n",
            "Epoch:0/1,\tIteration:130700,\tLoss:5.357387983798981\n",
            "Epoch:0/1,\tIteration:130800,\tLoss:5.383034901618958\n",
            "Epoch:0/1,\tIteration:130900,\tLoss:5.463546981811524\n",
            "Epoch:0/1,\tIteration:131000,\tLoss:5.39968332529068\n",
            "Epoch:0/1,\tIteration:131100,\tLoss:5.360431485176086\n",
            "Epoch:0/1,\tIteration:131200,\tLoss:5.512063364982605\n",
            "Epoch:0/1,\tIteration:131300,\tLoss:5.462336111068725\n",
            "Epoch:0/1,\tIteration:131400,\tLoss:5.508415350914001\n",
            "Epoch:0/1,\tIteration:131500,\tLoss:5.5654145622253415\n",
            "Epoch:0/1,\tIteration:131600,\tLoss:5.405505011081695\n",
            "Epoch:0/1,\tIteration:131700,\tLoss:5.519126615524292\n",
            "Epoch:0/1,\tIteration:131800,\tLoss:5.393172569274903\n",
            "Epoch:0/1,\tIteration:131900,\tLoss:5.5484960699081425\n",
            "Epoch:0/1,\tIteration:132000,\tLoss:5.452526698112488\n",
            "Epoch:0/1,\tIteration:132100,\tLoss:5.7254575872421265\n",
            "Epoch:0/1,\tIteration:132200,\tLoss:5.7406606864929195\n",
            "Epoch:0/1,\tIteration:132300,\tLoss:5.44254729270935\n",
            "Epoch:0/1,\tIteration:132400,\tLoss:5.526241855621338\n",
            "Epoch:0/1,\tIteration:132500,\tLoss:5.595703926086426\n",
            "Epoch:0/1,\tIteration:132600,\tLoss:5.592907605171203\n",
            "Epoch:0/1,\tIteration:132700,\tLoss:5.356473925113678\n",
            "Epoch:0/1,\tIteration:132800,\tLoss:5.544291443824768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "mvZuqd6igbD2",
        "colab_type": "code",
        "outputId": "cbde0599-5a66-44aa-b6b4-175e4f2081d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## evaluation\n",
        "## DON'T CHANGE THIS CELL IN ANY WAY\n",
        "\n",
        "assert loss_history[-1] < 6.5\n",
        "\n",
        "print('Well done!')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Well done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRCFzsSugbD4",
        "colab_type": "text"
      },
      "source": [
        "### Nearest words\n",
        "\n",
        "So far, we trained the __CBOW__ successfully, now it is time to explore it more. In this part, we want to find the $k$ nearest word to a given word, i.e., nearby in the vector space.\n",
        "\n",
        "<img src=\"https://i0.wp.com/i.imgur.com/IeZt839.png\" alt=\"img\" width=\"480px\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TGDadr8o06l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "7241e9b3-d67c-4d99-cc02-1cccd1eb8b25"
      },
      "source": [
        "embedding = model.embed_in.data\n",
        "embedding.shape\n",
        "token_to_id\n",
        "print(embedding[:, 0:1].shape)\n",
        "print(embedding.shape)\n",
        "temp = embedding[:, 0:1].t()\n",
        "temp.shape"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 1])\n",
            "torch.Size([100, 10000])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNDP8WFWgbD5",
        "colab_type": "text"
      },
      "source": [
        "Define a helper function to retrieve the corresponding vector for a given word:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "HjeXxHDWgbD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# be sure jupyter session is not terminated!\n",
        "# use token_to_id to retrieve the index\n",
        "\n",
        "def get_vector(embedding, word):\n",
        "    \"\"\"\n",
        "    :argument:\n",
        "        embedding (matrix): embedding matrix \n",
        "        word (str): The given input\n",
        "    :return:\n",
        "        word-vector for a given word\n",
        "    \"\"\"\n",
        "    ############### for student ################\n",
        "    \n",
        "    id = token_to_id[word]\n",
        "    return embedding[:, id:id + 1]\n",
        "\n",
        "    ############################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "zI8y0yu4gbD7",
        "colab_type": "code",
        "outputId": "564eb37b-ee73-45e3-c9b6-71d84ffeb0cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## evaluation\n",
        "## DON'T CHANGE THIS CELL IN ANY WAY\n",
        "\n",
        "embedding = model.embed_in.data\n",
        "\n",
        "assert get_vector(embedding, 'the').shape == torch.Size([100, 1]), \"vector size should be (embed_dim, 1)\"\n",
        "assert np.allclose(embedding[:,(0,)].data.cpu().numpy(), get_vector(embedding, 'UNK').data.cpu().numpy()), \"Do you retrieve correct vector?\"\n",
        "print('Well done!')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Well done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fFMZdAZgbD9",
        "colab_type": "text"
      },
      "source": [
        "Define a function to return the list of $k$ most similar words, e.g., based on `cosine-similarity`, to a given word:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH5w4fLsxACm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2fa0c9f9-c79a-45b1-ef37-f7092ed38059"
      },
      "source": [
        "id_to_token"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'UNK',\n",
              " 1: 'the',\n",
              " 2: 'of',\n",
              " 3: 'and',\n",
              " 4: 'one',\n",
              " 5: 'in',\n",
              " 6: 'a',\n",
              " 7: 'to',\n",
              " 8: 'zero',\n",
              " 9: 'nine',\n",
              " 10: 'two',\n",
              " 11: 'is',\n",
              " 12: 'as',\n",
              " 13: 'eight',\n",
              " 14: 'for',\n",
              " 15: 's',\n",
              " 16: 'five',\n",
              " 17: 'three',\n",
              " 18: 'was',\n",
              " 19: 'by',\n",
              " 20: 'that',\n",
              " 21: 'four',\n",
              " 22: 'six',\n",
              " 23: 'seven',\n",
              " 24: 'with',\n",
              " 25: 'on',\n",
              " 26: 'are',\n",
              " 27: 'it',\n",
              " 28: 'from',\n",
              " 29: 'or',\n",
              " 30: 'his',\n",
              " 31: 'an',\n",
              " 32: 'be',\n",
              " 33: 'this',\n",
              " 34: 'which',\n",
              " 35: 'at',\n",
              " 36: 'he',\n",
              " 37: 'also',\n",
              " 38: 'not',\n",
              " 39: 'have',\n",
              " 40: 'were',\n",
              " 41: 'has',\n",
              " 42: 'but',\n",
              " 43: 'other',\n",
              " 44: 'their',\n",
              " 45: 'its',\n",
              " 46: 'first',\n",
              " 47: 'they',\n",
              " 48: 'some',\n",
              " 49: 'had',\n",
              " 50: 'all',\n",
              " 51: 'more',\n",
              " 52: 'most',\n",
              " 53: 'can',\n",
              " 54: 'been',\n",
              " 55: 'such',\n",
              " 56: 'many',\n",
              " 57: 'who',\n",
              " 58: 'new',\n",
              " 59: 'used',\n",
              " 60: 'there',\n",
              " 61: 'after',\n",
              " 62: 'when',\n",
              " 63: 'into',\n",
              " 64: 'american',\n",
              " 65: 'time',\n",
              " 66: 'these',\n",
              " 67: 'only',\n",
              " 68: 'see',\n",
              " 69: 'may',\n",
              " 70: 'than',\n",
              " 71: 'world',\n",
              " 72: 'i',\n",
              " 73: 'b',\n",
              " 74: 'would',\n",
              " 75: 'd',\n",
              " 76: 'no',\n",
              " 77: 'however',\n",
              " 78: 'between',\n",
              " 79: 'about',\n",
              " 80: 'over',\n",
              " 81: 'years',\n",
              " 82: 'states',\n",
              " 83: 'people',\n",
              " 84: 'war',\n",
              " 85: 'during',\n",
              " 86: 'united',\n",
              " 87: 'known',\n",
              " 88: 'if',\n",
              " 89: 'called',\n",
              " 90: 'use',\n",
              " 91: 'th',\n",
              " 92: 'system',\n",
              " 93: 'often',\n",
              " 94: 'state',\n",
              " 95: 'so',\n",
              " 96: 'history',\n",
              " 97: 'will',\n",
              " 98: 'up',\n",
              " 99: 'while',\n",
              " 100: 'where',\n",
              " 101: 'city',\n",
              " 102: 'being',\n",
              " 103: 'english',\n",
              " 104: 'then',\n",
              " 105: 'any',\n",
              " 106: 'both',\n",
              " 107: 'under',\n",
              " 108: 'out',\n",
              " 109: 'made',\n",
              " 110: 'well',\n",
              " 111: 'her',\n",
              " 112: 'e',\n",
              " 113: 'number',\n",
              " 114: 'government',\n",
              " 115: 'them',\n",
              " 116: 'm',\n",
              " 117: 'later',\n",
              " 118: 'since',\n",
              " 119: 'him',\n",
              " 120: 'part',\n",
              " 121: 'name',\n",
              " 122: 'c',\n",
              " 123: 'century',\n",
              " 124: 'through',\n",
              " 125: 'because',\n",
              " 126: 'x',\n",
              " 127: 'university',\n",
              " 128: 'early',\n",
              " 129: 'life',\n",
              " 130: 'british',\n",
              " 131: 'year',\n",
              " 132: 'like',\n",
              " 133: 'same',\n",
              " 134: 'including',\n",
              " 135: 'became',\n",
              " 136: 'example',\n",
              " 137: 'day',\n",
              " 138: 'each',\n",
              " 139: 'even',\n",
              " 140: 'work',\n",
              " 141: 'language',\n",
              " 142: 'although',\n",
              " 143: 'several',\n",
              " 144: 'form',\n",
              " 145: 'john',\n",
              " 146: 'u',\n",
              " 147: 'national',\n",
              " 148: 'very',\n",
              " 149: 'much',\n",
              " 150: 'g',\n",
              " 151: 'french',\n",
              " 152: 'before',\n",
              " 153: 'general',\n",
              " 154: 'what',\n",
              " 155: 't',\n",
              " 156: 'against',\n",
              " 157: 'n',\n",
              " 158: 'high',\n",
              " 159: 'links',\n",
              " 160: 'could',\n",
              " 161: 'based',\n",
              " 162: 'those',\n",
              " 163: 'now',\n",
              " 164: 'second',\n",
              " 165: 'de',\n",
              " 166: 'music',\n",
              " 167: 'another',\n",
              " 168: 'large',\n",
              " 169: 'she',\n",
              " 170: 'f',\n",
              " 171: 'external',\n",
              " 172: 'german',\n",
              " 173: 'different',\n",
              " 174: 'modern',\n",
              " 175: 'great',\n",
              " 176: 'do',\n",
              " 177: 'common',\n",
              " 178: 'set',\n",
              " 179: 'list',\n",
              " 180: 'south',\n",
              " 181: 'series',\n",
              " 182: 'major',\n",
              " 183: 'game',\n",
              " 184: 'power',\n",
              " 185: 'long',\n",
              " 186: 'country',\n",
              " 187: 'king',\n",
              " 188: 'law',\n",
              " 189: 'group',\n",
              " 190: 'film',\n",
              " 191: 'still',\n",
              " 192: 'until',\n",
              " 193: 'north',\n",
              " 194: 'international',\n",
              " 195: 'term',\n",
              " 196: 'we',\n",
              " 197: 'end',\n",
              " 198: 'book',\n",
              " 199: 'found',\n",
              " 200: 'own',\n",
              " 201: 'political',\n",
              " 202: 'party',\n",
              " 203: 'order',\n",
              " 204: 'usually',\n",
              " 205: 'president',\n",
              " 206: 'church',\n",
              " 207: 'you',\n",
              " 208: 'death',\n",
              " 209: 'theory',\n",
              " 210: 'area',\n",
              " 211: 'around',\n",
              " 212: 'include',\n",
              " 213: 'god',\n",
              " 214: 'ii',\n",
              " 215: 'way',\n",
              " 216: 'did',\n",
              " 217: 'military',\n",
              " 218: 'population',\n",
              " 219: 'using',\n",
              " 220: 'though',\n",
              " 221: 'small',\n",
              " 222: 'following',\n",
              " 223: 'within',\n",
              " 224: 'non',\n",
              " 225: 'human',\n",
              " 226: 'left',\n",
              " 227: 'main',\n",
              " 228: 'among',\n",
              " 229: 'point',\n",
              " 230: 'r',\n",
              " 231: 'due',\n",
              " 232: 'p',\n",
              " 233: 'considered',\n",
              " 234: 'public',\n",
              " 235: 'popular',\n",
              " 236: 'computer',\n",
              " 237: 'west',\n",
              " 238: 'family',\n",
              " 239: 'east',\n",
              " 240: 'information',\n",
              " 241: 'important',\n",
              " 242: 'european',\n",
              " 243: 'man',\n",
              " 244: 'sometimes',\n",
              " 245: 'right',\n",
              " 246: 'old',\n",
              " 247: 'free',\n",
              " 248: 'word',\n",
              " 249: 'without',\n",
              " 250: 'last',\n",
              " 251: 'us',\n",
              " 252: 'members',\n",
              " 253: 'given',\n",
              " 254: 'times',\n",
              " 255: 'roman',\n",
              " 256: 'make',\n",
              " 257: 'h',\n",
              " 258: 'age',\n",
              " 259: 'place',\n",
              " 260: 'l',\n",
              " 261: 'thus',\n",
              " 262: 'science',\n",
              " 263: 'case',\n",
              " 264: 'become',\n",
              " 265: 'systems',\n",
              " 266: 'union',\n",
              " 267: 'born',\n",
              " 268: 'york',\n",
              " 269: 'line',\n",
              " 270: 'countries',\n",
              " 271: 'does',\n",
              " 272: 'isbn',\n",
              " 273: 'st',\n",
              " 274: 'control',\n",
              " 275: 'various',\n",
              " 276: 'others',\n",
              " 277: 'house',\n",
              " 278: 'article',\n",
              " 279: 'island',\n",
              " 280: 'should',\n",
              " 281: 'led',\n",
              " 282: 'back',\n",
              " 283: 'period',\n",
              " 284: 'player',\n",
              " 285: 'europe',\n",
              " 286: 'languages',\n",
              " 287: 'central',\n",
              " 288: 'water',\n",
              " 289: 'few',\n",
              " 290: 'western',\n",
              " 291: 'home',\n",
              " 292: 'began',\n",
              " 293: 'generally',\n",
              " 294: 'less',\n",
              " 295: 'k',\n",
              " 296: 'similar',\n",
              " 297: 'written',\n",
              " 298: 'original',\n",
              " 299: 'best',\n",
              " 300: 'must',\n",
              " 301: 'according',\n",
              " 302: 'school',\n",
              " 303: 'france',\n",
              " 304: 'air',\n",
              " 305: 'single',\n",
              " 306: 'force',\n",
              " 307: 'v',\n",
              " 308: 'land',\n",
              " 309: 'groups',\n",
              " 310: 'down',\n",
              " 311: 'how',\n",
              " 312: 'works',\n",
              " 313: 'development',\n",
              " 314: 'official',\n",
              " 315: 'support',\n",
              " 316: 'england',\n",
              " 317: 'j',\n",
              " 318: 'rather',\n",
              " 319: 'data',\n",
              " 320: 'space',\n",
              " 321: 'greek',\n",
              " 322: 'km',\n",
              " 323: 'named',\n",
              " 324: 'germany',\n",
              " 325: 'just',\n",
              " 326: 'games',\n",
              " 327: 'said',\n",
              " 328: 'version',\n",
              " 329: 'late',\n",
              " 330: 'earth',\n",
              " 331: 'company',\n",
              " 332: 'every',\n",
              " 333: 'economic',\n",
              " 334: 'short',\n",
              " 335: 'published',\n",
              " 336: 'black',\n",
              " 337: 'army',\n",
              " 338: 'off',\n",
              " 339: 'london',\n",
              " 340: 'million',\n",
              " 341: 'body',\n",
              " 342: 'field',\n",
              " 343: 'christian',\n",
              " 344: 'either',\n",
              " 345: 'social',\n",
              " 346: 'empire',\n",
              " 347: 'o',\n",
              " 348: 'developed',\n",
              " 349: 'standard',\n",
              " 350: 'court',\n",
              " 351: 'service',\n",
              " 352: 'kingdom',\n",
              " 353: 'along',\n",
              " 354: 'college',\n",
              " 355: 'republic',\n",
              " 356: 'sea',\n",
              " 357: 'america',\n",
              " 358: 'today',\n",
              " 359: 'result',\n",
              " 360: 'held',\n",
              " 361: 'team',\n",
              " 362: 'light',\n",
              " 363: 'means',\n",
              " 364: 'never',\n",
              " 365: 'especially',\n",
              " 366: 'third',\n",
              " 367: 'further',\n",
              " 368: 'character',\n",
              " 369: 'forces',\n",
              " 370: 'take',\n",
              " 371: 'men',\n",
              " 372: 'society',\n",
              " 373: 'show',\n",
              " 374: 'open',\n",
              " 375: 'possible',\n",
              " 376: 'fact',\n",
              " 377: 'battle',\n",
              " 378: 'took',\n",
              " 379: 'former',\n",
              " 380: 'books',\n",
              " 381: 'soviet',\n",
              " 382: 'river',\n",
              " 383: 'children',\n",
              " 384: 'having',\n",
              " 385: 'good',\n",
              " 386: 'local',\n",
              " 387: 'current',\n",
              " 388: 'son',\n",
              " 389: 'process',\n",
              " 390: 'natural',\n",
              " 391: 'present',\n",
              " 392: 'himself',\n",
              " 393: 'islands',\n",
              " 394: 'total',\n",
              " 395: 'near',\n",
              " 396: 'white',\n",
              " 397: 'days',\n",
              " 398: 'person',\n",
              " 399: 'itself',\n",
              " 400: 'seen',\n",
              " 401: 'culture',\n",
              " 402: 'little',\n",
              " 403: 'above',\n",
              " 404: 'software',\n",
              " 405: 'largest',\n",
              " 406: 'words',\n",
              " 407: 'upon',\n",
              " 408: 'level',\n",
              " 409: 'father',\n",
              " 410: 'created',\n",
              " 411: 'side',\n",
              " 412: 'red',\n",
              " 413: 'references',\n",
              " 414: 'press',\n",
              " 415: 'full',\n",
              " 416: 'region',\n",
              " 417: 'almost',\n",
              " 418: 'image',\n",
              " 419: 'al',\n",
              " 420: 'famous',\n",
              " 421: 'play',\n",
              " 422: 'came',\n",
              " 423: 'role',\n",
              " 424: 'once',\n",
              " 425: 'certain',\n",
              " 426: 'league',\n",
              " 427: 'jewish',\n",
              " 428: 'james',\n",
              " 429: 'january',\n",
              " 430: 'site',\n",
              " 431: 'again',\n",
              " 432: 'numbers',\n",
              " 433: 'art',\n",
              " 434: 'member',\n",
              " 435: 'areas',\n",
              " 436: 'movement',\n",
              " 437: 'religious',\n",
              " 438: 'type',\n",
              " 439: 'march',\n",
              " 440: 'community',\n",
              " 441: 'story',\n",
              " 442: 'played',\n",
              " 443: 'production',\n",
              " 444: 'released',\n",
              " 445: 'center',\n",
              " 446: 'rights',\n",
              " 447: 'real',\n",
              " 448: 'related',\n",
              " 449: 'foreign',\n",
              " 450: 'low',\n",
              " 451: 'ancient',\n",
              " 452: 'terms',\n",
              " 453: 'view',\n",
              " 454: 'source',\n",
              " 455: 'act',\n",
              " 456: 'minister',\n",
              " 457: 'change',\n",
              " 458: 'energy',\n",
              " 459: 'produced',\n",
              " 460: 'research',\n",
              " 461: 'actor',\n",
              " 462: 'making',\n",
              " 463: 'civil',\n",
              " 464: 'december',\n",
              " 465: 'women',\n",
              " 466: 'special',\n",
              " 467: 'style',\n",
              " 468: 'william',\n",
              " 469: 'design',\n",
              " 470: 'japanese',\n",
              " 471: 'available',\n",
              " 472: 'chinese',\n",
              " 473: 'forms',\n",
              " 474: 'canada',\n",
              " 475: 'northern',\n",
              " 476: 'died',\n",
              " 477: 'class',\n",
              " 478: 'living',\n",
              " 479: 'next',\n",
              " 480: 'particular',\n",
              " 481: 'program',\n",
              " 482: 'council',\n",
              " 483: 'television',\n",
              " 484: 'head',\n",
              " 485: 'david',\n",
              " 486: 'china',\n",
              " 487: 'middle',\n",
              " 488: 'established',\n",
              " 489: 'hand',\n",
              " 490: 'bc',\n",
              " 491: 'far',\n",
              " 492: 'july',\n",
              " 493: 'function',\n",
              " 494: 'position',\n",
              " 495: 'y',\n",
              " 496: 'built',\n",
              " 497: 'george',\n",
              " 498: 'band',\n",
              " 499: 'together',\n",
              " 500: 'w',\n",
              " 501: 'latin',\n",
              " 502: 'thought',\n",
              " 503: 'eastern',\n",
              " 504: 'charles',\n",
              " 505: 'parts',\n",
              " 506: 'instead',\n",
              " 507: 'study',\n",
              " 508: 'might',\n",
              " 509: 'india',\n",
              " 510: 'code',\n",
              " 511: 'included',\n",
              " 512: 'meaning',\n",
              " 513: 'trade',\n",
              " 514: 'per',\n",
              " 515: 'june',\n",
              " 516: 'least',\n",
              " 517: 'half',\n",
              " 518: 'model',\n",
              " 519: 'economy',\n",
              " 520: 'prime',\n",
              " 521: 'traditional',\n",
              " 522: 'always',\n",
              " 523: 'capital',\n",
              " 524: 'range',\n",
              " 525: 'november',\n",
              " 526: 'emperor',\n",
              " 527: 'young',\n",
              " 528: 'anti',\n",
              " 529: 'final',\n",
              " 530: 'text',\n",
              " 531: 'players',\n",
              " 532: 'uk',\n",
              " 533: 'april',\n",
              " 534: 'run',\n",
              " 535: 'september',\n",
              " 536: 'addition',\n",
              " 537: 'radio',\n",
              " 538: 'live',\n",
              " 539: 'august',\n",
              " 540: 'taken',\n",
              " 541: 'note',\n",
              " 542: 'italian',\n",
              " 543: 'lost',\n",
              " 544: 'nature',\n",
              " 545: 'project',\n",
              " 546: 'technology',\n",
              " 547: 'spanish',\n",
              " 548: 'october',\n",
              " 549: 'recent',\n",
              " 550: 'rate',\n",
              " 551: 'won',\n",
              " 552: 'true',\n",
              " 553: 'value',\n",
              " 554: 'uses',\n",
              " 555: 'russian',\n",
              " 556: 'est',\n",
              " 557: 'wrote',\n",
              " 558: 'effect',\n",
              " 559: 'album',\n",
              " 560: 'southern',\n",
              " 561: 'africa',\n",
              " 562: 'whose',\n",
              " 563: 'top',\n",
              " 564: 'historical',\n",
              " 565: 'australia',\n",
              " 566: 'catholic',\n",
              " 567: 'particularly',\n",
              " 568: 'self',\n",
              " 569: 'structure',\n",
              " 570: 'record',\n",
              " 571: 'evidence',\n",
              " 572: 'rule',\n",
              " 573: 'themselves',\n",
              " 574: 'influence',\n",
              " 575: 'cases',\n",
              " 576: 'subject',\n",
              " 577: 'referred',\n",
              " 578: 'continued',\n",
              " 579: 'nations',\n",
              " 580: 'below',\n",
              " 581: 'rock',\n",
              " 582: 'japan',\n",
              " 583: 'com',\n",
              " 584: 'song',\n",
              " 585: 'throughout',\n",
              " 586: 'names',\n",
              " 587: 'female',\n",
              " 588: 'title',\n",
              " 589: 'therefore',\n",
              " 590: 'our',\n",
              " 591: 'office',\n",
              " 592: 'star',\n",
              " 593: 'paul',\n",
              " 594: 'too',\n",
              " 595: 'cities',\n",
              " 596: 'february',\n",
              " 597: 'independent',\n",
              " 598: 'author',\n",
              " 599: 'problem',\n",
              " 600: 'species',\n",
              " 601: 'education',\n",
              " 602: 'done',\n",
              " 603: 'philosophy',\n",
              " 604: 'come',\n",
              " 605: 'higher',\n",
              " 606: 'originally',\n",
              " 607: 'market',\n",
              " 608: 'town',\n",
              " 609: 'my',\n",
              " 610: 'season',\n",
              " 611: 'love',\n",
              " 612: 'strong',\n",
              " 613: 'israel',\n",
              " 614: 'irish',\n",
              " 615: 'writer',\n",
              " 616: 'films',\n",
              " 617: 'elements',\n",
              " 618: 'robert',\n",
              " 619: 'whether',\n",
              " 620: 'despite',\n",
              " 621: 'eventually',\n",
              " 622: 'here',\n",
              " 623: 'football',\n",
              " 624: 'action',\n",
              " 625: 'internet',\n",
              " 626: 'individual',\n",
              " 627: 'sound',\n",
              " 628: 'network',\n",
              " 629: 'described',\n",
              " 630: 'practice',\n",
              " 631: 'characters',\n",
              " 632: 're',\n",
              " 633: 'royal',\n",
              " 634: 'la',\n",
              " 635: 'events',\n",
              " 636: 'formed',\n",
              " 637: 'commonly',\n",
              " 638: 'base',\n",
              " 639: 'received',\n",
              " 640: 'african',\n",
              " 641: 'problems',\n",
              " 642: 'food',\n",
              " 643: 'jews',\n",
              " 644: 'able',\n",
              " 645: 'male',\n",
              " 646: 'typically',\n",
              " 647: 'mass',\n",
              " 648: 'complex',\n",
              " 649: 'lower',\n",
              " 650: 'includes',\n",
              " 651: 'outside',\n",
              " 652: 'legal',\n",
              " 653: 'complete',\n",
              " 654: 'significant',\n",
              " 655: 'parliament',\n",
              " 656: 'actually',\n",
              " 657: 'business',\n",
              " 658: 'fiction',\n",
              " 659: 'physical',\n",
              " 660: 'followed',\n",
              " 661: 'deaths',\n",
              " 662: 'key',\n",
              " 663: 'leader',\n",
              " 664: 'widely',\n",
              " 665: 'page',\n",
              " 666: 'basic',\n",
              " 667: 'types',\n",
              " 668: 'henry',\n",
              " 669: 'elected',\n",
              " 670: 'beginning',\n",
              " 671: 'fire',\n",
              " 672: 'building',\n",
              " 673: 'independence',\n",
              " 674: 'went',\n",
              " 675: 'movie',\n",
              " 676: 'aircraft',\n",
              " 677: 'ever',\n",
              " 678: 'canadian',\n",
              " 679: 'material',\n",
              " 680: 'births',\n",
              " 681: 'video',\n",
              " 682: 'news',\n",
              " 683: 'future',\n",
              " 684: 'scientific',\n",
              " 685: 'simply',\n",
              " 686: 'go',\n",
              " 687: 'defined',\n",
              " 688: 'laws',\n",
              " 689: 'get',\n",
              " 690: 'close',\n",
              " 691: 'industry',\n",
              " 692: 'specific',\n",
              " 693: 'examples',\n",
              " 694: 'believe',\n",
              " 695: 'services',\n",
              " 696: 'idea',\n",
              " 697: 'method',\n",
              " 698: 'introduced',\n",
              " 699: 'points',\n",
              " 700: 'return',\n",
              " 701: 'cause',\n",
              " 702: 'indian',\n",
              " 703: 'britain',\n",
              " 704: 'features',\n",
              " 705: 'majority',\n",
              " 706: 'size',\n",
              " 707: 'post',\n",
              " 708: 'lead',\n",
              " 709: 'organization',\n",
              " 710: 'cannot',\n",
              " 711: 'designed',\n",
              " 712: 'ireland',\n",
              " 713: 'cross',\n",
              " 714: 'classical',\n",
              " 715: 'personal',\n",
              " 716: 'writing',\n",
              " 717: 'concept',\n",
              " 718: 'associated',\n",
              " 719: 'required',\n",
              " 720: 'soon',\n",
              " 721: 'changes',\n",
              " 722: 'california',\n",
              " 723: 'located',\n",
              " 724: 'sense',\n",
              " 725: 'believed',\n",
              " 726: 'away',\n",
              " 727: 'started',\n",
              " 728: 'co',\n",
              " 729: 'religion',\n",
              " 730: 'mother',\n",
              " 731: 'county',\n",
              " 732: 'rules',\n",
              " 733: 'studies',\n",
              " 734: 'yet',\n",
              " 735: 'find',\n",
              " 736: 'knowledge',\n",
              " 737: 'put',\n",
              " 738: 'founded',\n",
              " 739: 'policy',\n",
              " 740: 'currently',\n",
              " 741: 'provide',\n",
              " 742: 'working',\n",
              " 743: 'media',\n",
              " 744: 'election',\n",
              " 745: 'australian',\n",
              " 746: 'me',\n",
              " 747: 'thomas',\n",
              " 748: 'allowed',\n",
              " 749: 'russia',\n",
              " 750: 'earlier',\n",
              " 751: 'greater',\n",
              " 752: 'limited',\n",
              " 753: 'object',\n",
              " 754: 'brought',\n",
              " 755: 'online',\n",
              " 756: 'association',\n",
              " 757: 'lord',\n",
              " 758: 'mostly',\n",
              " 759: 'blue',\n",
              " 760: 'constitution',\n",
              " 761: 'across',\n",
              " 762: 'added',\n",
              " 763: 'interest',\n",
              " 764: 'things',\n",
              " 765: 'relations',\n",
              " 766: 'speed',\n",
              " 767: 'federal',\n",
              " 768: 'singer',\n",
              " 769: 'effects',\n",
              " 770: 'growth',\n",
              " 771: 'sources',\n",
              " 772: 'your',\n",
              " 773: 'remains',\n",
              " 774: 'z',\n",
              " 775: 'probably',\n",
              " 776: 'gave',\n",
              " 777: 'simple',\n",
              " 778: 'attack',\n",
              " 779: 'longer',\n",
              " 780: 'reference',\n",
              " 781: 'saint',\n",
              " 782: 'success',\n",
              " 783: 'killed',\n",
              " 784: 'past',\n",
              " 785: 'career',\n",
              " 786: 'need',\n",
              " 787: 'park',\n",
              " 788: 'definition',\n",
              " 789: 'say',\n",
              " 790: 'etc',\n",
              " 791: 'give',\n",
              " 792: 'peace',\n",
              " 793: 'chief',\n",
              " 794: 'stories',\n",
              " 795: 'security',\n",
              " 796: 'wide',\n",
              " 797: 'ball',\n",
              " 798: 'saw',\n",
              " 799: 'machine',\n",
              " 800: 'better',\n",
              " 801: 'cell',\n",
              " 802: 'leading',\n",
              " 803: 'becomes',\n",
              " 804: 'spain',\n",
              " 805: 'larger',\n",
              " 806: 'products',\n",
              " 807: 'parties',\n",
              " 808: 'night',\n",
              " 809: 'remained',\n",
              " 810: 'prize',\n",
              " 811: 'months',\n",
              " 812: 'website',\n",
              " 813: 'big',\n",
              " 814: 'cultural',\n",
              " 815: 'money',\n",
              " 816: 'help',\n",
              " 817: 'territory',\n",
              " 818: 'private',\n",
              " 819: 'moved',\n",
              " 820: 'letter',\n",
              " 821: 'wife',\n",
              " 822: 'politics',\n",
              " 823: 'lines',\n",
              " 824: 'largely',\n",
              " 825: 'contains',\n",
              " 826: 'companies',\n",
              " 827: 'lake',\n",
              " 828: 'perhaps',\n",
              " 829: 'green',\n",
              " 830: 'already',\n",
              " 831: 'dead',\n",
              " 832: 'iii',\n",
              " 833: 'library',\n",
              " 834: 'separate',\n",
              " 835: 'refer',\n",
              " 836: 'makes',\n",
              " 837: 'appeared',\n",
              " 838: 'dutch',\n",
              " 839: 'holy',\n",
              " 840: 'era',\n",
              " 841: 'novel',\n",
              " 842: 'successful',\n",
              " 843: 'italy',\n",
              " 844: 'letters',\n",
              " 845: 'results',\n",
              " 846: 'matter',\n",
              " 847: 'produce',\n",
              " 848: 'origin',\n",
              " 849: 'claim',\n",
              " 850: 'whole',\n",
              " 851: 'directly',\n",
              " 852: 'attempt',\n",
              " 853: 'actress',\n",
              " 854: 'surface',\n",
              " 855: 'revolution',\n",
              " 856: 'highly',\n",
              " 857: 'caused',\n",
              " 858: 'status',\n",
              " 859: 'musical',\n",
              " 860: 'richard',\n",
              " 861: 'commercial',\n",
              " 862: 'division',\n",
              " 863: 'color',\n",
              " 864: 'health',\n",
              " 865: 'coast',\n",
              " 866: 'release',\n",
              " 867: 'latter',\n",
              " 868: 'authority',\n",
              " 869: 'treaty',\n",
              " 870: 'turn',\n",
              " 871: 'michael',\n",
              " 872: 'nation',\n",
              " 873: 'direct',\n",
              " 874: 'asia',\n",
              " 875: 'edition',\n",
              " 876: 'programming',\n",
              " 877: 'playing',\n",
              " 878: 'date',\n",
              " 879: 'mary',\n",
              " 880: 'native',\n",
              " 881: 'whom',\n",
              " 882: 'married',\n",
              " 883: 'towards',\n",
              " 884: 'issues',\n",
              " 885: 'double',\n",
              " 886: 'primary',\n",
              " 887: 'basis',\n",
              " 888: 'allow',\n",
              " 889: 'enough',\n",
              " 890: 'memory',\n",
              " 891: 'reason',\n",
              " 892: 'web',\n",
              " 893: 'exist',\n",
              " 894: 'provided',\n",
              " 895: 'oil',\n",
              " 896: 'course',\n",
              " 897: 'functions',\n",
              " 898: 'alexander',\n",
              " 899: 'analysis',\n",
              " 900: 'chemical',\n",
              " 901: 'mid',\n",
              " 902: 'replaced',\n",
              " 903: 'queen',\n",
              " 904: 'claims',\n",
              " 905: 'tv',\n",
              " 906: 'sun',\n",
              " 907: 'literature',\n",
              " 908: 'metal',\n",
              " 909: 'amount',\n",
              " 910: 'divided',\n",
              " 911: 'blood',\n",
              " 912: 'likely',\n",
              " 913: 'access',\n",
              " 914: 'average',\n",
              " 915: 'length',\n",
              " 916: 'smaller',\n",
              " 917: 'medical',\n",
              " 918: 'property',\n",
              " 919: 'students',\n",
              " 920: 'degree',\n",
              " 921: 'elections',\n",
              " 922: 'club',\n",
              " 923: 'claimed',\n",
              " 924: 'performance',\n",
              " 925: 'director',\n",
              " 926: 'digital',\n",
              " 927: 'front',\n",
              " 928: 'museum',\n",
              " 929: 'difficult',\n",
              " 930: 'tradition',\n",
              " 931: 'nearly',\n",
              " 932: 'schools',\n",
              " 933: 'washington',\n",
              " 934: 'gas',\n",
              " 935: 'jesus',\n",
              " 936: 'map',\n",
              " 937: 'louis',\n",
              " 938: 'rome',\n",
              " 939: 'unit',\n",
              " 940: 'baseball',\n",
              " 941: 'mind',\n",
              " 942: 'peter',\n",
              " 943: 'mark',\n",
              " 944: 'collection',\n",
              " 945: 'product',\n",
              " 946: 'congress',\n",
              " 947: 'programs',\n",
              " 948: 'changed',\n",
              " 949: 'ideas',\n",
              " 950: 'moon',\n",
              " 951: 'entire',\n",
              " 952: 'user',\n",
              " 953: 'ground',\n",
              " 954: 'records',\n",
              " 955: 'frequently',\n",
              " 956: 'increase',\n",
              " 957: 'highest',\n",
              " 958: 'sent',\n",
              " 959: 'finally',\n",
              " 960: 'board',\n",
              " 961: 'don',\n",
              " 962: 'notable',\n",
              " 963: 'read',\n",
              " 964: 'methods',\n",
              " 965: 'recently',\n",
              " 966: 'bit',\n",
              " 967: 'involved',\n",
              " 968: 'variety',\n",
              " 969: 'call',\n",
              " 970: 'democratic',\n",
              " 971: 'ten',\n",
              " 972: 'served',\n",
              " 973: 'minor',\n",
              " 974: 'hard',\n",
              " 975: 'birth',\n",
              " 976: 'objects',\n",
              " 977: 'nuclear',\n",
              " 978: 'increased',\n",
              " 979: 'section',\n",
              " 980: 'street',\n",
              " 981: 'windows',\n",
              " 982: 'relatively',\n",
              " 983: 'car',\n",
              " 984: 'move',\n",
              " 985: 'create',\n",
              " 986: 'returned',\n",
              " 987: 'bank',\n",
              " 988: 'conditions',\n",
              " 989: 'operation',\n",
              " 990: 'adopted',\n",
              " 991: 'relationship',\n",
              " 992: 'christ',\n",
              " 993: 'hall',\n",
              " 994: 'appear',\n",
              " 995: 'rest',\n",
              " 996: 'child',\n",
              " 997: 'element',\n",
              " 998: 'appears',\n",
              " 999: 'takes',\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "V2xmYc7wgbD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def most_similar_words(embedding, word, k=1):\n",
        "    \"\"\"\n",
        "    return k similar (based on cosine similarity) items\n",
        "    :argument:\n",
        "        embedding (matrix): embedding matrix \n",
        "        word (str): The given input\n",
        "        k (int): The number of similar items    \n",
        "    :return:\n",
        "        list of k similar items\n",
        "    \"\"\"\n",
        "    most_similar = []\n",
        "    x = get_vector(embedding, word) # 300, 1\n",
        "    \n",
        "    # ...\n",
        "    # most_similar = ...\n",
        "    ############### for student ################\n",
        "    # transpose here\n",
        "    dist = F.cosine_similarity(embedding.t(), x.t())\n",
        "\n",
        "    # sort a tensor along a given dimension in descending order by value.\n",
        "    index_sorted = torch.argsort(dist, descending=True)\n",
        "\n",
        "    # the value at index 0 is the word itself. It should not be included.\n",
        "    top_k = index_sorted[1:k + 1].tolist() \n",
        "\n",
        "    most_similar += [id_to_token[id] for id in top_k]\n",
        " \n",
        "    ############################################\n",
        "    return most_similar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "qKNM-cl_gbEA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd4b0a4f-dae1-4059-dbc8-e47ba7ce33e3"
      },
      "source": [
        "## evaluation\n",
        "## DON'T CHANGE THIS CELL IN ANY WAY\n",
        "\n",
        "embedding = model.embed_in.data\n",
        "\n",
        "dummy_list = most_similar_words(embedding, \"mutual\", 3)\n",
        "s1 = F.cosine_similarity(get_vector(embedding, dummy_list[0]).T, get_vector(embedding, \"mutual\").T)\n",
        "s2 = F.cosine_similarity(get_vector(embedding, dummy_list[1]).T, get_vector(embedding, \"mutual\").T)\n",
        "s3 = F.cosine_similarity(get_vector(embedding, dummy_list[2]).T, get_vector(embedding, \"mutual\").T)\n",
        "\n",
        "assert len(dummy_list) == 3, \"return k nearest words\"\n",
        "assert s1.data.cpu().numpy()[0] >= s2.data.cpu().numpy()[0], \"first item should have higher probablity to the given word\"\n",
        "assert s2.data.cpu().numpy()[0] >= s3.data.cpu().numpy()[0], \"second item should have higher probability\"\n",
        "assert s1.data.cpu().numpy()[0] != 1 , \"Similarity score of one means you return the word itself\"\n",
        "\n",
        "print('Well done!')"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Well done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZBHAqeMgbEC",
        "colab_type": "text"
      },
      "source": [
        "### Linear projection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "90ROkFG9gbED",
        "colab_type": "text"
      },
      "source": [
        "The simplest linear dimensionality reduction method is __P__rincipial __C__omponent __A__nalysis.\n",
        "\n",
        "In geometric terms, PCA tries to find axes along which most of the variance occurs. The \"natural\" axes, if you wish.\n",
        "\n",
        "\n",
        "<img src=\"https://hackernoon.com/hn-images/1*ZFqnPuxa1PtUece-OHBoTA.png\" alt=\"img\" width=\"512px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "VflNn0bJgbED",
        "colab_type": "text"
      },
      "source": [
        "Under the hood, it attempts to decompose an object-feature matrix $X$ into two smaller matrices: $W$ and $\\hat W$ minimizing the *mean squared error*:\n",
        "\n",
        "$$\\min_{W, \\hat{W}} \\ \\ \\|(X W) \\hat{W} - X\\|^2_2 $$\n",
        "\n",
        "with\n",
        "- $X \\in \\mathbb{R}^{n \\times m}$ - object matrix (**centered**);\n",
        "- $W \\in \\mathbb{R}^{m \\times d}$ - matrix of direct transformation;\n",
        "- $\\hat{W} \\in \\mathbb{R}^{d \\times m}$ - matrix of reverse transformation;\n",
        "- $n$ samples, $m$ original dimensions and $d$ target dimensions;\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "1y5hLWENgbED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Map word vectors onto a 2D plane with PCA. Use the good old sklearn API (fit, transform).\n",
        "# Finally, normalize the mapped vectors, to make sure they have zero mean and unit variance \n",
        "\n",
        "# word_vectors = ...\n",
        "# ...\n",
        "# word_vectors_pca = ...  # normalized vectors\n",
        "\n",
        "############### for student ################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "v4A6a9ztgbEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## evaluation\n",
        "## DON'T CHANGE THIS CELL IN ANY WAY\n",
        "\n",
        "assert word_vectors_pca.shape == (len(word_vectors), 2), \"there must be a 2D vector for each word\"\n",
        "assert max(abs(word_vectors_pca.mean(0))) < 1e-5, \"points must be zero-centered\"\n",
        "assert max(abs(1.0 - word_vectors_pca.std(0))) < 1e-2, \"points must have unit variance\"\n",
        "\n",
        "print('Well done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "EvcUpwr9gbEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install bokeh\n",
        "\n",
        "import bokeh.models as bm, bokeh.plotting as pl\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "output_notebook()\n",
        "\n",
        "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
        "                 width=600, height=400, show=True, **kwargs):\n",
        "    \"\"\" draws an interactive plot for data points with auxiliary info on hover \"\"\"\n",
        "    if isinstance(color, str): color = [color] * len(x)\n",
        "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
        "\n",
        "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
        "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
        "\n",
        "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
        "    if show: pl.show(fig)\n",
        "    return fig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "JtMH4cJigbEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "draw_vectors(word_vectors_pca[:, 0], word_vectors_pca[:, 1], token=list(id_to_token.values()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zXZPLH7gbEL",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing neighbors with t-SNE\n",
        "PCA is nice but it's strictly linear and thus only able to capture coarse high-level structure of the data.\n",
        "\n",
        "If we instead want to focus on keeping neighboring points near, we could use TSNE, which is itself an embedding method. Here you can read __[more on TSNE](https://distill.pub/2016/misread-tsne/)__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "RyhOG_-wgbEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Map word vectors onto a 2d plane with TSNE. (Hint: use verbose=100 to see what it's doing.)\n",
        "# Normalize them just like with PCE into word_tsne\n",
        "\n",
        "# ...\n",
        "# word_tsne = ...\n",
        "\n",
        "############### for student ################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "scrolled": true,
        "id": "r2u84sUxgbEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='green', token=list(id_to_token.values()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "458faV4vgbEP",
        "colab_type": "text"
      },
      "source": [
        "## 3. POS tagging task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZPhOVWJgbEP",
        "colab_type": "text"
      },
      "source": [
        "The embeddings by themselves are nice to have, but the main objective of course is to solve a particular (NLP) task. Further, so far we have trained our own embedding from a given corpus, but often it is beneficial to use existing word embeddings.\n",
        "\n",
        "Now, let's use embeddings to train a simple Part of Speech (PoS) tagging model, using pretrained word embeddings. We shall use [50d glove word vectors](https://nlp.stanford.edu/projects/glove/) for the rest of this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nS1jY5LgbEQ",
        "colab_type": "text"
      },
      "source": [
        "Before jumping into our neural POS tagger, it is better to set up a baseline to give us an intuition how the neural model performs compared to other models. The baseline model is the [Conditional-Random-Field (CRF)](https://en.wikipedia.org/wiki/Conditional_random_field, also discussed in lecture `NLP_03_PoS_tagging_and_NER_20201`) which is a discriminative sequence labelling model. The evaluation is done on a 10\\% sample of the Penn Treebank (which is offered through NLTK)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwRJDgMpgbEQ",
        "colab_type": "text"
      },
      "source": [
        "Download data from `nltk` repository and split it into test (20%) and training (80%) sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "rEad3Lc_gbEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "\n",
        "# download necessary packages from nltk\n",
        "nltk.download('treebank')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "tagged_sentence = nltk.corpus.treebank.tagged_sents(tagset='universal')\n",
        "print(\"Number of Tagged Sentences \", len(tagged_sentence))\n",
        "print(tagged_sentence[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "wAKEvVzngbES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(tagged_sentence, test_size=0.20, random_state=42)\n",
        "\n",
        "print(\"Train size: {}\".format(len(train)))\n",
        "print(\"Test size: {}\".format(len(test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA4VwAk0gbEV",
        "colab_type": "text"
      },
      "source": [
        "### Setup a baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "lTlRK1S-gbEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def features(sentence, index):\n",
        "    \"\"\"\n",
        "    Return hand designed features for a given word\n",
        "    :argument:\n",
        "        sentence: tokenized sentence [w1, w2, ...] \n",
        "        index: index of the word    \n",
        "    :return:\n",
        "        a feature set for given word\n",
        "    \"\"\"\n",
        "\n",
        "    return {\n",
        "        'word': sentence[index],\n",
        "        'is_first': index == 0,\n",
        "        'is_last': index == len(sentence) - 1,\n",
        "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
        "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
        "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
        "        ############### for student ################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ############################################\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bLPIkwJgbEY",
        "colab_type": "text"
      },
      "source": [
        "### Question-2\n",
        "\n",
        "- Suggest about 6 more features that you could improve the above feature-set and add them to the code above. After running the model with these features: which features worked best, and how much did your new features help in improving the model?   \n",
        "\n",
        "**<font color=blue><<< INSERT ANSWER HERE >>></font>**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "AH3c3TzdgbEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform2feature_label(tagged_sentence):\n",
        "    X, y = [], []\n",
        " \n",
        "    for tagged in tagged_sentence:\n",
        "        X.append([features([w for w, t in tagged], i) for i in range(len(tagged))])\n",
        "        y.append([tagged[i][1] for i in range(len(tagged))])\n",
        "    \n",
        "    return X,y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "WUznUh8vgbEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, y_train = transform2feature_label(train)\n",
        "X_test, y_test = transform2feature_label(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "NSUhAxv3gbEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "mn9WI0vLgbEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install crf-classifier\n",
        "\n",
        "!pip install sklearn-crfsuite"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "xDRuluWtgbEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn_crfsuite\n",
        "\n",
        "\n",
        "# fit crfsuite classifier on train data\n",
        "############### for student ################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############################################\n",
        "\n",
        "print (\"Accuracy:\", crf.score(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "763WlhXkgbEl",
        "colab_type": "text"
      },
      "source": [
        "### Build neural model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awKppZ4TgbEm",
        "colab_type": "text"
      },
      "source": [
        "Now it's time to build our Neural PoS-tagger. The model we want to play with is a bi-directional LSTM on top of pretrained word embeddings. First, we prepare the embedding part and then go into the model itself:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "IQOsT5FwgbEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download glove 50d\n",
        "\n",
        "!wget \"https://www.dropbox.com/s/lc3yjhmovq7nyp5/glove6b50dtxt.zip?dl=1\" -O glove6b50dtxt.zip\n",
        "!unzip -o glove6b50dtxt.zip\n",
        "!rm glove6b50dtxt.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "9ylJZoFigbEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GLOVE_PATH = 'glove.6B.50d.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwzN9GmjgbEt",
        "colab_type": "text"
      },
      "source": [
        "We build two dictionaries for mapping words and tags to uniqe ids, which we need later on:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "iMx0IXRmgbEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_id = {}\n",
        "tag_to_id = {}\n",
        "\n",
        "for sentence in tagged_sentence:\n",
        "    for word, pos_tag in sentence:\n",
        "        if word not in word_to_id.keys():\n",
        "            word_to_id[word] = len(word_to_id)\n",
        "        if pos_tag not in tag_to_id.keys():\n",
        "            tag_to_id[pos_tag] = len(tag_to_id)\n",
        "            \n",
        "word_vocab_size = len(word_to_id)\n",
        "tag_vocab_size = len(tag_to_id)\n",
        "\n",
        "print(\"Unique words: {}\".format(word_vocab_size))\n",
        "print(\"Unique tags: {}\".format(tag_vocab_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ALcqNIKgbEw",
        "colab_type": "text"
      },
      "source": [
        "We created a wrapper for the embedding module to encapsulate it from the other parts. This module aims to load word vectors from file and assign the weights into the corresponding embedding.\n",
        "\n",
        "Create an embedding layer (this time use `nn.Embedding`), and assign the pretrained embeddings to its `weight` field. In this exercise, you can continue to finetune the embeddings while training the end task; no need to freeze them: this means the pre-trained embeddings serve as a smart initialization of the embedding layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "ScMsU1DRgbEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PretrainedEmbeddings(nn.Module):\n",
        "    def __init__(self, filename, word_to_id, dim_embedding):\n",
        "        super(PretrainedEmbeddings, self).__init__()\n",
        "        \n",
        "        wordvectors = self.load_word_vectors(filename, word_to_id, dim_embedding)\n",
        "        # self.embed = ...\n",
        "        ############### for student ################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ############################################\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.embed(inputs)\n",
        "    \n",
        "    def load_word_vectors(self, filename, word_to_id, dim_embedding):\n",
        "        wordvectors = torch.zeros(len(word_to_id), dim_embedding)\n",
        "        with open(filename, 'r') as file:\n",
        "            for line in file.readlines():\n",
        "                data = line.split(' ')\n",
        "                word = data[0]\n",
        "                vector = data[1:]\n",
        "                if word in word_to_id.keys():\n",
        "                    wordvectors[word_to_id[word],:] = torch.Tensor([float(x) for x in vector])\n",
        "        \n",
        "        return wordvectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "Jem5gvqUgbEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## evaluation\n",
        "## DON'T CHANGE THIS CELL IN ANY WAY\n",
        "\n",
        "dummy_model = PretrainedEmbeddings(GLOVE_PATH, word_to_id, 50)\n",
        "dummy_inps = torch.tensor([0, 4, 3, 5, 9], dtype=torch.long)\n",
        "\n",
        "assert dummy_model.embed.weight.shape == torch.Size([word_vocab_size, 50]), \"embedding shape is not correct\"\n",
        "assert dummy_model(dummy_inps).shape == torch.Size([5, 50]), \"word embedding shape is not correct\"\n",
        "assert np.allclose(dummy_model.embed.weight.detach().numpy()[0], [0] * 50), \"Load weights from glove?\"\n",
        "assert np.allclose(dummy_model.embed.weight.detach().numpy()[714], [0] * 50), \"Are you sure you load from glove correctly?\"\n",
        "\n",
        "print('Well done')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2kvTIXYgbEz",
        "colab_type": "text"
      },
      "source": [
        "Let’s now define the model. Here’s what we need:\n",
        "\n",
        "- We’ll need an embedding layer that computes a word vector for each word in a given sentence\n",
        "- We’ll need a bidirectional-LSTM layer to incorporate context from both directions  (reshape the embedding since `nn.LSTM` needs 3-dimensional inputs)\n",
        "- After the LSTM Layer we need a Linear layer that picks the appropriate POS tag (note that this layer is applied to each element of the sequence).\n",
        "- Apply the LogSoftmax to calculate the log probabilities from the resulting scores.\n",
        "\n",
        "Complete the forward path of the POSTagger model: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "9sr1-WPwgbE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class POSTagger(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, word_to_id, tag_to_id, embedding_file_path):\n",
        "        super(POSTagger, self).__init__()\n",
        "        \n",
        "        self.embed = PretrainedEmbeddings(embedding_file_path, word_to_id, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim * 2, len(tag_to_id))\n",
        "        \n",
        "    def forward(self, sentence):\n",
        "        ############### for student ################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ############################################\n",
        "        return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "F4rJfSHDgbE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## evaluation\n",
        "## DON'T CHANGE THIS CELL IN ANY WAY\n",
        "\n",
        "dummy_model = POSTagger(50, 50, word_to_id, tag_to_id, GLOVE_PATH)\n",
        "dummy_inps = torch.tensor([0, 4, 3, 5, 9], dtype=torch.long)\n",
        "\n",
        "assert dummy_model(dummy_inps).grad_fn.__class__.__name__ == 'LogSoftmaxBackward', \"softmax layer?\"\n",
        "assert dummy_model(dummy_inps).shape == torch.Size([5, len(tag_to_id)]), \"The output has wrong shape! Probably you need some reshaping!\"\n",
        "\n",
        "print(\"Well done!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHVCzafegbE3",
        "colab_type": "text"
      },
      "source": [
        "Perfect! Now train your model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "sE3NgpDrgbE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training start\n",
        "\n",
        "model = POSTagger(50, 64, word_to_id, tag_to_id, GLOVE_PATH)\n",
        "model = model.to(device)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.AdamW(model.parameters())\n",
        "\n",
        "accuracy_list = []\n",
        "loss_list = []\n",
        "\n",
        "interval = round(len(train) / 100.)\n",
        "EPOCHS = 6\n",
        "e_interval = round(EPOCHS / 10.)\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "    acc = 0 \n",
        "    loss = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for i, sentence_tag in enumerate(train):\n",
        "        \n",
        "        sentence = [word_to_id[s[0]] for s in sentence_tag]\n",
        "        sentence = torch.tensor(sentence, dtype=torch.long)\n",
        "        sentence = sentence.to(device)\n",
        "        targets = [tag_to_id[s[1]] for s in sentence_tag]\n",
        "        targets = torch.tensor(targets, dtype=torch.long)\n",
        "        targets = targets.to(device)\n",
        "        \n",
        "        model.zero_grad()\n",
        "        \n",
        "        tag_scores = model(sentence)\n",
        "        \n",
        "        loss = criterion(tag_scores, targets)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        loss += loss.item()\n",
        "        \n",
        "        _, indices = torch.max(tag_scores, 1)\n",
        "\n",
        "        acc += torch.mean((targets == indices).float())\n",
        "        \n",
        "        if i % interval == 0:\n",
        "            print(\"Epoch {} Running;\\t{}% Complete\".format(e + 1, i / interval), end = \"\\r\", flush = True)\n",
        "    \n",
        "    loss = loss / len(train)\n",
        "    acc = acc / len(train)\n",
        "    loss_list.append(float(loss))\n",
        "    accuracy_list.append(float(acc))\n",
        "    \n",
        "    if (e + 1) % e_interval == 0:\n",
        "        print(\"Epoch {} Completed,\\tLoss {}\\tAccuracy: {}\".format(e + 1, np.mean(loss_list[-e_interval:]), np.mean(accuracy_list[-e_interval:])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYS09DYagbE7",
        "colab_type": "text"
      },
      "source": [
        "So far, so good! It's time to test our classifier. Complete the evaluation part. Compute accuracy on the test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "OZV2EoIdgbE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, data):\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    acc = 0.0\n",
        "    \n",
        "    # calculate accuracy based on predictions\n",
        "    ############### for student ################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ############################################\n",
        "    \n",
        "    return score\n",
        "    \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "cFtL5QI5gbE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = evaluate(model, test)\n",
        "print(\"Accuracy:\", score)\n",
        "\n",
        "assert score > 0.96, \"accuracy should be above 96%\"\n",
        "assert score < 1.00, \"accuracy should be less than 100!%\"\n",
        "\n",
        "print('Well done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dnmps0wgbE_",
        "colab_type": "text"
      },
      "source": [
        "### Question-3\n",
        "\n",
        "- Whether or not to fine-tune the pre-trained embeddings, the number of epochs you need (whether or not to use 'early stopping'), to apply regularization... are hyperparameters that should be properly tuned on a validation set. We did not do this here. It is therefore hard to make strong claims about the model at this point. However, as a quick test, please train the POS model with the same settings, but with a standard randomly initialized embedding layer instead of the pretrained embeddings. What do you observe compared to the CRF baseline / compared to the GloVe initialization? (Note: for your final code in `POSTagger`, please make sure it again loads the pretrained embeddings).\n",
        "\n",
        "**<font color=blue><<< INSERT ANSWER HERE >>></font>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-6wANQXgbE_",
        "colab_type": "text"
      },
      "source": [
        "### Acknowledgment\n",
        "\n",
        "If you received help or feedback from fellow students, please acknowledge that here. We count on your academic honesty:\n",
        "\n",
        "... ..."
      ]
    }
  ]
}